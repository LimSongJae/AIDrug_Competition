{"cells":[{"cell_type":"code","execution_count":1,"id":"91ce4806","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"91ce4806","executionInfo":{"status":"ok","timestamp":1693233053188,"user_tz":-540,"elapsed":11623,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"d69506ff-ac03-4e58-ec14-80d1cdc95b3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["import random\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import missingno\n","\n","# device 설정\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print (device)\n","\n","seed = 42 # seed 값 설정\n","random.seed(seed) # 파이썬 난수 생성기\n","os.environ['PYTHONHASHSEED'] = str(seed) # 해시 시크릿값 고정\n","np.random.seed(seed) # 넘파이 난수 생성기\n","\n","torch.manual_seed(seed) # 파이토치 CPU 난수 생성기\n","torch.backends.cudnn.deterministic = True # 확정적 연산 사용 설정\n","torch.backends.cudnn.benchmark = False   # 벤치마크 기능 사용 해제\n","torch.backends.cudnn.enabled = False        # cudnn 기능 사용 해제\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed(seed) # 파이토치 GPU 난수 생성기\n","    torch.cuda.manual_seed_all(seed) # 파이토치 멀티 GPU 난수 생성기"]},{"cell_type":"code","execution_count":2,"id":"05190699","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"05190699","executionInfo":{"status":"ok","timestamp":1693233072303,"user_tz":-540,"elapsed":19120,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"20c23a4b-0b93-45cc-b871-1b304995774d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"2cc1c881","metadata":{"id":"2cc1c881","executionInfo":{"status":"ok","timestamp":1693233073158,"user_tz":-540,"elapsed":858,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/train.csv')\n","test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/test.csv')"]},{"cell_type":"code","execution_count":4,"id":"9915c913","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":513},"id":"9915c913","executionInfo":{"status":"ok","timestamp":1693233073159,"user_tz":-540,"elapsed":10,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"6dd6b958-42fe-4ea4-8ec5-dfb314321798"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              id                                             SMILES     MLM  \\\n","0     TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n","1     TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n","2     TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n","3     TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n","4     TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n","...          ...                                                ...     ...   \n","3493  TRAIN_3493     Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl   1.556   \n","3494  TRAIN_3494  CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...  35.560   \n","3495  TRAIN_3495                       CCOC(=O)CCCc1nc2cc(N)ccc2n1C  56.150   \n","3496  TRAIN_3496                     Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl   0.030   \n","3497  TRAIN_3497                   COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1   0.450   \n","\n","         HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n","0     50.680  3.259           400.495                5             2   \n","1     50.590  2.169           301.407                2             1   \n","2     80.892  1.593           297.358                5             0   \n","3      2.000  4.771           494.652                6             0   \n","4     99.990  2.335           268.310                3             0   \n","...      ...    ...               ...              ...           ...   \n","3493   3.079  3.409           396.195                3             1   \n","3494  47.630  1.912           359.381                4             1   \n","3495   1.790  1.941           261.320                3             1   \n","3496   2.770  0.989           284.696                5             1   \n","3497   2.650  4.321           295.399                2             0   \n","\n","      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  \n","0                      8  3.259                      117.37  \n","1                      2  2.172                       73.47  \n","2                      3  1.585                       62.45  \n","3                      5  3.475                       92.60  \n","4                      1  2.337                       42.43  \n","...                  ...    ...                         ...  \n","3493                   5  3.409                       64.74  \n","3494                   3  1.844                       77.37  \n","3495                   6  2.124                       70.14  \n","3496                   5  0.989                       91.51  \n","3497                   4  4.321                       50.36  \n","\n","[3498 rows x 11 columns]"],"text/html":["\n","  <div id=\"df-6376668e-901f-4521-b696-1eb9509aaffa\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>SMILES</th>\n","      <th>MLM</th>\n","      <th>HLM</th>\n","      <th>AlogP</th>\n","      <th>Molecular_Weight</th>\n","      <th>Num_H_Acceptors</th>\n","      <th>Num_H_Donors</th>\n","      <th>Num_RotatableBonds</th>\n","      <th>LogD</th>\n","      <th>Molecular_PolarSurfaceArea</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_0000</td>\n","      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n","      <td>26.010</td>\n","      <td>50.680</td>\n","      <td>3.259</td>\n","      <td>400.495</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>3.259</td>\n","      <td>117.37</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_0001</td>\n","      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n","      <td>29.270</td>\n","      <td>50.590</td>\n","      <td>2.169</td>\n","      <td>301.407</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2.172</td>\n","      <td>73.47</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_0002</td>\n","      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n","      <td>5.586</td>\n","      <td>80.892</td>\n","      <td>1.593</td>\n","      <td>297.358</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.585</td>\n","      <td>62.45</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_0003</td>\n","      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n","      <td>5.710</td>\n","      <td>2.000</td>\n","      <td>4.771</td>\n","      <td>494.652</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>3.475</td>\n","      <td>92.60</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_0004</td>\n","      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n","      <td>93.270</td>\n","      <td>99.990</td>\n","      <td>2.335</td>\n","      <td>268.310</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.337</td>\n","      <td>42.43</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3493</th>\n","      <td>TRAIN_3493</td>\n","      <td>Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl</td>\n","      <td>1.556</td>\n","      <td>3.079</td>\n","      <td>3.409</td>\n","      <td>396.195</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3.409</td>\n","      <td>64.74</td>\n","    </tr>\n","    <tr>\n","      <th>3494</th>\n","      <td>TRAIN_3494</td>\n","      <td>CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...</td>\n","      <td>35.560</td>\n","      <td>47.630</td>\n","      <td>1.912</td>\n","      <td>359.381</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1.844</td>\n","      <td>77.37</td>\n","    </tr>\n","    <tr>\n","      <th>3495</th>\n","      <td>TRAIN_3495</td>\n","      <td>CCOC(=O)CCCc1nc2cc(N)ccc2n1C</td>\n","      <td>56.150</td>\n","      <td>1.790</td>\n","      <td>1.941</td>\n","      <td>261.320</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>2.124</td>\n","      <td>70.14</td>\n","    </tr>\n","    <tr>\n","      <th>3496</th>\n","      <td>TRAIN_3496</td>\n","      <td>Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl</td>\n","      <td>0.030</td>\n","      <td>2.770</td>\n","      <td>0.989</td>\n","      <td>284.696</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0.989</td>\n","      <td>91.51</td>\n","    </tr>\n","    <tr>\n","      <th>3497</th>\n","      <td>TRAIN_3497</td>\n","      <td>COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1</td>\n","      <td>0.450</td>\n","      <td>2.650</td>\n","      <td>4.321</td>\n","      <td>295.399</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.321</td>\n","      <td>50.36</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3498 rows × 11 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6376668e-901f-4521-b696-1eb9509aaffa')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-6376668e-901f-4521-b696-1eb9509aaffa button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-6376668e-901f-4521-b696-1eb9509aaffa');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-974edf73-e17f-47b0-b913-acb5939ea1a9\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-974edf73-e17f-47b0-b913-acb5939ea1a9')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-974edf73-e17f-47b0-b913-acb5939ea1a9 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}],"source":["train"]},{"cell_type":"code","execution_count":5,"id":"50578fa3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"50578fa3","executionInfo":{"status":"ok","timestamp":1693233084690,"user_tz":-540,"elapsed":11536,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"4c3aaef9-e5a2-480a-a5d0-3c98ac0b38e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rdkit-pypi\n","  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n","Installing collected packages: rdkit-pypi\n","Successfully installed rdkit-pypi-2022.9.5\n"]}],"source":["!pip install rdkit-pypi"]},{"cell_type":"code","execution_count":6,"id":"26b73baf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"26b73baf","executionInfo":{"status":"ok","timestamp":1693233096697,"user_tz":-540,"elapsed":12014,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"f418ca8c-b83b-4145-86c7-286109fbca11"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              id                                             SMILES     MLM  \\\n","0     TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n","1     TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n","2     TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n","3     TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n","4     TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n","...          ...                                                ...     ...   \n","3493  TRAIN_3493     Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl   1.556   \n","3494  TRAIN_3494  CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...  35.560   \n","3495  TRAIN_3495                       CCOC(=O)CCCc1nc2cc(N)ccc2n1C  56.150   \n","3496  TRAIN_3496                     Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl   0.030   \n","3497  TRAIN_3497                   COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1   0.450   \n","\n","         HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n","0     50.680  3.259           400.495                5             2   \n","1     50.590  2.169           301.407                2             1   \n","2     80.892  1.593           297.358                5             0   \n","3      2.000  4.771           494.652                6             0   \n","4     99.990  2.335           268.310                3             0   \n","...      ...    ...               ...              ...           ...   \n","3493   3.079  3.409           396.195                3             1   \n","3494  47.630  1.912           359.381                4             1   \n","3495   1.790  1.941           261.320                3             1   \n","3496   2.770  0.989           284.696                5             1   \n","3497   2.650  4.321           295.399                2             0   \n","\n","      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea     logP     apka  \\\n","0                      8  3.259                      117.37  3.87744  400.504   \n","1                      2  2.172                       73.47  3.35474  301.415   \n","2                      3  1.585                       62.45  1.20450  297.366   \n","3                      5  3.475                       92.60  3.89356  494.665   \n","4                      1  2.337                       42.43  2.81772  268.316   \n","...                  ...    ...                         ...      ...      ...   \n","3493                   5  3.409                       64.74  2.74730  396.200   \n","3494                   3  1.844                       77.37  2.27630  359.389   \n","3495                   6  2.124                       70.14  2.04130  261.325   \n","3496                   5  0.989                       91.51  1.42720  284.699   \n","3497                   4  4.321                       50.36  4.71792  295.407   \n","\n","      num_rotatable_bonds  num_heteroatoms  num_hydrogen_acceptors  \\\n","0                       8                8                       6   \n","1                       2                5                       4   \n","2                       3                7                       7   \n","3                       5                9                       7   \n","4                       1                4                       3   \n","...                   ...              ...                     ...   \n","3493                    4               11                       5   \n","3494                    3                7                       5   \n","3495                    5                5                       5   \n","3496                    4                7                       6   \n","3497                    4                3                       3   \n","\n","      num_hydrogen_donors                                 morgan_fingerprint  \n","0                       2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","1                       1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3                       0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","...                   ...                                                ...  \n","3493                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3494                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3495                    1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3496                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n","3497                    0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","\n","[3498 rows x 18 columns]"],"text/html":["\n","  <div id=\"df-2e106cd1-45de-49bb-b977-9e9bb0bff499\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>SMILES</th>\n","      <th>MLM</th>\n","      <th>HLM</th>\n","      <th>AlogP</th>\n","      <th>Molecular_Weight</th>\n","      <th>Num_H_Acceptors</th>\n","      <th>Num_H_Donors</th>\n","      <th>Num_RotatableBonds</th>\n","      <th>LogD</th>\n","      <th>Molecular_PolarSurfaceArea</th>\n","      <th>logP</th>\n","      <th>apka</th>\n","      <th>num_rotatable_bonds</th>\n","      <th>num_heteroatoms</th>\n","      <th>num_hydrogen_acceptors</th>\n","      <th>num_hydrogen_donors</th>\n","      <th>morgan_fingerprint</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_0000</td>\n","      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n","      <td>26.010</td>\n","      <td>50.680</td>\n","      <td>3.259</td>\n","      <td>400.495</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>3.259</td>\n","      <td>117.37</td>\n","      <td>3.87744</td>\n","      <td>400.504</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_0001</td>\n","      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n","      <td>29.270</td>\n","      <td>50.590</td>\n","      <td>2.169</td>\n","      <td>301.407</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2.172</td>\n","      <td>73.47</td>\n","      <td>3.35474</td>\n","      <td>301.415</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_0002</td>\n","      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n","      <td>5.586</td>\n","      <td>80.892</td>\n","      <td>1.593</td>\n","      <td>297.358</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.585</td>\n","      <td>62.45</td>\n","      <td>1.20450</td>\n","      <td>297.366</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_0003</td>\n","      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n","      <td>5.710</td>\n","      <td>2.000</td>\n","      <td>4.771</td>\n","      <td>494.652</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>3.475</td>\n","      <td>92.60</td>\n","      <td>3.89356</td>\n","      <td>494.665</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_0004</td>\n","      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n","      <td>93.270</td>\n","      <td>99.990</td>\n","      <td>2.335</td>\n","      <td>268.310</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.337</td>\n","      <td>42.43</td>\n","      <td>2.81772</td>\n","      <td>268.316</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3493</th>\n","      <td>TRAIN_3493</td>\n","      <td>Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl</td>\n","      <td>1.556</td>\n","      <td>3.079</td>\n","      <td>3.409</td>\n","      <td>396.195</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3.409</td>\n","      <td>64.74</td>\n","      <td>2.74730</td>\n","      <td>396.200</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3494</th>\n","      <td>TRAIN_3494</td>\n","      <td>CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...</td>\n","      <td>35.560</td>\n","      <td>47.630</td>\n","      <td>1.912</td>\n","      <td>359.381</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1.844</td>\n","      <td>77.37</td>\n","      <td>2.27630</td>\n","      <td>359.389</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3495</th>\n","      <td>TRAIN_3495</td>\n","      <td>CCOC(=O)CCCc1nc2cc(N)ccc2n1C</td>\n","      <td>56.150</td>\n","      <td>1.790</td>\n","      <td>1.941</td>\n","      <td>261.320</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>2.124</td>\n","      <td>70.14</td>\n","      <td>2.04130</td>\n","      <td>261.325</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3496</th>\n","      <td>TRAIN_3496</td>\n","      <td>Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl</td>\n","      <td>0.030</td>\n","      <td>2.770</td>\n","      <td>0.989</td>\n","      <td>284.696</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0.989</td>\n","      <td>91.51</td>\n","      <td>1.42720</td>\n","      <td>284.699</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3497</th>\n","      <td>TRAIN_3497</td>\n","      <td>COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1</td>\n","      <td>0.450</td>\n","      <td>2.650</td>\n","      <td>4.321</td>\n","      <td>295.399</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.321</td>\n","      <td>50.36</td>\n","      <td>4.71792</td>\n","      <td>295.407</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3498 rows × 18 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e106cd1-45de-49bb-b977-9e9bb0bff499')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2e106cd1-45de-49bb-b977-9e9bb0bff499 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2e106cd1-45de-49bb-b977-9e9bb0bff499');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-2a9eba4c-7ae2-469a-b861-7b35f6b1697d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a9eba4c-7ae2-469a-b861-7b35f6b1697d')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-2a9eba4c-7ae2-469a-b861-7b35f6b1697d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":6}],"source":["import pandas as pd\n","import numpy as np\n","from rdkit import Chem, DataStructs\n","from rdkit.Chem import AllChem, Descriptors\n","\n","def calculate_metabolic_stability_descriptors(smiles):\n","    mol = Chem.MolFromSmiles(smiles)\n","    logP = Descriptors.MolLogP(mol)\n","    # 화합물의 친유성을 측정한 것으로 지질 또는 비극성 환경에서의 용해도를 나타냅니다. 생물학적 막을 통과하는 화합물의 능력을 반영합니다.\n","    apka = Descriptors.MolWt(mol)\n","    # 화합물의 산 해리 상수의 추정치로 다양한 pH 조건에서 이온화 거동에 대한 정보를 제공합니다.\n","    num_rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n","    # 화합물에서 회전 가능한 결합의 수입니다. 이것은 화합물의 유연성과 효소 또는 다른 분자와의 잠재적인 상호 작용에 대한 통찰력을 제공할 수 있습니다.\n","    num_heteroatoms = Descriptors.NumHeteroatoms(mol)\n","    # 분자 내 헤테로원자(탄소 및 수소 이외의 원자) 수. 이는 화합물의 반응성과 대사 안정성에 영향을 줄 수 있습니다.\n","    num_hydrogen_acceptors = Descriptors.NumHAcceptors(mol)\n","    # 분자 내 수소 결합 수용체의 수. 이들은 결합 및 반응성에 영향을 미치는 다른 분자의 수소 결합 기증자와 상호 작용할 수 있는 사이트입니다.\n","    num_hydrogen_donors = Descriptors.NumHDonors(mol)\n","    # 분자 내 수소 결합 기증자의 수입니다. 이들은 수소 결합 상호작용에서 수소 원자를 제공할 수 있는 사이트입니다.\n","    # morgan_fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n","    morgan_fingerprint = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n","    # 분자 하위 구조의 이진 벡터 표현입니다. 이 열에는 화합물과 효소의 상호 작용 및 대사 안정성에 영향을 줄 수 있는 구조적 특징을 포착하는 이진 지문이 포함되어 있습니다.\n","    morgan_array = np.zeros((1,), dtype=np.int8)\n","    DataStructs.ConvertToNumpyArray(morgan_fingerprint, morgan_array)\n","\n","    return logP, apka, num_rotatable_bonds, num_heteroatoms, num_hydrogen_acceptors, num_hydrogen_donors, morgan_array\n","\n","train[[\n","    'logP', 'apka', 'num_rotatable_bonds', 'num_heteroatoms',\n","    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint'\n","]] = train['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n","\n","test[[\n","    'logP', 'apka', 'num_rotatable_bonds', 'num_heteroatoms',\n","    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint'\n","]] = test['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n","\n","train\n"]},{"cell_type":"code","execution_count":7,"id":"327572d2","metadata":{"id":"327572d2","executionInfo":{"status":"ok","timestamp":1693233097453,"user_tz":-540,"elapsed":3,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["train['AlogP'].fillna(train['AlogP'].median(), inplace=True)\n","test['AlogP'].fillna(test['AlogP'].median(), inplace=True)"]},{"cell_type":"code","execution_count":8,"id":"1b4142d8","metadata":{"id":"1b4142d8","executionInfo":{"status":"ok","timestamp":1693233097453,"user_tz":-540,"elapsed":2,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, target_col=None, transform=None, is_test=False):\n","        self.is_test = is_test\n","        self.transform = transform\n","        self.is_test = is_test\n","\n","        if not self.is_test:\n","            self.data = self.transform.fit_transform(np.stack(data['morgan_fingerprint']))\n","        else: # test\n","            self.data = self.transform.transform(np.stack(data['morgan_fingerprint']))\n","\n","        if target_col is not None and not self.is_test:\n","            self.target = data[target_col]\n","\n","    def __getitem__(self, index):\n","        features = self.data[index]\n","\n","        if hasattr(self, 'target'):\n","            target = self.target[index]\n","            return torch.tensor(features).to(device).float(), torch.tensor(target).to(device).float().unsqueeze(dim=-1)\n","        else:\n","            return torch.tensor(features).to(device).float()\n","\n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"code","execution_count":13,"id":"fd4677c4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fd4677c4","executionInfo":{"status":"ok","timestamp":1693233144431,"user_tz":-540,"elapsed":851,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"73770a29-d48f-4424-8b15-30b2d1ca10b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["251"]},"metadata":{},"execution_count":13}],"source":["transform = VarianceThreshold(threshold=0.05)\n","\n","train_MLM = CustomDataset(train, target_col='MLM', transform=transform, is_test=False)\n","train_HLM = CustomDataset(train, target_col='HLM', transform=transform, is_test=False)\n","\n","input_size = train_MLM.data.shape[1]\n","input_size"]},{"cell_type":"code","execution_count":22,"id":"e97e0190","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e97e0190","executionInfo":{"status":"ok","timestamp":1693235032717,"user_tz":-540,"elapsed":6,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"6e7893ac-fe72-474b-d208-e55fb9c50edb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 0, ..., 0, 1, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 1, 0, ..., 0, 0, 0],\n","       [0, 0, 1, ..., 0, 0, 0]], dtype=int8)"]},"metadata":{},"execution_count":22}],"source":["train_HLM.data"]},{"cell_type":"code","source":["# train,valid split\n","train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42)\n","train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42)"],"metadata":{"id":"WeUTO-lrVcXn","executionInfo":{"status":"ok","timestamp":1693233164699,"user_tz":-540,"elapsed":976,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"id":"WeUTO-lrVcXn","execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":18,"id":"c0d85841","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0d85841","executionInfo":{"status":"ok","timestamp":1693233172901,"user_tz":-540,"elapsed":3,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"d1b4651e-a58c-4c2f-c43a-65e842fcde88"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([251]), torch.Size([1]))"]},"metadata":{},"execution_count":18}],"source":["torch.tensor(train_MLM.data[1]).shape, torch.tensor(train['MLM'][1]).float().unsqueeze(dim=-1).shape"]},{"cell_type":"code","execution_count":64,"id":"4c18e8c5","metadata":{"id":"4c18e8c5","executionInfo":{"status":"ok","timestamp":1693239139594,"user_tz":-540,"elapsed":324,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["# Hyperparameter\n","CFG = {'BATCH_SIZE': 256,\n","       'EPOCHS': 1000,\n","       'HIDDEN_SIZE': 251,\n","       'OUTPUT_SIZE': 1,\n","       'DROPOUT_RATE': 0.7,\n","       'FEATURE_SIZE': 251,\n","       'NUM_LAYERS' : 4,\n","       'LEARNING_RATE': 0.001\n","      }"]},{"cell_type":"code","execution_count":65,"id":"4d41c18d","metadata":{"id":"4d41c18d","executionInfo":{"status":"ok","timestamp":1693239139935,"user_tz":-540,"elapsed":2,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=True)\n","\n","valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=False)\n","\n","\n","train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=True)\n","\n","valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=False)"]},{"cell_type":"code","execution_count":66,"id":"44c3d429","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44c3d429","executionInfo":{"status":"ok","timestamp":1693239140233,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"5a9f0a90-d4e2-480b-fab7-0e429ed5cd0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256, 251]) torch.Size([256, 1])\n"]}],"source":["X_train, y_train = next(iter(train_MLM_loader))\n","print (X_train.shape, y_train.shape)"]},{"cell_type":"code","execution_count":67,"id":"28c77589","metadata":{"id":"28c77589","executionInfo":{"status":"ok","timestamp":1693239140953,"user_tz":-540,"elapsed":3,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, feature_size, hidden_size, num_layers, dropout_p, output_size):\n","        super(LSTMModel, self).__init__()\n","\n","        self.sequenceclassifier = nn.LSTM(\n","                input_size=feature_size,\n","                hidden_size=hidden_size,\n","                num_layers=num_layers,\n","                batch_first=True,\n","                bidirectional=True,\n","                dropout=dropout_p\n","        )\n","        # (256, 1, 251) -> (256, 1, hidden_size * 2)\n","\n","        self.fc = nn.Sequential(\n","            nn.ReLU(),  # LeakyReLU 대신 ReLU 사용\n","            nn.Linear(hidden_size * 2, output_size)\n","        )\n","        # (256, hidden_size * 2) -> (256, output_size)\n","\n","    def forward(self, x):\n","        # |x| = (256, 1, 251)\n","        output, _ = self.sequenceclassifier(x)  # |output| = (256, 1, hidden_size * 2)\n","        output = output[:, -1, :]  # |output| = (256, hidden_size * 2)\n","        y = self.fc(output)  # (256, output_size)\n","        return y\n"]},{"cell_type":"code","execution_count":68,"id":"b12d36a6","metadata":{"id":"b12d36a6","executionInfo":{"status":"ok","timestamp":1693239141992,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["model_MLM = LSTMModel(CFG['FEATURE_SIZE'],CFG['HIDDEN_SIZE'],CFG['NUM_LAYERS'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(device)\n","model_HLM = LSTMModel(CFG['FEATURE_SIZE'],CFG['HIDDEN_SIZE'],CFG['NUM_LAYERS'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE']).to(device)"]},{"cell_type":"code","execution_count":69,"id":"858c9e8f","metadata":{"id":"858c9e8f","executionInfo":{"status":"ok","timestamp":1693239142657,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super(RMSELoss, self).__init__()\n","        self.mse = nn.MSELoss()  # 기존의 MSELoss 함수 사용\n","\n","    def forward(self, output, target):\n","        mse_loss = self.mse(output, target)  # 기존의 MSELoss를 계산\n","        rmse_loss = torch.sqrt(mse_loss)  # MSE에 제곱근 씌워 RMSE 계산\n","        return rmse_loss\n","\n","criterion = RMSELoss()\n","optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n","optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n"]},{"cell_type":"code","execution_count":70,"id":"83db2239","metadata":{"id":"83db2239","executionInfo":{"status":"ok","timestamp":1693239143564,"user_tz":-540,"elapsed":470,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["def train(train_loader, valid_loader, model, criterion, optimizer, epochs, patience=100):\n","    best_valid_loss = float('inf')\n","    no_improvement_count = 0\n","\n","    for epoch in range(epochs):\n","        model.train()  # 모델을 훈련 모드로 설정\n","        running_loss = 0\n","        for inputs, targets in train_loader:\n","\n","            inputs = inputs.unsqueeze(1)  # (256, 251) -> (256, 1, 251)\n","\n","            optimizer.zero_grad()\n","\n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        model.eval()  # 모델을 검증 모드로 설정\n","        valid_loss = 0\n","        with torch.no_grad():\n","          for inputs, targets in valid_loader:\n","\n","            inputs = inputs.unsqueeze(1)  # (256, 251) -> (256, 1, 251)\n","\n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            valid_loss += loss.item()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","        avg_valid_loss = valid_loss / len(valid_loader)\n","        print(f'Epoch: {epoch}/{epochs}, Train Loss: {avg_train_loss}, Valid Loss: {avg_valid_loss}')\n","\n","        if avg_valid_loss < best_valid_loss:\n","          best_valid_loss = avg_valid_loss\n","          no_improvement_count = 0\n","          best_model_state = model.state_dict()\n","        else:\n","          no_improvement_count += 1\n","          if no_improvement_count >= patience:\n","            print(f'얼리 스토핑: {patience} 에포크 동안 검증 손실이 향상되지 않음. 에포크 {epoch}에서 훈련 중단.')\n","            break\n","\n","    # 최적의 모델 상태 불러오기\n","    model.load_state_dict(best_model_state)\n","\n","    return model"]},{"cell_type":"code","execution_count":71,"id":"51e8d425","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51e8d425","outputId":"f53c3c12-c7cc-4cfb-9bb4-9a7ed7c1c451","executionInfo":{"status":"ok","timestamp":1693239898697,"user_tz":-540,"elapsed":754667,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Start: MLM\n","Epoch: 0/1000, Train Loss: 50.903778769753195, Valid Loss: 50.56126149495443\n","Epoch: 1/1000, Train Loss: 47.585411765358664, Valid Loss: 46.77966054280599\n","Epoch: 2/1000, Train Loss: 44.65989615700462, Valid Loss: 44.80829747517904\n","Epoch: 3/1000, Train Loss: 42.87137707796964, Valid Loss: 43.24353154500326\n","Epoch: 4/1000, Train Loss: 41.364351445978336, Valid Loss: 41.89552307128906\n","Epoch: 5/1000, Train Loss: 40.12920518354936, Valid Loss: 40.75764846801758\n","Epoch: 6/1000, Train Loss: 39.0369873046875, Valid Loss: 39.82798767089844\n","Epoch: 7/1000, Train Loss: 38.191538377241656, Valid Loss: 39.05725351969401\n","Epoch: 8/1000, Train Loss: 37.472902818159625, Valid Loss: 38.43890889485677\n","Epoch: 9/1000, Train Loss: 36.88442542336204, Valid Loss: 37.96820068359375\n","Epoch: 10/1000, Train Loss: 36.478118896484375, Valid Loss: 37.60636647542318\n","Epoch: 11/1000, Train Loss: 36.16472660411488, Valid Loss: 37.33470662434896\n","Epoch: 12/1000, Train Loss: 35.926597595214844, Valid Loss: 37.13662974039713\n","Epoch: 13/1000, Train Loss: 35.76607478748668, Valid Loss: 37.00243123372396\n","Epoch: 14/1000, Train Loss: 35.638052160089664, Valid Loss: 36.905503590901695\n","Epoch: 15/1000, Train Loss: 35.566510980779476, Valid Loss: 36.83493296305338\n","Epoch: 16/1000, Train Loss: 35.522919394753195, Valid Loss: 36.790121714274086\n","Epoch: 17/1000, Train Loss: 35.48435904762962, Valid Loss: 36.759543100992836\n","Epoch: 18/1000, Train Loss: 35.447636344216086, Valid Loss: 36.74070612589518\n","Epoch: 19/1000, Train Loss: 35.45477502996271, Valid Loss: 36.72893142700195\n","Epoch: 20/1000, Train Loss: 35.454045382413, Valid Loss: 36.721771240234375\n","Epoch: 21/1000, Train Loss: 35.44076642123136, Valid Loss: 36.71687571207682\n","Epoch: 22/1000, Train Loss: 35.42866134643555, Valid Loss: 36.71264521280924\n","Epoch: 23/1000, Train Loss: 35.43846962668679, Valid Loss: 36.71149698893229\n","Epoch: 24/1000, Train Loss: 35.43108853426847, Valid Loss: 36.71075312296549\n","Epoch: 25/1000, Train Loss: 35.42845431241122, Valid Loss: 36.70934168497721\n","Epoch: 26/1000, Train Loss: 35.42328470403498, Valid Loss: 36.70855458577474\n","Epoch: 27/1000, Train Loss: 35.42764178189364, Valid Loss: 36.70807139078776\n","Epoch: 28/1000, Train Loss: 35.435921062122695, Valid Loss: 36.70780817667643\n","Epoch: 29/1000, Train Loss: 35.42968021739613, Valid Loss: 36.70785140991211\n","Epoch: 30/1000, Train Loss: 35.43245731700551, Valid Loss: 36.70855458577474\n","Epoch: 31/1000, Train Loss: 35.427072351629086, Valid Loss: 36.70783996582031\n","Epoch: 32/1000, Train Loss: 35.42601880160245, Valid Loss: 36.708251953125\n","Epoch: 33/1000, Train Loss: 35.438644062389024, Valid Loss: 36.707586924235024\n","Epoch: 34/1000, Train Loss: 35.44679919156161, Valid Loss: 36.7077267964681\n","Epoch: 35/1000, Train Loss: 35.42631634798917, Valid Loss: 36.707584381103516\n","Epoch: 36/1000, Train Loss: 35.442940798672765, Valid Loss: 36.70708465576172\n","Epoch: 37/1000, Train Loss: 35.42897761951793, Valid Loss: 36.70675150553385\n","Epoch: 38/1000, Train Loss: 35.43017786199396, Valid Loss: 36.707550048828125\n","Epoch: 39/1000, Train Loss: 35.441808527166195, Valid Loss: 36.7087033589681\n","Epoch: 40/1000, Train Loss: 35.42411492087624, Valid Loss: 36.70766067504883\n","Epoch: 41/1000, Train Loss: 35.439238808371805, Valid Loss: 36.70805231730143\n","Epoch: 42/1000, Train Loss: 35.42807180231268, Valid Loss: 36.70759963989258\n","Epoch: 43/1000, Train Loss: 35.42623450539329, Valid Loss: 36.7072385152181\n","Epoch: 44/1000, Train Loss: 35.43039772727273, Valid Loss: 36.70677185058594\n","Epoch: 45/1000, Train Loss: 35.43621236627752, Valid Loss: 36.70724360148112\n","Epoch: 46/1000, Train Loss: 35.42925054376776, Valid Loss: 36.70740254720052\n","Epoch: 47/1000, Train Loss: 35.42453106966886, Valid Loss: 36.70730336507162\n","Epoch: 48/1000, Train Loss: 35.43293658169833, Valid Loss: 36.706905364990234\n","Epoch: 49/1000, Train Loss: 35.42299548062411, Valid Loss: 36.70746612548828\n","Epoch: 50/1000, Train Loss: 35.438486619429156, Valid Loss: 36.707478841145836\n","Epoch: 51/1000, Train Loss: 35.42752040516246, Valid Loss: 36.708282470703125\n","Epoch: 52/1000, Train Loss: 35.43283462524414, Valid Loss: 36.70714441935221\n","Epoch: 53/1000, Train Loss: 35.43001105568626, Valid Loss: 36.707619984944664\n","Epoch: 54/1000, Train Loss: 35.42653517289595, Valid Loss: 36.70804341634115\n","Epoch: 55/1000, Train Loss: 35.43848731301048, Valid Loss: 36.70846048990885\n","Epoch: 56/1000, Train Loss: 35.435857252641156, Valid Loss: 36.70696894327799\n","Epoch: 57/1000, Train Loss: 35.431256380948156, Valid Loss: 36.706398010253906\n","Epoch: 58/1000, Train Loss: 35.43128897927024, Valid Loss: 36.70688247680664\n","Epoch: 59/1000, Train Loss: 35.4450139132413, Valid Loss: 36.70752843221029\n","Epoch: 60/1000, Train Loss: 35.442285017533735, Valid Loss: 36.70725123087565\n","Epoch: 61/1000, Train Loss: 35.440767461603336, Valid Loss: 36.70654805501302\n","Epoch: 62/1000, Train Loss: 35.431719693270594, Valid Loss: 36.70729319254557\n","Epoch: 63/1000, Train Loss: 35.433161302046344, Valid Loss: 36.70706049601237\n","Epoch: 64/1000, Train Loss: 35.44700137051669, Valid Loss: 36.707244873046875\n","Epoch: 65/1000, Train Loss: 35.43543832952326, Valid Loss: 36.70701599121094\n","Epoch: 66/1000, Train Loss: 35.43202799016779, Valid Loss: 36.70686340332031\n","Epoch: 67/1000, Train Loss: 35.41835403442383, Valid Loss: 36.707497914632164\n","Epoch: 68/1000, Train Loss: 35.43935845114968, Valid Loss: 36.70906194051107\n","Epoch: 69/1000, Train Loss: 35.444911609996446, Valid Loss: 36.707045237223305\n","Epoch: 70/1000, Train Loss: 35.423703627152875, Valid Loss: 36.70693588256836\n","Epoch: 71/1000, Train Loss: 35.42113633589311, Valid Loss: 36.70742925008138\n","Epoch: 72/1000, Train Loss: 35.43705922907049, Valid Loss: 36.707969665527344\n","Epoch: 73/1000, Train Loss: 35.42951861294833, Valid Loss: 36.70840835571289\n","Epoch: 74/1000, Train Loss: 35.43338949030096, Valid Loss: 36.70929845174154\n","Epoch: 75/1000, Train Loss: 35.42002383145419, Valid Loss: 36.70891189575195\n","Epoch: 76/1000, Train Loss: 35.41938365589488, Valid Loss: 36.70978673299154\n","Epoch: 77/1000, Train Loss: 35.42826947298917, Valid Loss: 36.71515401204427\n","Epoch: 78/1000, Train Loss: 35.41515974564986, Valid Loss: 36.72546641031901\n","Epoch: 79/1000, Train Loss: 35.411236849698156, Valid Loss: 36.749610900878906\n","Epoch: 80/1000, Train Loss: 35.42796395041726, Valid Loss: 36.81122843424479\n","Epoch: 81/1000, Train Loss: 35.41666169600053, Valid Loss: 36.89482625325521\n","Epoch: 82/1000, Train Loss: 35.393694097345524, Valid Loss: 36.95806757609049\n","Epoch: 83/1000, Train Loss: 35.39362335205078, Valid Loss: 37.011783599853516\n","Epoch: 84/1000, Train Loss: 35.3779296875, Valid Loss: 36.95269775390625\n","Epoch: 85/1000, Train Loss: 35.3645300431685, Valid Loss: 37.01316706339518\n","Epoch: 86/1000, Train Loss: 35.37223642522638, Valid Loss: 37.03443908691406\n","Epoch: 87/1000, Train Loss: 35.358659917658024, Valid Loss: 36.99842325846354\n","Epoch: 88/1000, Train Loss: 35.36665344238281, Valid Loss: 37.00387954711914\n","Epoch: 89/1000, Train Loss: 35.34576207941229, Valid Loss: 36.99422073364258\n","Epoch: 90/1000, Train Loss: 35.32597732543945, Valid Loss: 36.951847076416016\n","Epoch: 91/1000, Train Loss: 35.299931959672406, Valid Loss: 36.96400705973307\n","Epoch: 92/1000, Train Loss: 35.275048689408735, Valid Loss: 36.92605845133463\n","Epoch: 93/1000, Train Loss: 35.26320266723633, Valid Loss: 36.94926834106445\n","Epoch: 94/1000, Train Loss: 34.994389273903586, Valid Loss: 36.60565058390299\n","Epoch: 95/1000, Train Loss: 34.34449213201349, Valid Loss: 35.913953145345054\n","Epoch: 96/1000, Train Loss: 33.45886178450151, Valid Loss: 35.450992584228516\n","Epoch: 97/1000, Train Loss: 32.73590590737083, Valid Loss: 34.8684933980306\n","Epoch: 98/1000, Train Loss: 32.04607339338823, Valid Loss: 34.68200429280599\n","Epoch: 99/1000, Train Loss: 31.417151537808504, Valid Loss: 34.52073415120443\n","Epoch: 100/1000, Train Loss: 30.75643643465909, Valid Loss: 34.37344614664713\n","Epoch: 101/1000, Train Loss: 30.33445236899636, Valid Loss: 34.22183481852213\n","Epoch: 102/1000, Train Loss: 29.966257095336914, Valid Loss: 34.171644846598305\n","Epoch: 103/1000, Train Loss: 29.382993177934125, Valid Loss: 34.12695566813151\n","Epoch: 104/1000, Train Loss: 29.020748832009055, Valid Loss: 34.2162233988444\n","Epoch: 105/1000, Train Loss: 28.498768199573863, Valid Loss: 34.55789438883463\n","Epoch: 106/1000, Train Loss: 28.06581583890048, Valid Loss: 34.71596145629883\n","Epoch: 107/1000, Train Loss: 27.664541071111504, Valid Loss: 34.77658462524414\n","Epoch: 108/1000, Train Loss: 27.156123074618254, Valid Loss: 35.13780212402344\n","Epoch: 109/1000, Train Loss: 27.154321150346235, Valid Loss: 34.87822469075521\n","Epoch: 110/1000, Train Loss: 26.481537212025035, Valid Loss: 35.23396428426107\n","Epoch: 111/1000, Train Loss: 25.869638963179156, Valid Loss: 35.78474044799805\n","Epoch: 112/1000, Train Loss: 26.00319376858798, Valid Loss: 35.34715143839518\n","Epoch: 113/1000, Train Loss: 25.07130206714977, Valid Loss: 35.65117518107096\n","Epoch: 114/1000, Train Loss: 24.575518174604937, Valid Loss: 35.52485020955404\n","Epoch: 115/1000, Train Loss: 24.347281195900656, Valid Loss: 36.023993174235024\n","Epoch: 116/1000, Train Loss: 23.737778056751598, Valid Loss: 35.851036071777344\n","Epoch: 117/1000, Train Loss: 23.055585341020063, Valid Loss: 36.497870127360024\n","Epoch: 118/1000, Train Loss: 23.132963180541992, Valid Loss: 36.113747914632164\n","Epoch: 119/1000, Train Loss: 22.513406580144707, Valid Loss: 36.59745534261068\n","Epoch: 120/1000, Train Loss: 22.1046818819913, Valid Loss: 36.328234354654946\n","Epoch: 121/1000, Train Loss: 21.44153456254439, Valid Loss: 36.743028004964195\n","Epoch: 122/1000, Train Loss: 21.219430056485262, Valid Loss: 36.602246602376304\n","Epoch: 123/1000, Train Loss: 20.62366450916637, Valid Loss: 36.804892222086586\n","Epoch: 124/1000, Train Loss: 20.100448088212445, Valid Loss: 37.0257682800293\n","Epoch: 125/1000, Train Loss: 19.6100077195601, Valid Loss: 37.179449717203774\n","Epoch: 126/1000, Train Loss: 19.32408280806108, Valid Loss: 37.14317830403646\n","Epoch: 127/1000, Train Loss: 19.11637392911044, Valid Loss: 37.47542826334635\n","Epoch: 128/1000, Train Loss: 18.958153811368074, Valid Loss: 37.46626536051432\n","Epoch: 129/1000, Train Loss: 18.518582604148172, Valid Loss: 37.362412770589195\n","Epoch: 130/1000, Train Loss: 18.260826977816496, Valid Loss: 37.798126220703125\n","Epoch: 131/1000, Train Loss: 17.54418286410245, Valid Loss: 37.39732233683268\n","Epoch: 132/1000, Train Loss: 17.2986573305997, Valid Loss: 37.40679931640625\n","Epoch: 133/1000, Train Loss: 16.900569742376153, Valid Loss: 37.8070182800293\n","Epoch: 134/1000, Train Loss: 17.088986830277875, Valid Loss: 38.06201299031576\n","Epoch: 135/1000, Train Loss: 16.354532501914285, Valid Loss: 38.047594706217446\n","Epoch: 136/1000, Train Loss: 16.00576686859131, Valid Loss: 37.992305755615234\n","Epoch: 137/1000, Train Loss: 15.805763244628906, Valid Loss: 38.21379089355469\n","Epoch: 138/1000, Train Loss: 15.827676599675959, Valid Loss: 38.12869135538737\n","Epoch: 139/1000, Train Loss: 15.90965799851851, Valid Loss: 38.28304799397787\n","Epoch: 140/1000, Train Loss: 14.89074438268488, Valid Loss: 38.207776387532554\n","Epoch: 141/1000, Train Loss: 15.137656732039018, Valid Loss: 38.129372914632164\n","Epoch: 142/1000, Train Loss: 14.459600275213068, Valid Loss: 38.303314208984375\n","Epoch: 143/1000, Train Loss: 14.394725972955877, Valid Loss: 38.376572926839195\n","Epoch: 144/1000, Train Loss: 14.271651094610041, Valid Loss: 38.35387674967448\n","Epoch: 145/1000, Train Loss: 14.177230748263272, Valid Loss: 38.358238220214844\n","Epoch: 146/1000, Train Loss: 14.151320544156162, Valid Loss: 38.27656173706055\n","Epoch: 147/1000, Train Loss: 13.595802393826572, Valid Loss: 38.2803586324056\n","Epoch: 148/1000, Train Loss: 13.466888254339045, Valid Loss: 38.438254038492836\n","Epoch: 149/1000, Train Loss: 13.232503457502885, Valid Loss: 38.62244669596354\n","Epoch: 150/1000, Train Loss: 13.016860008239746, Valid Loss: 38.467969258626304\n","Epoch: 151/1000, Train Loss: 13.025962829589844, Valid Loss: 38.38562266031901\n","Epoch: 152/1000, Train Loss: 12.80739012631503, Valid Loss: 38.4791145324707\n","Epoch: 153/1000, Train Loss: 12.689180374145508, Valid Loss: 38.682074228922524\n","Epoch: 154/1000, Train Loss: 12.666304414922541, Valid Loss: 38.58205795288086\n","Epoch: 155/1000, Train Loss: 12.301112521778453, Valid Loss: 38.40984344482422\n","Epoch: 156/1000, Train Loss: 12.199654232371937, Valid Loss: 38.260729471842446\n","Epoch: 157/1000, Train Loss: 12.00448955189098, Valid Loss: 38.572147369384766\n","Epoch: 158/1000, Train Loss: 12.217803521589799, Valid Loss: 38.61737314860026\n","Epoch: 159/1000, Train Loss: 12.05954716422341, Valid Loss: 38.446885426839195\n","Epoch: 160/1000, Train Loss: 11.98410328951749, Valid Loss: 38.4138069152832\n","Epoch: 161/1000, Train Loss: 11.612153573469682, Valid Loss: 38.404335021972656\n","Epoch: 162/1000, Train Loss: 11.815817312760787, Valid Loss: 38.49898147583008\n","Epoch: 163/1000, Train Loss: 12.080816008827902, Valid Loss: 38.47625478108724\n","Epoch: 164/1000, Train Loss: 11.763058055530895, Valid Loss: 38.4825185139974\n","Epoch: 165/1000, Train Loss: 11.370182557539506, Valid Loss: 38.87551625569662\n","Epoch: 166/1000, Train Loss: 11.497455250133168, Valid Loss: 38.731396993001304\n","Epoch: 167/1000, Train Loss: 11.109732454473322, Valid Loss: 38.776641845703125\n","Epoch: 168/1000, Train Loss: 11.199332930824973, Valid Loss: 38.75491078694662\n","Epoch: 169/1000, Train Loss: 10.956276460127397, Valid Loss: 38.53187942504883\n","Epoch: 170/1000, Train Loss: 10.992261106317693, Valid Loss: 38.416131337483726\n","Epoch: 171/1000, Train Loss: 10.882929195057262, Valid Loss: 38.64965057373047\n","Epoch: 172/1000, Train Loss: 10.713356018066406, Valid Loss: 38.680169423421226\n","Epoch: 173/1000, Train Loss: 10.645103974775834, Valid Loss: 38.632928212483726\n","Epoch: 174/1000, Train Loss: 10.65633114901456, Valid Loss: 38.61108271280924\n","Epoch: 175/1000, Train Loss: 11.109479644081809, Valid Loss: 38.56647745768229\n","Epoch: 176/1000, Train Loss: 10.517401261763139, Valid Loss: 38.62085978190104\n","Epoch: 177/1000, Train Loss: 10.702476414767178, Valid Loss: 38.5142453511556\n","Epoch: 178/1000, Train Loss: 10.073224674571644, Valid Loss: 38.650988260904946\n","Epoch: 179/1000, Train Loss: 10.222155831076883, Valid Loss: 38.69793955485026\n","Epoch: 180/1000, Train Loss: 10.081082430752842, Valid Loss: 38.76362991333008\n","Epoch: 181/1000, Train Loss: 10.080696279352361, Valid Loss: 38.716373443603516\n","Epoch: 182/1000, Train Loss: 10.336925766684793, Valid Loss: 38.621771494547524\n","Epoch: 183/1000, Train Loss: 10.122313412753018, Valid Loss: 38.62695439656576\n","Epoch: 184/1000, Train Loss: 10.067574240944602, Valid Loss: 38.62083943684896\n","Epoch: 185/1000, Train Loss: 10.273508592085404, Valid Loss: 38.74803924560547\n","Epoch: 186/1000, Train Loss: 10.120067249644887, Valid Loss: 38.57574335734049\n","Epoch: 187/1000, Train Loss: 10.021112702109598, Valid Loss: 38.72763697306315\n","Epoch: 188/1000, Train Loss: 9.914045073769309, Valid Loss: 38.501546223958336\n","Epoch: 189/1000, Train Loss: 9.493248332630504, Valid Loss: 38.74611790974935\n","Epoch: 190/1000, Train Loss: 9.564067493785512, Valid Loss: 38.5213254292806\n","Epoch: 191/1000, Train Loss: 9.669825813987039, Valid Loss: 38.70603942871094\n","Epoch: 192/1000, Train Loss: 9.892530874772506, Valid Loss: 38.84806696573893\n","Epoch: 193/1000, Train Loss: 9.85256273096258, Valid Loss: 38.53262710571289\n","Epoch: 194/1000, Train Loss: 9.47116435657848, Valid Loss: 38.81981531778971\n","Epoch: 195/1000, Train Loss: 9.546699437228115, Valid Loss: 38.899575551350914\n","Epoch: 196/1000, Train Loss: 9.508702278137207, Valid Loss: 38.73615646362305\n","Epoch: 197/1000, Train Loss: 9.566223231228916, Valid Loss: 38.764330546061196\n","Epoch: 198/1000, Train Loss: 9.39015379819003, Valid Loss: 38.669081370035805\n","Epoch: 199/1000, Train Loss: 9.223034598610617, Valid Loss: 38.69226582845052\n","Epoch: 200/1000, Train Loss: 9.362020752646707, Valid Loss: 38.74225870768229\n","Epoch: 201/1000, Train Loss: 9.270815762606533, Valid Loss: 38.538875579833984\n","Epoch: 202/1000, Train Loss: 9.12367803400213, Valid Loss: 38.66919072469076\n","Epoch: 203/1000, Train Loss: 9.107902613553135, Valid Loss: 38.6241455078125\n","얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 203에서 훈련 중단.\n","Training Start: HLM\n","Epoch: 0/1000, Train Loss: 63.81646000255238, Valid Loss: 60.99911244710287\n","Epoch: 1/1000, Train Loss: 60.292297016490586, Valid Loss: 56.83222198486328\n","Epoch: 2/1000, Train Loss: 56.979639226740055, Valid Loss: 54.52107493082682\n","Epoch: 3/1000, Train Loss: 54.84275575117631, Valid Loss: 52.541829427083336\n","Epoch: 4/1000, Train Loss: 52.90906316583807, Valid Loss: 50.68336613972982\n","Epoch: 5/1000, Train Loss: 51.08409361405806, Valid Loss: 48.95961380004883\n","Epoch: 6/1000, Train Loss: 49.38378282026811, Valid Loss: 47.35259882609049\n","Epoch: 7/1000, Train Loss: 47.81468651511452, Valid Loss: 45.87133280436198\n","Epoch: 8/1000, Train Loss: 46.371207150545985, Valid Loss: 44.51483917236328\n","Epoch: 9/1000, Train Loss: 45.012400193647906, Valid Loss: 43.24986775716146\n","Epoch: 10/1000, Train Loss: 43.75344467163086, Valid Loss: 42.10564168294271\n","Epoch: 11/1000, Train Loss: 42.622108459472656, Valid Loss: 41.06823603312174\n","Epoch: 12/1000, Train Loss: 41.5914473100142, Valid Loss: 40.152539571126304\n","Epoch: 13/1000, Train Loss: 40.68598799272017, Valid Loss: 39.363836924235024\n","Epoch: 14/1000, Train Loss: 39.89722650701349, Valid Loss: 38.68331654866537\n","Epoch: 15/1000, Train Loss: 39.230988242409445, Valid Loss: 38.095986684163414\n","Epoch: 16/1000, Train Loss: 38.64569022438743, Valid Loss: 37.59976704915365\n","Epoch: 17/1000, Train Loss: 38.158911965110086, Valid Loss: 37.18791071573893\n","Epoch: 18/1000, Train Loss: 37.730958765203304, Valid Loss: 36.84291966756185\n","Epoch: 19/1000, Train Loss: 37.36629798195579, Valid Loss: 36.568546295166016\n","Epoch: 20/1000, Train Loss: 37.09455802223899, Valid Loss: 36.34884516398112\n","Epoch: 21/1000, Train Loss: 36.854473460804336, Valid Loss: 36.18499247233073\n","Epoch: 22/1000, Train Loss: 36.67193672873757, Valid Loss: 36.058021545410156\n","Epoch: 23/1000, Train Loss: 36.532940951260656, Valid Loss: 35.96259562174479\n","Epoch: 24/1000, Train Loss: 36.44416878440163, Valid Loss: 35.890541076660156\n","Epoch: 25/1000, Train Loss: 36.35752001675692, Valid Loss: 35.84191004435221\n","Epoch: 26/1000, Train Loss: 36.27967938509855, Valid Loss: 35.80559412638346\n","Epoch: 27/1000, Train Loss: 36.23734179410067, Valid Loss: 35.78164545694987\n","Epoch: 28/1000, Train Loss: 36.21244812011719, Valid Loss: 35.76666005452474\n","Epoch: 29/1000, Train Loss: 36.17124522816051, Valid Loss: 35.75737508138021\n","Epoch: 30/1000, Train Loss: 36.16442454944957, Valid Loss: 35.751312255859375\n","Epoch: 31/1000, Train Loss: 36.148414611816406, Valid Loss: 35.74934005737305\n","Epoch: 32/1000, Train Loss: 36.14598950472745, Valid Loss: 35.749123891194664\n","Epoch: 33/1000, Train Loss: 36.12748579545455, Valid Loss: 35.750040690104164\n","Epoch: 34/1000, Train Loss: 36.12838224931197, Valid Loss: 35.75134404500326\n","Epoch: 35/1000, Train Loss: 36.10943950306285, Valid Loss: 35.7534065246582\n","Epoch: 36/1000, Train Loss: 36.1276689009233, Valid Loss: 35.75539906819662\n","Epoch: 37/1000, Train Loss: 36.11031653664329, Valid Loss: 35.7571907043457\n","Epoch: 38/1000, Train Loss: 36.120360287753016, Valid Loss: 35.75926971435547\n","Epoch: 39/1000, Train Loss: 36.11746458573775, Valid Loss: 35.76045481363932\n","Epoch: 40/1000, Train Loss: 36.11705710671165, Valid Loss: 35.7620849609375\n","Epoch: 41/1000, Train Loss: 36.11467118696733, Valid Loss: 35.76371383666992\n","Epoch: 42/1000, Train Loss: 36.11636040427468, Valid Loss: 35.76314036051432\n","Epoch: 43/1000, Train Loss: 36.120262839577414, Valid Loss: 35.764113108317055\n","Epoch: 44/1000, Train Loss: 36.10835335471413, Valid Loss: 35.76568476359049\n","Epoch: 45/1000, Train Loss: 36.11289284446023, Valid Loss: 35.76667277018229\n","Epoch: 46/1000, Train Loss: 36.12279545177113, Valid Loss: 35.76670710245768\n","Epoch: 47/1000, Train Loss: 36.11333534934304, Valid Loss: 35.76601537068685\n","Epoch: 48/1000, Train Loss: 36.121347947554156, Valid Loss: 35.76797231038412\n","Epoch: 49/1000, Train Loss: 36.104644775390625, Valid Loss: 35.766961415608726\n","Epoch: 50/1000, Train Loss: 36.095756183971055, Valid Loss: 35.76627985636393\n","Epoch: 51/1000, Train Loss: 36.11700647527521, Valid Loss: 35.768333435058594\n","Epoch: 52/1000, Train Loss: 36.118417566472836, Valid Loss: 35.768697102864586\n","Epoch: 53/1000, Train Loss: 36.11114293878729, Valid Loss: 35.76801045735677\n","Epoch: 54/1000, Train Loss: 36.12108542702415, Valid Loss: 35.76630401611328\n","Epoch: 55/1000, Train Loss: 36.12331112948331, Valid Loss: 35.76691182454427\n","Epoch: 56/1000, Train Loss: 36.120528828014024, Valid Loss: 35.76810963948568\n","Epoch: 57/1000, Train Loss: 36.11833641745827, Valid Loss: 35.770301818847656\n","Epoch: 58/1000, Train Loss: 36.11981825395064, Valid Loss: 35.770057678222656\n","Epoch: 59/1000, Train Loss: 36.1174729087136, Valid Loss: 35.770033518473305\n","Epoch: 60/1000, Train Loss: 36.12599008733576, Valid Loss: 35.76860555013021\n","Epoch: 61/1000, Train Loss: 36.11787969415838, Valid Loss: 35.76884078979492\n","Epoch: 62/1000, Train Loss: 36.10217215798118, Valid Loss: 35.7693837483724\n","Epoch: 63/1000, Train Loss: 36.10949013449929, Valid Loss: 35.76744079589844\n","Epoch: 64/1000, Train Loss: 36.113948822021484, Valid Loss: 35.767321268717446\n","Epoch: 65/1000, Train Loss: 36.110908855091445, Valid Loss: 35.767904917399086\n","Epoch: 66/1000, Train Loss: 36.12603551691229, Valid Loss: 35.76813252766927\n","Epoch: 67/1000, Train Loss: 36.118834408846766, Valid Loss: 35.76714960734049\n","Epoch: 68/1000, Train Loss: 36.117718089710586, Valid Loss: 35.76958211263021\n","Epoch: 69/1000, Train Loss: 36.100525249134414, Valid Loss: 35.76851018269857\n","Epoch: 70/1000, Train Loss: 36.11451409079812, Valid Loss: 35.770835876464844\n","Epoch: 71/1000, Train Loss: 36.10868939486417, Valid Loss: 35.76966222127279\n","Epoch: 72/1000, Train Loss: 36.12307461825284, Valid Loss: 35.767860412597656\n","Epoch: 73/1000, Train Loss: 36.10407985340465, Valid Loss: 35.76806131998698\n","Epoch: 74/1000, Train Loss: 36.12109860506925, Valid Loss: 35.76814397176107\n","Epoch: 75/1000, Train Loss: 36.12291127985174, Valid Loss: 35.76941808064779\n","Epoch: 76/1000, Train Loss: 36.10409927368164, Valid Loss: 35.76972961425781\n","Epoch: 77/1000, Train Loss: 36.120771234685726, Valid Loss: 35.76989618937174\n","Epoch: 78/1000, Train Loss: 36.12243513627486, Valid Loss: 35.76784896850586\n","Epoch: 79/1000, Train Loss: 36.124509291215375, Valid Loss: 35.769823710123696\n","Epoch: 80/1000, Train Loss: 36.1248945756392, Valid Loss: 35.771010080973305\n","Epoch: 81/1000, Train Loss: 36.11370433460582, Valid Loss: 35.76978556315104\n","Epoch: 82/1000, Train Loss: 36.10388322310014, Valid Loss: 35.77818171183268\n","Epoch: 83/1000, Train Loss: 36.11110930009322, Valid Loss: 35.79868189493815\n","Epoch: 84/1000, Train Loss: 36.10623897205699, Valid Loss: 35.8647715250651\n","Epoch: 85/1000, Train Loss: 36.09031919999556, Valid Loss: 36.119529724121094\n","Epoch: 86/1000, Train Loss: 36.0801887512207, Valid Loss: 36.20259348551432\n","Epoch: 87/1000, Train Loss: 36.06646381724965, Valid Loss: 36.21992874145508\n","Epoch: 88/1000, Train Loss: 36.04457092285156, Valid Loss: 36.20358657836914\n","Epoch: 89/1000, Train Loss: 36.04551766135476, Valid Loss: 36.216957092285156\n","Epoch: 90/1000, Train Loss: 36.040001609108664, Valid Loss: 36.23155212402344\n","Epoch: 91/1000, Train Loss: 36.033682042902164, Valid Loss: 36.24633534749349\n","Epoch: 92/1000, Train Loss: 36.03687251697887, Valid Loss: 36.219818115234375\n","Epoch: 93/1000, Train Loss: 36.018644853071734, Valid Loss: 36.23112869262695\n","Epoch: 94/1000, Train Loss: 36.01582163030451, Valid Loss: 36.249837239583336\n","Epoch: 95/1000, Train Loss: 36.016723979603164, Valid Loss: 36.231201171875\n","Epoch: 96/1000, Train Loss: 36.00844261863015, Valid Loss: 36.23032760620117\n","Epoch: 97/1000, Train Loss: 36.017363114790484, Valid Loss: 36.25130844116211\n","Epoch: 98/1000, Train Loss: 36.00511446866122, Valid Loss: 36.2718505859375\n","Epoch: 99/1000, Train Loss: 35.99226760864258, Valid Loss: 36.308817545572914\n","Epoch: 100/1000, Train Loss: 35.99082287875089, Valid Loss: 36.30918502807617\n","Epoch: 101/1000, Train Loss: 35.968325875022195, Valid Loss: 36.29789606730143\n","Epoch: 102/1000, Train Loss: 35.94770431518555, Valid Loss: 36.258402506510414\n","Epoch: 103/1000, Train Loss: 35.897207433527164, Valid Loss: 36.249090830485024\n","Epoch: 104/1000, Train Loss: 35.80302880027077, Valid Loss: 36.19568634033203\n","Epoch: 105/1000, Train Loss: 35.60702167857777, Valid Loss: 36.08725611368815\n","Epoch: 106/1000, Train Loss: 35.42237715287642, Valid Loss: 35.94282023111979\n","Epoch: 107/1000, Train Loss: 35.252763921564274, Valid Loss: 35.71776453653971\n","Epoch: 108/1000, Train Loss: 34.990581859241836, Valid Loss: 35.61390813191732\n","Epoch: 109/1000, Train Loss: 34.66853922063654, Valid Loss: 35.42188262939453\n","Epoch: 110/1000, Train Loss: 34.39477261629972, Valid Loss: 35.31003061930338\n","Epoch: 111/1000, Train Loss: 34.008150274103336, Valid Loss: 35.16018931070963\n","Epoch: 112/1000, Train Loss: 33.71173511851918, Valid Loss: 34.915200551350914\n","Epoch: 113/1000, Train Loss: 33.16884335604581, Valid Loss: 34.56245422363281\n","Epoch: 114/1000, Train Loss: 32.349961194125086, Valid Loss: 34.22966003417969\n","Epoch: 115/1000, Train Loss: 31.6312049518932, Valid Loss: 33.97670237223307\n","Epoch: 116/1000, Train Loss: 31.06561244617809, Valid Loss: 33.924233754475914\n","Epoch: 117/1000, Train Loss: 30.561594009399414, Valid Loss: 34.00334803263346\n","Epoch: 118/1000, Train Loss: 30.20068324695934, Valid Loss: 34.29629643758138\n","Epoch: 119/1000, Train Loss: 29.89169762351296, Valid Loss: 33.98246637980143\n","Epoch: 120/1000, Train Loss: 29.387996326793324, Valid Loss: 34.06518046061198\n","Epoch: 121/1000, Train Loss: 28.87530621615323, Valid Loss: 34.31733194986979\n","Epoch: 122/1000, Train Loss: 28.538559306751598, Valid Loss: 34.13493092854818\n","Epoch: 123/1000, Train Loss: 27.978375174782492, Valid Loss: 34.075592041015625\n","Epoch: 124/1000, Train Loss: 27.874232378872957, Valid Loss: 34.35938008626302\n","Epoch: 125/1000, Train Loss: 27.452631690285422, Valid Loss: 34.49781163533529\n","Epoch: 126/1000, Train Loss: 27.229538310657848, Valid Loss: 34.50432586669922\n","Epoch: 127/1000, Train Loss: 26.534575549038973, Valid Loss: 34.54903920491537\n","Epoch: 128/1000, Train Loss: 26.250655607743695, Valid Loss: 34.94857660929362\n","Epoch: 129/1000, Train Loss: 25.756043000654742, Valid Loss: 35.11834208170573\n","Epoch: 130/1000, Train Loss: 25.275393052534625, Valid Loss: 34.94117101033529\n","Epoch: 131/1000, Train Loss: 24.981469414450906, Valid Loss: 35.038439432779946\n","Epoch: 132/1000, Train Loss: 24.60110022804954, Valid Loss: 35.59539667765299\n","Epoch: 133/1000, Train Loss: 24.19544081254439, Valid Loss: 35.63196309407552\n","Epoch: 134/1000, Train Loss: 23.7743258042769, Valid Loss: 35.65971883138021\n","Epoch: 135/1000, Train Loss: 23.182165839455344, Valid Loss: 35.74841435750326\n","Epoch: 136/1000, Train Loss: 22.73505939136852, Valid Loss: 35.74824651082357\n","Epoch: 137/1000, Train Loss: 22.650768106633965, Valid Loss: 35.84644444783529\n","Epoch: 138/1000, Train Loss: 22.164548700506035, Valid Loss: 35.86464055379232\n","Epoch: 139/1000, Train Loss: 21.887257662686434, Valid Loss: 36.05412928263346\n","Epoch: 140/1000, Train Loss: 21.023976065895774, Valid Loss: 36.25067647298177\n","Epoch: 141/1000, Train Loss: 20.913385911421344, Valid Loss: 36.50336583455404\n","Epoch: 142/1000, Train Loss: 20.42270816456188, Valid Loss: 36.58583450317383\n","Epoch: 143/1000, Train Loss: 20.156428770585492, Valid Loss: 36.69513066609701\n","Epoch: 144/1000, Train Loss: 19.38041184165261, Valid Loss: 36.86874008178711\n","Epoch: 145/1000, Train Loss: 19.78876443342729, Valid Loss: 36.761706034342446\n","Epoch: 146/1000, Train Loss: 18.964106993241742, Valid Loss: 36.9250742594401\n","Epoch: 147/1000, Train Loss: 18.619812185114082, Valid Loss: 37.070699055989586\n","Epoch: 148/1000, Train Loss: 18.361953128467906, Valid Loss: 36.97829055786133\n","Epoch: 149/1000, Train Loss: 18.140653610229492, Valid Loss: 37.00975036621094\n","Epoch: 150/1000, Train Loss: 17.753250122070312, Valid Loss: 37.18120829264323\n","Epoch: 151/1000, Train Loss: 17.2133771722967, Valid Loss: 37.14433288574219\n","Epoch: 152/1000, Train Loss: 16.928745529868387, Valid Loss: 37.24756749471029\n","Epoch: 153/1000, Train Loss: 16.800997387279164, Valid Loss: 37.40410868326823\n","Epoch: 154/1000, Train Loss: 16.832532622597434, Valid Loss: 37.37850697835287\n","Epoch: 155/1000, Train Loss: 16.976633938876066, Valid Loss: 37.56489944458008\n","Epoch: 156/1000, Train Loss: 15.887057651172984, Valid Loss: 37.54047648111979\n","Epoch: 157/1000, Train Loss: 15.961144187233664, Valid Loss: 37.38873418172201\n","Epoch: 158/1000, Train Loss: 15.540983200073242, Valid Loss: 37.63400904337565\n","Epoch: 159/1000, Train Loss: 15.439306952736594, Valid Loss: 37.829734802246094\n","Epoch: 160/1000, Train Loss: 15.463155139576305, Valid Loss: 37.73898696899414\n","Epoch: 161/1000, Train Loss: 15.001093864440918, Valid Loss: 37.57878494262695\n","Epoch: 162/1000, Train Loss: 15.397298812866211, Valid Loss: 37.80223592122396\n","Epoch: 163/1000, Train Loss: 15.176821275190873, Valid Loss: 37.9912961324056\n","Epoch: 164/1000, Train Loss: 14.817534706809305, Valid Loss: 37.29875691731771\n","Epoch: 165/1000, Train Loss: 14.617419069463557, Valid Loss: 37.390357971191406\n","Epoch: 166/1000, Train Loss: 14.157232197848233, Valid Loss: 37.72181193033854\n","Epoch: 167/1000, Train Loss: 13.850241054188121, Valid Loss: 37.860432942708336\n","Epoch: 168/1000, Train Loss: 13.876664941961115, Valid Loss: 37.68936665852865\n","Epoch: 169/1000, Train Loss: 13.635375543074174, Valid Loss: 37.7898203531901\n","Epoch: 170/1000, Train Loss: 13.84242075139826, Valid Loss: 37.75690460205078\n","Epoch: 171/1000, Train Loss: 13.424558292735707, Valid Loss: 37.88758977254232\n","Epoch: 172/1000, Train Loss: 13.547070243141867, Valid Loss: 38.018026987711586\n","Epoch: 173/1000, Train Loss: 12.90814191644842, Valid Loss: 38.04762649536133\n","Epoch: 174/1000, Train Loss: 13.288792436773127, Valid Loss: 37.699537913004555\n","Epoch: 175/1000, Train Loss: 12.854655092412775, Valid Loss: 38.018988291422524\n","Epoch: 176/1000, Train Loss: 12.744560068303889, Valid Loss: 37.92473347981771\n","Epoch: 177/1000, Train Loss: 12.529875755310059, Valid Loss: 37.999202728271484\n","Epoch: 178/1000, Train Loss: 12.573337468233975, Valid Loss: 38.06520334879557\n","Epoch: 179/1000, Train Loss: 12.265314709056508, Valid Loss: 37.9264767964681\n","Epoch: 180/1000, Train Loss: 12.527899742126465, Valid Loss: 38.073988596598305\n","Epoch: 181/1000, Train Loss: 12.453625332225453, Valid Loss: 38.20353698730469\n","Epoch: 182/1000, Train Loss: 12.270606127652256, Valid Loss: 38.20532099405924\n","Epoch: 183/1000, Train Loss: 12.082263339649547, Valid Loss: 38.07375462849935\n","Epoch: 184/1000, Train Loss: 12.056381398981268, Valid Loss: 38.07782872517904\n","Epoch: 185/1000, Train Loss: 12.055686603892934, Valid Loss: 38.08141072591146\n","Epoch: 186/1000, Train Loss: 11.850032546303488, Valid Loss: 38.06425476074219\n","Epoch: 187/1000, Train Loss: 12.092808983542703, Valid Loss: 38.083229064941406\n","Epoch: 188/1000, Train Loss: 11.773311268199574, Valid Loss: 37.89665095011393\n","Epoch: 189/1000, Train Loss: 11.537209077314897, Valid Loss: 37.96282450358073\n","Epoch: 190/1000, Train Loss: 11.73673066225919, Valid Loss: 37.88643900553385\n","Epoch: 191/1000, Train Loss: 11.86897173794833, Valid Loss: 38.04999796549479\n","Epoch: 192/1000, Train Loss: 11.12220660122958, Valid Loss: 38.17430623372396\n","Epoch: 193/1000, Train Loss: 11.38801097869873, Valid Loss: 38.33807881673177\n","Epoch: 194/1000, Train Loss: 11.360175999728115, Valid Loss: 38.188350677490234\n","Epoch: 195/1000, Train Loss: 11.159968462857334, Valid Loss: 37.995174407958984\n","Epoch: 196/1000, Train Loss: 10.976654572920365, Valid Loss: 38.261854807535805\n","Epoch: 197/1000, Train Loss: 11.170891328291459, Valid Loss: 38.123772939046226\n","Epoch: 198/1000, Train Loss: 11.105635816400701, Valid Loss: 38.18673451741537\n","Epoch: 199/1000, Train Loss: 11.118571454828436, Valid Loss: 38.14281336466471\n","Epoch: 200/1000, Train Loss: 11.046855666420676, Valid Loss: 38.25340779622396\n","Epoch: 201/1000, Train Loss: 10.72595145485618, Valid Loss: 38.12346394856771\n","Epoch: 202/1000, Train Loss: 10.899300228465687, Valid Loss: 38.18827565511068\n","Epoch: 203/1000, Train Loss: 10.541311697526412, Valid Loss: 38.17201487223307\n","Epoch: 204/1000, Train Loss: 10.785962451588023, Valid Loss: 38.35324223836263\n","Epoch: 205/1000, Train Loss: 10.610436352816494, Valid Loss: 38.39413197835287\n","Epoch: 206/1000, Train Loss: 10.399416143243963, Valid Loss: 38.353370666503906\n","Epoch: 207/1000, Train Loss: 10.622355287725275, Valid Loss: 38.23541259765625\n","Epoch: 208/1000, Train Loss: 10.512075510892002, Valid Loss: 38.38289006551107\n","Epoch: 209/1000, Train Loss: 10.32926151969216, Valid Loss: 38.450927734375\n","Epoch: 210/1000, Train Loss: 10.340621341358531, Valid Loss: 38.159470876057945\n","Epoch: 211/1000, Train Loss: 10.273606993935324, Valid Loss: 38.11346689860026\n","Epoch: 212/1000, Train Loss: 9.960959607904607, Valid Loss: 38.20519510904948\n","Epoch: 213/1000, Train Loss: 10.143666354092685, Valid Loss: 38.23849105834961\n","Epoch: 214/1000, Train Loss: 10.025260145013982, Valid Loss: 38.07393137613932\n","Epoch: 215/1000, Train Loss: 10.048125527121805, Valid Loss: 38.04816563924154\n","Epoch: 216/1000, Train Loss: 10.192487629977139, Valid Loss: 38.28418223063151\n","얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 216에서 훈련 중단.\n"]}],"source":["print(\"Training Start: MLM\")\n","model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, epochs=CFG['EPOCHS'])\n","\n","print(\"Training Start: HLM\")\n","model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, epochs=CFG['EPOCHS'])"]},{"cell_type":"code","execution_count":72,"id":"cbd82248","metadata":{"id":"cbd82248","executionInfo":{"status":"ok","timestamp":1693240124209,"user_tz":-540,"elapsed":4,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["torch.save(model_MLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/Many-to-one_LSTM_Model_MLM_1.pth')  # 모델 객체의 state_dict 저장\n","torch.save(model_HLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/Many-to-one_LSTM_Model_HLM_1.pth')"]},{"cell_type":"code","execution_count":73,"id":"d9a52fd6","metadata":{"id":"d9a52fd6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693240125777,"user_tz":-540,"elapsed":2,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"b67f055a-5a8e-457b-edbf-475b3e1b00f5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":73}],"source":["model_MLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/Many-to-one_LSTM_Model_MLM_1.pth'))\n","model_HLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/Many-to-one_LSTM_Model_HLM_1.pth'))"]},{"cell_type":"code","execution_count":78,"id":"cbd8e060","metadata":{"id":"cbd8e060","executionInfo":{"status":"ok","timestamp":1693240168347,"user_tz":-540,"elapsed":288,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["test_MLM = CustomDataset(test, target_col=None, transform=transform, is_test=True)\n","test_HLM = CustomDataset(test, target_col=None, transform=transform, is_test=True)\n","\n","test_MLM_loader = DataLoader(dataset=test_MLM,\n","                             batch_size=CFG['BATCH_SIZE'],\n","                             shuffle=False)\n","\n","test_HLM_loader = DataLoader(dataset=test_HLM,\n","                             batch_size=CFG['BATCH_SIZE'],\n","                             shuffle=False)"]},{"cell_type":"code","execution_count":81,"id":"07796373","metadata":{"id":"07796373","executionInfo":{"status":"ok","timestamp":1693240217928,"user_tz":-540,"elapsed":321,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["def inference(test_loader, model):\n","    model.eval()\n","    preds = []\n","\n","    with torch.no_grad():\n","        for inputs in test_loader:\n","\n","            inputs = inputs.unsqueeze(1)  # (256, 251) -> (256, 1, 251)\n","\n","            output = model(inputs)\n","            preds.extend(output.cpu().numpy().flatten().tolist())\n","\n","    return preds"]},{"cell_type":"code","execution_count":82,"id":"2f9d73ac","metadata":{"id":"2f9d73ac","executionInfo":{"status":"ok","timestamp":1693240219445,"user_tz":-540,"elapsed":366,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["predictions_MLM = inference(test_MLM_loader, model_MLM)\n","predictions_HLM = inference(test_HLM_loader, model_HLM)"]},{"cell_type":"code","execution_count":83,"id":"d8aed1aa","metadata":{"id":"d8aed1aa","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1693240222951,"user_tz":-540,"elapsed":417,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"4420f18e-3017-4b9e-825e-38a26c758b69"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["           id        MLM        HLM\n","0    TEST_000  15.006695  61.335728\n","1    TEST_001  61.088417  70.075821\n","2    TEST_002  57.311104  87.030212\n","3    TEST_003  23.964069  72.702866\n","4    TEST_004  53.175064  80.534409\n","..        ...        ...        ...\n","478  TEST_478  34.738350  64.757706\n","479  TEST_479  92.616318  90.107727\n","480  TEST_480  36.332047  79.415115\n","481  TEST_481  32.527252  96.071335\n","482  TEST_482   6.581669  64.894760\n","\n","[483 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-9511d70c-545e-4640-9852-85709e5064d5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>MLM</th>\n","      <th>HLM</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TEST_000</td>\n","      <td>15.006695</td>\n","      <td>61.335728</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TEST_001</td>\n","      <td>61.088417</td>\n","      <td>70.075821</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TEST_002</td>\n","      <td>57.311104</td>\n","      <td>87.030212</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TEST_003</td>\n","      <td>23.964069</td>\n","      <td>72.702866</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TEST_004</td>\n","      <td>53.175064</td>\n","      <td>80.534409</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>TEST_478</td>\n","      <td>34.738350</td>\n","      <td>64.757706</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>TEST_479</td>\n","      <td>92.616318</td>\n","      <td>90.107727</td>\n","    </tr>\n","    <tr>\n","      <th>480</th>\n","      <td>TEST_480</td>\n","      <td>36.332047</td>\n","      <td>79.415115</td>\n","    </tr>\n","    <tr>\n","      <th>481</th>\n","      <td>TEST_481</td>\n","      <td>32.527252</td>\n","      <td>96.071335</td>\n","    </tr>\n","    <tr>\n","      <th>482</th>\n","      <td>TEST_482</td>\n","      <td>6.581669</td>\n","      <td>64.894760</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>483 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9511d70c-545e-4640-9852-85709e5064d5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9511d70c-545e-4640-9852-85709e5064d5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9511d70c-545e-4640-9852-85709e5064d5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-3af69b15-d0db-4305-9612-f41d14cd07df\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3af69b15-d0db-4305-9612-f41d14cd07df')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-3af69b15-d0db-4305-9612-f41d14cd07df button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":83}],"source":["submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/sample_submission.csv')\n","\n","submission['MLM'] = predictions_MLM\n","submission['HLM'] = predictions_HLM\n","submission"]},{"cell_type":"code","execution_count":84,"id":"1e2c41b4","metadata":{"id":"1e2c41b4","executionInfo":{"status":"ok","timestamp":1693240251783,"user_tz":-540,"elapsed":290,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["submission.to_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/submissions/Many-to-one_LSTM_Model_1_submission.csv', index=False)"]},{"cell_type":"code","source":[],"metadata":{"id":"tXZonM_X-9FT"},"id":"tXZonM_X-9FT","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}