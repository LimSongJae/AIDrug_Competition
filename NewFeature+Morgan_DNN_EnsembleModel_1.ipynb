{"cells":[{"cell_type":"code","execution_count":1,"id":"86cd3349","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7050,"status":"ok","timestamp":1693229278430,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"86cd3349","outputId":"f9d33f97-39d0-498c-b377-a7ceda9b29e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["import random\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","import missingno\n","\n","# device 설정\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print (device)\n","\n","seed = 42 # seed 값 설정\n","random.seed(seed) # 파이썬 난수 생성기\n","os.environ['PYTHONHASHSEED'] = str(seed) # 해시 시크릿값 고정\n","np.random.seed(seed) # 넘파이 난수 생성기\n","\n","torch.manual_seed(seed) # 파이토치 CPU 난수 생성기\n","torch.backends.cudnn.deterministic = True # 확정적 연산 사용 설정\n","torch.backends.cudnn.benchmark = False   # 벤치마크 기능 사용 해제\n","torch.backends.cudnn.enabled = False        # cudnn 기능 사용 해제\n","\n","if device == 'cuda':\n","    torch.cuda.manual_seed(seed) # 파이토치 GPU 난수 생성기\n","    torch.cuda.manual_seed_all(seed) # 파이토치 멀티 GPU 난수 생성기"]},{"cell_type":"code","execution_count":2,"id":"5a30f491","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24063,"status":"ok","timestamp":1693229302489,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"5a30f491","outputId":"9f3c3d68-acfe-40d4-d376-51038c206d30"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"id":"a2911bd2","metadata":{"id":"a2911bd2","executionInfo":{"status":"ok","timestamp":1693229306086,"user_tz":-540,"elapsed":3600,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/train.csv')\n","test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/test.csv')"]},{"cell_type":"code","execution_count":4,"id":"6e6800c9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18628,"status":"ok","timestamp":1693229324708,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"6e6800c9","outputId":"67ea9cca-38ad-4a4f-9d1c-64683b5e9a5f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rdkit-pypi\n","  Downloading rdkit_pypi-2022.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.4/29.4 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n","Installing collected packages: rdkit-pypi\n","Successfully installed rdkit-pypi-2022.9.5\n"]}],"source":["!pip install rdkit-pypi"]},{"cell_type":"code","execution_count":5,"id":"3c2cda0b","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"elapsed":8715,"status":"ok","timestamp":1693229333420,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"3c2cda0b","outputId":"f3dbb537-4f6c-4105-8617-642098e1faf1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["              id                                             SMILES     MLM  \\\n","0     TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n","1     TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n","2     TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n","3     TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n","4     TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n","...          ...                                                ...     ...   \n","3493  TRAIN_3493     Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl   1.556   \n","3494  TRAIN_3494  CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...  35.560   \n","3495  TRAIN_3495                       CCOC(=O)CCCc1nc2cc(N)ccc2n1C  56.150   \n","3496  TRAIN_3496                     Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl   0.030   \n","3497  TRAIN_3497                   COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1   0.450   \n","\n","         HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n","0     50.680  3.259           400.495                5             2   \n","1     50.590  2.169           301.407                2             1   \n","2     80.892  1.593           297.358                5             0   \n","3      2.000  4.771           494.652                6             0   \n","4     99.990  2.335           268.310                3             0   \n","...      ...    ...               ...              ...           ...   \n","3493   3.079  3.409           396.195                3             1   \n","3494  47.630  1.912           359.381                4             1   \n","3495   1.790  1.941           261.320                3             1   \n","3496   2.770  0.989           284.696                5             1   \n","3497   2.650  4.321           295.399                2             0   \n","\n","      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea     logP     apka  \\\n","0                      8  3.259                      117.37  3.87744  400.504   \n","1                      2  2.172                       73.47  3.35474  301.415   \n","2                      3  1.585                       62.45  1.20450  297.366   \n","3                      5  3.475                       92.60  3.89356  494.665   \n","4                      1  2.337                       42.43  2.81772  268.316   \n","...                  ...    ...                         ...      ...      ...   \n","3493                   5  3.409                       64.74  2.74730  396.200   \n","3494                   3  1.844                       77.37  2.27630  359.389   \n","3495                   6  2.124                       70.14  2.04130  261.325   \n","3496                   5  0.989                       91.51  1.42720  284.699   \n","3497                   4  4.321                       50.36  4.71792  295.407   \n","\n","      num_rotatable_bonds  num_heteroatoms  num_hydrogen_acceptors  \\\n","0                       8                8                       6   \n","1                       2                5                       4   \n","2                       3                7                       7   \n","3                       5                9                       7   \n","4                       1                4                       3   \n","...                   ...              ...                     ...   \n","3493                    4               11                       5   \n","3494                    3                7                       5   \n","3495                    5                5                       5   \n","3496                    4                7                       6   \n","3497                    4                3                       3   \n","\n","      num_hydrogen_donors                                 morgan_fingerprint  \n","0                       2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","1                       1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","2                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3                       0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","4                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","...                   ...                                                ...  \n","3493                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3494                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3495                    1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","3496                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n","3497                    0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n","\n","[3498 rows x 18 columns]"],"text/html":["\n","  <div id=\"df-f126b29d-bbc1-43e5-a373-a48e7e3a8ea4\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>SMILES</th>\n","      <th>MLM</th>\n","      <th>HLM</th>\n","      <th>AlogP</th>\n","      <th>Molecular_Weight</th>\n","      <th>Num_H_Acceptors</th>\n","      <th>Num_H_Donors</th>\n","      <th>Num_RotatableBonds</th>\n","      <th>LogD</th>\n","      <th>Molecular_PolarSurfaceArea</th>\n","      <th>logP</th>\n","      <th>apka</th>\n","      <th>num_rotatable_bonds</th>\n","      <th>num_heteroatoms</th>\n","      <th>num_hydrogen_acceptors</th>\n","      <th>num_hydrogen_donors</th>\n","      <th>morgan_fingerprint</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_0000</td>\n","      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n","      <td>26.010</td>\n","      <td>50.680</td>\n","      <td>3.259</td>\n","      <td>400.495</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>3.259</td>\n","      <td>117.37</td>\n","      <td>3.87744</td>\n","      <td>400.504</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_0001</td>\n","      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n","      <td>29.270</td>\n","      <td>50.590</td>\n","      <td>2.169</td>\n","      <td>301.407</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2.172</td>\n","      <td>73.47</td>\n","      <td>3.35474</td>\n","      <td>301.415</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_0002</td>\n","      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n","      <td>5.586</td>\n","      <td>80.892</td>\n","      <td>1.593</td>\n","      <td>297.358</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.585</td>\n","      <td>62.45</td>\n","      <td>1.20450</td>\n","      <td>297.366</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_0003</td>\n","      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n","      <td>5.710</td>\n","      <td>2.000</td>\n","      <td>4.771</td>\n","      <td>494.652</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>3.475</td>\n","      <td>92.60</td>\n","      <td>3.89356</td>\n","      <td>494.665</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_0004</td>\n","      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n","      <td>93.270</td>\n","      <td>99.990</td>\n","      <td>2.335</td>\n","      <td>268.310</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.337</td>\n","      <td>42.43</td>\n","      <td>2.81772</td>\n","      <td>268.316</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3493</th>\n","      <td>TRAIN_3493</td>\n","      <td>Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl</td>\n","      <td>1.556</td>\n","      <td>3.079</td>\n","      <td>3.409</td>\n","      <td>396.195</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3.409</td>\n","      <td>64.74</td>\n","      <td>2.74730</td>\n","      <td>396.200</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3494</th>\n","      <td>TRAIN_3494</td>\n","      <td>CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...</td>\n","      <td>35.560</td>\n","      <td>47.630</td>\n","      <td>1.912</td>\n","      <td>359.381</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1.844</td>\n","      <td>77.37</td>\n","      <td>2.27630</td>\n","      <td>359.389</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3495</th>\n","      <td>TRAIN_3495</td>\n","      <td>CCOC(=O)CCCc1nc2cc(N)ccc2n1C</td>\n","      <td>56.150</td>\n","      <td>1.790</td>\n","      <td>1.941</td>\n","      <td>261.320</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>2.124</td>\n","      <td>70.14</td>\n","      <td>2.04130</td>\n","      <td>261.325</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3496</th>\n","      <td>TRAIN_3496</td>\n","      <td>Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl</td>\n","      <td>0.030</td>\n","      <td>2.770</td>\n","      <td>0.989</td>\n","      <td>284.696</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0.989</td>\n","      <td>91.51</td>\n","      <td>1.42720</td>\n","      <td>284.699</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3497</th>\n","      <td>TRAIN_3497</td>\n","      <td>COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1</td>\n","      <td>0.450</td>\n","      <td>2.650</td>\n","      <td>4.321</td>\n","      <td>295.399</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.321</td>\n","      <td>50.36</td>\n","      <td>4.71792</td>\n","      <td>295.407</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3498 rows × 18 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f126b29d-bbc1-43e5-a373-a48e7e3a8ea4')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f126b29d-bbc1-43e5-a373-a48e7e3a8ea4 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f126b29d-bbc1-43e5-a373-a48e7e3a8ea4');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-c98d00cb-e91b-4871-8431-ec734302bba7\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c98d00cb-e91b-4871-8431-ec734302bba7')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-c98d00cb-e91b-4871-8431-ec734302bba7 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":5}],"source":["import pandas as pd\n","import numpy as np\n","from rdkit import Chem, DataStructs\n","from rdkit.Chem import AllChem, Descriptors\n","\n","def calculate_metabolic_stability_descriptors(smiles):\n","    mol = Chem.MolFromSmiles(smiles)\n","    logP = Descriptors.MolLogP(mol)\n","    # 화합물의 친유성을 측정한 것으로 지질 또는 비극성 환경에서의 용해도를 나타냅니다. 생물학적 막을 통과하는 화합물의 능력을 반영합니다.\n","    apka = Descriptors.MolWt(mol)\n","    # 화합물의 산 해리 상수의 추정치로 다양한 pH 조건에서 이온화 거동에 대한 정보를 제공합니다.\n","    num_rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n","    # 화합물에서 회전 가능한 결합의 수입니다. 이것은 화합물의 유연성과 효소 또는 다른 분자와의 잠재적인 상호 작용에 대한 통찰력을 제공할 수 있습니다.\n","    num_heteroatoms = Descriptors.NumHeteroatoms(mol)\n","    # 분자 내 헤테로원자(탄소 및 수소 이외의 원자) 수. 이는 화합물의 반응성과 대사 안정성에 영향을 줄 수 있습니다.\n","    num_hydrogen_acceptors = Descriptors.NumHAcceptors(mol)\n","    # 분자 내 수소 결합 수용체의 수. 이들은 결합 및 반응성에 영향을 미치는 다른 분자의 수소 결합 기증자와 상호 작용할 수 있는 사이트입니다.\n","    num_hydrogen_donors = Descriptors.NumHDonors(mol)\n","    # 분자 내 수소 결합 기증자의 수입니다. 이들은 수소 결합 상호작용에서 수소 원자를 제공할 수 있는 사이트입니다.\n","    # morgan_fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n","    morgan_fingerprint = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n","    # 분자 하위 구조의 이진 벡터 표현입니다. 이 열에는 화합물과 효소의 상호 작용 및 대사 안정성에 영향을 줄 수 있는 구조적 특징을 포착하는 이진 지문이 포함되어 있습니다.\n","    morgan_array = np.zeros((1,), dtype=np.int8)\n","    DataStructs.ConvertToNumpyArray(morgan_fingerprint, morgan_array)\n","\n","    return logP, apka, num_rotatable_bonds, num_heteroatoms, num_hydrogen_acceptors, num_hydrogen_donors, morgan_array\n","\n","train[[\n","    'logP', 'apka', 'num_rotatable_bonds', 'num_heteroatoms',\n","    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint'\n","]] = train['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n","\n","test[[\n","    'logP', 'apka', 'num_rotatable_bonds', 'num_heteroatoms',\n","    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint'\n","]] = test['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n","\n","train\n"]},{"cell_type":"code","execution_count":6,"id":"c72ec77f","metadata":{"id":"c72ec77f","executionInfo":{"status":"ok","timestamp":1693229333420,"user_tz":-540,"elapsed":13,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["train['AlogP'].fillna(train['AlogP'].median(), inplace=True)\n","test['AlogP'].fillna(test['AlogP'].median(), inplace=True)"]},{"cell_type":"code","execution_count":7,"id":"CyrKl5mXTYFG","metadata":{"id":"CyrKl5mXTYFG","executionInfo":{"status":"ok","timestamp":1693229333421,"user_tz":-540,"elapsed":13,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["class NewFeatureDataset(Dataset):\n","    def __init__(self, data, target_col=None, transform=None, is_test=False):\n","        self.is_test = is_test\n","        self.transform = transform\n","        self.is_test = is_test\n","\n","        if not self.is_test:\n","            self.data = data.drop(['id', 'SMILES', 'morgan_fingerprint','MLM', 'HLM'], axis=1)\n","        else: # test\n","            self.data = data.drop(['id', 'SMILES', 'morgan_fingerprint'], axis=1)\n","\n","\n","        if self.transform is not None and not self.is_test:  # 훈련 데이터에만 fit_transform 적용\n","            self.data = self.transform.fit_transform(self.data)\n","        elif self.transform is not None and self.is_test:  # 테스트 데이터에는 transform만 적용\n","            self.data = self.transform.transform(self.data)\n","\n","        if target_col is not None and not self.is_test:\n","            self.target = data[target_col]\n","\n","    def __getitem__(self, index):\n","        features = self.data[index]\n","\n","        if hasattr(self, 'target'):\n","            target = self.target[index]\n","            return torch.tensor(features).to(device).float(), torch.tensor(target).to(device).float().unsqueeze(dim=-1)\n","        else:\n","            return torch.tensor(features).to(device).float()\n","\n","    def __len__(self):\n","        return len(self.data)\n"]},{"cell_type":"code","execution_count":8,"id":"a5dfdf8b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1693229333421,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"a5dfdf8b","outputId":"bdf40fe8-8124-404c-8483-39a6074e5760"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["13"]},"metadata":{},"execution_count":8}],"source":["transform = StandardScaler()\n","transform.fit(train.drop(['id','SMILES','morgan_fingerprint', 'MLM', 'HLM'], axis=1))\n","\n","train_MLM = NewFeatureDataset(train, target_col='MLM', transform=transform, is_test=False)\n","train_HLM = NewFeatureDataset(train, target_col='HLM', transform=transform, is_test=False)\n","\n","input_size = train_MLM.data.shape[1]\n","input_size"]},{"cell_type":"code","execution_count":9,"id":"e46c90d2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1693229333421,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"e46c90d2","outputId":"21f25e4b-9ce3-4b8c-a9f2-5d397716f1fb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3498, 13)"]},"metadata":{},"execution_count":9}],"source":["train_HLM.data.shape"]},{"cell_type":"code","execution_count":10,"id":"78a83026","metadata":{"id":"78a83026","executionInfo":{"status":"ok","timestamp":1693229334203,"user_tz":-540,"elapsed":791,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["# train,valid split\n","train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42)\n","train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":11,"id":"b15bd4f4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693229334203,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"b15bd4f4","outputId":"28c06ca1-155e-4db5-a4a5-2af9a6a9b868"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([13]), torch.Size([1]))"]},"metadata":{},"execution_count":11}],"source":["torch.tensor(train_MLM.data[1]).shape, torch.tensor(train['MLM'][1]).float().unsqueeze(dim=-1).shape"]},{"cell_type":"code","execution_count":12,"id":"d73f8af6","metadata":{"id":"d73f8af6","executionInfo":{"status":"ok","timestamp":1693229334203,"user_tz":-540,"elapsed":4,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["# Hyperparameter\n","CFG = {'BATCH_SIZE': 256,\n","       'EPOCHS': 5000,\n","       'INPUT_SIZE': input_size,\n","       'HIDDEN_SIZE': 256,\n","       'OUTPUT_SIZE': 1,\n","       'DROPOUT_RATE': 0.5,\n","       'LEARNING_RATE': 0.0001}"]},{"cell_type":"code","execution_count":13,"id":"f328f1d7","metadata":{"id":"f328f1d7","executionInfo":{"status":"ok","timestamp":1693229334203,"user_tz":-540,"elapsed":4,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=True)\n","\n","valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=False)\n","\n","\n","train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=True)\n","\n","valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=False)"]},{"cell_type":"code","execution_count":14,"id":"7f29fcb9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693229334203,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"7f29fcb9","outputId":"79717fd1-6967-497c-ae1b-485eed6a16f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256, 13]) torch.Size([256, 1])\n"]}],"source":["X_train, y_train = next(iter(train_MLM_loader))\n","print (X_train.shape, y_train.shape)"]},{"cell_type":"code","execution_count":15,"id":"bfd4adf6","metadata":{"id":"bfd4adf6","executionInfo":{"status":"ok","timestamp":1693229334203,"user_tz":-540,"elapsed":3,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["class NewFeatureModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n","        super(NewFeatureModel, self).__init__()\n","\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(input_size, hidden_size),\n","            nn.BatchNorm1d(hidden_size),\n","            nn.LeakyReLU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(256, 128),\n","            nn.BatchNorm1d(128),\n","            nn.LeakyReLU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(128, 64),\n","            nn.BatchNorm1d(64),\n","            nn.LeakyReLU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.fc_out = nn.Linear(64, out_size)\n","\n","    def forward(self, x):\n","        out = self.fc_layers(x)\n","        out = self.fc_out(out)\n","        return out\n"]},{"cell_type":"code","execution_count":16,"id":"ab4fd7ce","metadata":{"id":"ab4fd7ce","executionInfo":{"status":"ok","timestamp":1693229334204,"user_tz":-540,"elapsed":4,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["featuresModel_MLM = NewFeatureModel(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])\n","featuresModel_HLM = NewFeatureModel(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])"]},{"cell_type":"code","execution_count":17,"id":"941a230e","metadata":{"id":"941a230e","executionInfo":{"status":"ok","timestamp":1693229334204,"user_tz":-540,"elapsed":4,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["import torch.nn as nn\n","\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super(RMSELoss, self).__init__()\n","        self.mse = nn.MSELoss()  # 기존의 MSELoss 함수 사용\n","\n","    def forward(self, output, target):\n","        mse_loss = self.mse(output, target)  # 기존의 MSELoss를 계산\n","        rmse_loss = torch.sqrt(mse_loss)  # MSE에 제곱근 씌워 RMSE 계산\n","        return rmse_loss\n","\n","criterion = RMSELoss()\n","optimizer_MLM = torch.optim.Adam(featuresModel_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n","optimizer_HLM = torch.optim.Adam(featuresModel_HLM.parameters(), lr=CFG['LEARNING_RATE'])"]},{"cell_type":"code","execution_count":18,"id":"c8c50632","metadata":{"id":"c8c50632","executionInfo":{"status":"ok","timestamp":1693229334204,"user_tz":-540,"elapsed":4,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["def train(train_loader, valid_loader, model, criterion, optimizer, epochs, patience=100):\n","    best_valid_loss = float('inf')\n","    no_improvement_count = 0\n","\n","    for epoch in range(epochs):\n","        model.train()  # 모델을 훈련 모드로 설정\n","        running_loss = 0\n","        for inputs, targets in train_loader:\n","            optimizer.zero_grad()\n","\n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        model.eval()  # 모델을 검증 모드로 설정\n","        valid_loss = 0\n","        with torch.no_grad():\n","          for inputs, targets in valid_loader:\n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            valid_loss += loss.item()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","        avg_valid_loss = valid_loss / len(valid_loader)\n","        print(f'Epoch: {epoch}/{epochs}, Train Loss: {avg_train_loss}, Valid Loss: {avg_valid_loss}')\n","\n","        if avg_valid_loss < best_valid_loss:\n","          best_valid_loss = avg_valid_loss\n","          no_improvement_count = 0\n","          best_model_state = model.state_dict()\n","        else:\n","          no_improvement_count += 1\n","          if no_improvement_count >= patience:\n","            print(f'얼리 스토핑: {patience} 에포크 동안 검증 손실이 향상되지 않음. 에포크 {epoch}에서 훈련 중단.')\n","            break\n","\n","    # 최적의 모델 상태 불러오기\n","    model.load_state_dict(best_model_state)\n","    return model"]},{"cell_type":"code","execution_count":19,"id":"7bf20b4b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":211324,"status":"ok","timestamp":1693229545525,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"7bf20b4b","outputId":"8a08ae59-6aad-4287-d981-1a8ef3f126e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Start: MLM\n","Epoch: 0/5000, Train Loss: 51.27079876986417, Valid Loss: 52.43033981323242\n","Epoch: 1/5000, Train Loss: 51.27351205999201, Valid Loss: 52.38189951578776\n","Epoch: 2/5000, Train Loss: 51.25491298328746, Valid Loss: 52.326367696126304\n","Epoch: 3/5000, Train Loss: 51.15074400468306, Valid Loss: 52.27757263183594\n","Epoch: 4/5000, Train Loss: 51.121328527277164, Valid Loss: 52.23943201700846\n","Epoch: 5/5000, Train Loss: 51.1218920621005, Valid Loss: 52.20205307006836\n","Epoch: 6/5000, Train Loss: 51.04326421564276, Valid Loss: 52.16639200846354\n","Epoch: 7/5000, Train Loss: 51.00367494062944, Valid Loss: 52.13565190633138\n","Epoch: 8/5000, Train Loss: 50.95009994506836, Valid Loss: 52.104974110921226\n","Epoch: 9/5000, Train Loss: 50.92027213356712, Valid Loss: 52.06761678059896\n","Epoch: 10/5000, Train Loss: 50.84490897438743, Valid Loss: 52.02779006958008\n","Epoch: 11/5000, Train Loss: 50.78615292635831, Valid Loss: 51.98272196451823\n","Epoch: 12/5000, Train Loss: 50.77085460316051, Valid Loss: 51.942910512288414\n","Epoch: 13/5000, Train Loss: 50.73594804243608, Valid Loss: 51.90640767415365\n","Epoch: 14/5000, Train Loss: 50.6247163252397, Valid Loss: 51.87347412109375\n","Epoch: 15/5000, Train Loss: 50.61533875898881, Valid Loss: 51.816900889078774\n","Epoch: 16/5000, Train Loss: 50.57508503306996, Valid Loss: 51.77270634969076\n","Epoch: 17/5000, Train Loss: 50.535432988947086, Valid Loss: 51.71929931640625\n","Epoch: 18/5000, Train Loss: 50.47169355912642, Valid Loss: 51.6817881266276\n","Epoch: 19/5000, Train Loss: 50.47608115456321, Valid Loss: 51.64329528808594\n","Epoch: 20/5000, Train Loss: 50.40731846202504, Valid Loss: 51.594340006510414\n","Epoch: 21/5000, Train Loss: 50.383541800759055, Valid Loss: 51.55752182006836\n","Epoch: 22/5000, Train Loss: 50.34106306596236, Valid Loss: 51.51585006713867\n","Epoch: 23/5000, Train Loss: 50.307185433127664, Valid Loss: 51.5035146077474\n","Epoch: 24/5000, Train Loss: 50.240323846990414, Valid Loss: 51.43963114420573\n","Epoch: 25/5000, Train Loss: 50.17342342029918, Valid Loss: 51.39769490559896\n","Epoch: 26/5000, Train Loss: 50.15052171186967, Valid Loss: 51.360357920328774\n","Epoch: 27/5000, Train Loss: 50.153020338578656, Valid Loss: 51.33287556966146\n","Epoch: 28/5000, Train Loss: 50.08781433105469, Valid Loss: 51.31290054321289\n","Epoch: 29/5000, Train Loss: 50.073610825972125, Valid Loss: 51.256656646728516\n","Epoch: 30/5000, Train Loss: 49.988069014115766, Valid Loss: 51.210184733072914\n","Epoch: 31/5000, Train Loss: 49.94069428877397, Valid Loss: 51.17057673136393\n","Epoch: 32/5000, Train Loss: 49.9263784235174, Valid Loss: 51.116354624430336\n","Epoch: 33/5000, Train Loss: 49.90010556307706, Valid Loss: 51.08355458577474\n","Epoch: 34/5000, Train Loss: 49.84738124500621, Valid Loss: 51.05447769165039\n","Epoch: 35/5000, Train Loss: 49.81407928466797, Valid Loss: 51.000406901041664\n","Epoch: 36/5000, Train Loss: 49.79623100974343, Valid Loss: 50.98276392618815\n","Epoch: 37/5000, Train Loss: 49.74452729658647, Valid Loss: 50.929115295410156\n","Epoch: 38/5000, Train Loss: 49.70342740145597, Valid Loss: 50.89026387532552\n","Epoch: 39/5000, Train Loss: 49.70481976595792, Valid Loss: 50.84005610148112\n","Epoch: 40/5000, Train Loss: 49.621726989746094, Valid Loss: 50.84051386515299\n","Epoch: 41/5000, Train Loss: 49.57179086858576, Valid Loss: 50.7942148844401\n","Epoch: 42/5000, Train Loss: 49.54647861827504, Valid Loss: 50.76367060343424\n","Epoch: 43/5000, Train Loss: 49.51748830621893, Valid Loss: 50.740973154703774\n","Epoch: 44/5000, Train Loss: 49.51746368408203, Valid Loss: 50.675558725992836\n","Epoch: 45/5000, Train Loss: 49.44016543301669, Valid Loss: 50.63942082722982\n","Epoch: 46/5000, Train Loss: 49.36126917058771, Valid Loss: 50.60423914591471\n","Epoch: 47/5000, Train Loss: 49.344054135409266, Valid Loss: 50.596700032552086\n","Epoch: 48/5000, Train Loss: 49.342581662264735, Valid Loss: 50.551614125569664\n","Epoch: 49/5000, Train Loss: 49.300784371115945, Valid Loss: 50.51215489705404\n","Epoch: 50/5000, Train Loss: 49.218917499889024, Valid Loss: 50.46616236368815\n","Epoch: 51/5000, Train Loss: 49.19236269864169, Valid Loss: 50.42843500773112\n","Epoch: 52/5000, Train Loss: 49.1597505049272, Valid Loss: 50.37967300415039\n","Epoch: 53/5000, Train Loss: 49.13193754716353, Valid Loss: 50.31110636393229\n","Epoch: 54/5000, Train Loss: 49.10599899291992, Valid Loss: 50.30493291219076\n","Epoch: 55/5000, Train Loss: 49.035156596790664, Valid Loss: 50.25820795694987\n","Epoch: 56/5000, Train Loss: 49.07337119362571, Valid Loss: 50.25380325317383\n","Epoch: 57/5000, Train Loss: 49.0247837413441, Valid Loss: 50.21569951375326\n","Epoch: 58/5000, Train Loss: 48.933567393909804, Valid Loss: 50.160474141438804\n","Epoch: 59/5000, Train Loss: 48.847590012983844, Valid Loss: 50.11744944254557\n","Epoch: 60/5000, Train Loss: 48.86572889848189, Valid Loss: 50.09403737386068\n","Epoch: 61/5000, Train Loss: 48.81293695623224, Valid Loss: 50.07206471761068\n","Epoch: 62/5000, Train Loss: 48.75557257912376, Valid Loss: 50.00785700480143\n","Epoch: 63/5000, Train Loss: 48.7716758034446, Valid Loss: 49.93134307861328\n","Epoch: 64/5000, Train Loss: 48.68372483686967, Valid Loss: 49.886278788248696\n","Epoch: 65/5000, Train Loss: 48.5691067088734, Valid Loss: 49.87297566731771\n","Epoch: 66/5000, Train Loss: 48.57993420687589, Valid Loss: 49.8252321879069\n","Epoch: 67/5000, Train Loss: 48.576407345858485, Valid Loss: 49.76868184407552\n","Epoch: 68/5000, Train Loss: 48.442794453014024, Valid Loss: 49.750250498453774\n","Epoch: 69/5000, Train Loss: 48.46195567737926, Valid Loss: 49.675174713134766\n","Epoch: 70/5000, Train Loss: 48.362689625133164, Valid Loss: 49.65545399983724\n","Epoch: 71/5000, Train Loss: 48.35645883733576, Valid Loss: 49.546026865641274\n","Epoch: 72/5000, Train Loss: 48.342226548628375, Valid Loss: 49.52594121297201\n","Epoch: 73/5000, Train Loss: 48.26735721934926, Valid Loss: 49.525716145833336\n","Epoch: 74/5000, Train Loss: 48.25758951360529, Valid Loss: 49.506795247395836\n","Epoch: 75/5000, Train Loss: 48.20960582386363, Valid Loss: 49.46259435017904\n","Epoch: 76/5000, Train Loss: 48.17357427423651, Valid Loss: 49.41814931233724\n","Epoch: 77/5000, Train Loss: 48.10008343783292, Valid Loss: 49.360696156819664\n","Epoch: 78/5000, Train Loss: 48.00748339566317, Valid Loss: 49.335182189941406\n","Epoch: 79/5000, Train Loss: 48.01580186323686, Valid Loss: 49.274218241373696\n","Epoch: 80/5000, Train Loss: 47.97415126453746, Valid Loss: 49.1999257405599\n","Epoch: 81/5000, Train Loss: 47.908829082142226, Valid Loss: 49.17263158162435\n","Epoch: 82/5000, Train Loss: 47.86002731323242, Valid Loss: 49.09083811442057\n","Epoch: 83/5000, Train Loss: 47.832646109841086, Valid Loss: 49.08938217163086\n","Epoch: 84/5000, Train Loss: 47.832460923628375, Valid Loss: 49.067352294921875\n","Epoch: 85/5000, Train Loss: 47.77656347101385, Valid Loss: 48.989925384521484\n","Epoch: 86/5000, Train Loss: 47.699096332896836, Valid Loss: 48.98554484049479\n","Epoch: 87/5000, Train Loss: 47.61304924704812, Valid Loss: 48.952317555745445\n","Epoch: 88/5000, Train Loss: 47.57644896073775, Valid Loss: 48.8925666809082\n","Epoch: 89/5000, Train Loss: 47.57785138216886, Valid Loss: 48.84739557902018\n","Epoch: 90/5000, Train Loss: 47.480172243985265, Valid Loss: 48.76411437988281\n","Epoch: 91/5000, Train Loss: 47.48158090764826, Valid Loss: 48.75566482543945\n","Epoch: 92/5000, Train Loss: 47.42425190318715, Valid Loss: 48.65937296549479\n","Epoch: 93/5000, Train Loss: 47.38422879305753, Valid Loss: 48.62078857421875\n","Epoch: 94/5000, Train Loss: 47.28477928855202, Valid Loss: 48.569801330566406\n","Epoch: 95/5000, Train Loss: 47.24364194003019, Valid Loss: 48.52211888631185\n","Epoch: 96/5000, Train Loss: 47.156536449085586, Valid Loss: 48.482462565104164\n","Epoch: 97/5000, Train Loss: 47.19088433005593, Valid Loss: 48.46167246500651\n","Epoch: 98/5000, Train Loss: 47.11103751442649, Valid Loss: 48.43486022949219\n","Epoch: 99/5000, Train Loss: 47.0409743569114, Valid Loss: 48.40139897664388\n","Epoch: 100/5000, Train Loss: 46.93246009133079, Valid Loss: 48.301745096842446\n","Epoch: 101/5000, Train Loss: 46.96912453391335, Valid Loss: 48.26519775390625\n","Epoch: 102/5000, Train Loss: 46.89111328125, Valid Loss: 48.19389088948568\n","Epoch: 103/5000, Train Loss: 46.84203824129972, Valid Loss: 48.18403244018555\n","Epoch: 104/5000, Train Loss: 46.81429949673739, Valid Loss: 48.1062266031901\n","Epoch: 105/5000, Train Loss: 46.77175556529652, Valid Loss: 48.108343760172524\n","Epoch: 106/5000, Train Loss: 46.748254255814985, Valid Loss: 48.01537831624349\n","Epoch: 107/5000, Train Loss: 46.6411906155673, Valid Loss: 47.9537353515625\n","Epoch: 108/5000, Train Loss: 46.59167549826882, Valid Loss: 47.9098269144694\n","Epoch: 109/5000, Train Loss: 46.51020466197621, Valid Loss: 47.873861948649086\n","Epoch: 110/5000, Train Loss: 46.527495644309305, Valid Loss: 47.79731369018555\n","Epoch: 111/5000, Train Loss: 46.51624991677024, Valid Loss: 47.820177714029946\n","Epoch: 112/5000, Train Loss: 46.35199529474432, Valid Loss: 47.75631968180338\n","Epoch: 113/5000, Train Loss: 46.32615488225763, Valid Loss: 47.65400822957357\n","Epoch: 114/5000, Train Loss: 46.33989368785512, Valid Loss: 47.63589604695638\n","Epoch: 115/5000, Train Loss: 46.24822581898082, Valid Loss: 47.65861256917318\n","Epoch: 116/5000, Train Loss: 46.208653536709875, Valid Loss: 47.55678176879883\n","Epoch: 117/5000, Train Loss: 46.190155029296875, Valid Loss: 47.563666025797524\n","Epoch: 118/5000, Train Loss: 46.07382375543768, Valid Loss: 47.48893737792969\n","Epoch: 119/5000, Train Loss: 46.04547674005682, Valid Loss: 47.36366399129232\n","Epoch: 120/5000, Train Loss: 45.95542248812589, Valid Loss: 47.371116638183594\n","Epoch: 121/5000, Train Loss: 45.91433403708718, Valid Loss: 47.304986317952476\n","Epoch: 122/5000, Train Loss: 45.82638827237216, Valid Loss: 47.29179000854492\n","Epoch: 123/5000, Train Loss: 45.84941655939276, Valid Loss: 47.174922943115234\n","Epoch: 124/5000, Train Loss: 45.6891843622381, Valid Loss: 47.13418324788412\n","Epoch: 125/5000, Train Loss: 45.68976905129173, Valid Loss: 47.082192738850914\n","Epoch: 126/5000, Train Loss: 45.67599938132546, Valid Loss: 47.03001022338867\n","Epoch: 127/5000, Train Loss: 45.63048692183061, Valid Loss: 47.06212361653646\n","Epoch: 128/5000, Train Loss: 45.585601113059305, Valid Loss: 46.94942982991537\n","Epoch: 129/5000, Train Loss: 45.49181088534269, Valid Loss: 46.84252039591471\n","Epoch: 130/5000, Train Loss: 45.403407703746446, Valid Loss: 46.83105723063151\n","Epoch: 131/5000, Train Loss: 45.42460805719549, Valid Loss: 46.69970957438151\n","Epoch: 132/5000, Train Loss: 45.35588871348988, Valid Loss: 46.661842346191406\n","Epoch: 133/5000, Train Loss: 45.20927880027077, Valid Loss: 46.694801330566406\n","Epoch: 134/5000, Train Loss: 45.203196092085406, Valid Loss: 46.603355407714844\n","Epoch: 135/5000, Train Loss: 45.18229536576705, Valid Loss: 46.55240249633789\n","Epoch: 136/5000, Train Loss: 45.12805106423118, Valid Loss: 46.48497772216797\n","Epoch: 137/5000, Train Loss: 45.050466017289594, Valid Loss: 46.44656499226888\n","Epoch: 138/5000, Train Loss: 45.03222864324396, Valid Loss: 46.38789367675781\n","Epoch: 139/5000, Train Loss: 44.88726217096502, Valid Loss: 46.372745513916016\n","Epoch: 140/5000, Train Loss: 44.820528203790836, Valid Loss: 46.25368881225586\n","Epoch: 141/5000, Train Loss: 44.80872865156694, Valid Loss: 46.232872009277344\n","Epoch: 142/5000, Train Loss: 44.78171435269442, Valid Loss: 46.16678110758463\n","Epoch: 143/5000, Train Loss: 44.721051996404476, Valid Loss: 46.15184656778971\n","Epoch: 144/5000, Train Loss: 44.64289751919833, Valid Loss: 46.08234405517578\n","Epoch: 145/5000, Train Loss: 44.480836694890804, Valid Loss: 46.01394399007162\n","Epoch: 146/5000, Train Loss: 44.462806701660156, Valid Loss: 45.97499211629232\n","Epoch: 147/5000, Train Loss: 44.44230651855469, Valid Loss: 45.862467447916664\n","Epoch: 148/5000, Train Loss: 44.424263347278945, Valid Loss: 45.854061126708984\n","Epoch: 149/5000, Train Loss: 44.37748544866388, Valid Loss: 45.88939921061198\n","Epoch: 150/5000, Train Loss: 44.19249378551137, Valid Loss: 45.72471491495768\n","Epoch: 151/5000, Train Loss: 44.35240658846769, Valid Loss: 45.671026865641274\n","Epoch: 152/5000, Train Loss: 44.151507290926844, Valid Loss: 45.637335459391274\n","Epoch: 153/5000, Train Loss: 44.15241726962003, Valid Loss: 45.601298014322914\n","Epoch: 154/5000, Train Loss: 44.13156301325018, Valid Loss: 45.57129414876302\n","Epoch: 155/5000, Train Loss: 44.058750499378554, Valid Loss: 45.517069498697914\n","Epoch: 156/5000, Train Loss: 44.03710868141868, Valid Loss: 45.43390655517578\n","Epoch: 157/5000, Train Loss: 43.86455952037465, Valid Loss: 45.38365809122721\n","Epoch: 158/5000, Train Loss: 43.75380949540572, Valid Loss: 45.34592946370443\n","Epoch: 159/5000, Train Loss: 43.78723907470703, Valid Loss: 45.260023752848305\n","Epoch: 160/5000, Train Loss: 43.67974576083097, Valid Loss: 45.13360341389974\n","Epoch: 161/5000, Train Loss: 43.70805289528587, Valid Loss: 45.090667724609375\n","Epoch: 162/5000, Train Loss: 43.66479006680575, Valid Loss: 45.069323221842446\n","Epoch: 163/5000, Train Loss: 43.58561879938299, Valid Loss: 45.016780853271484\n","Epoch: 164/5000, Train Loss: 43.45767107876864, Valid Loss: 44.95208613077799\n","Epoch: 165/5000, Train Loss: 43.39051333340731, Valid Loss: 44.933475494384766\n","Epoch: 166/5000, Train Loss: 43.47288582541726, Valid Loss: 44.872230529785156\n","Epoch: 167/5000, Train Loss: 43.38261482932351, Valid Loss: 44.89914321899414\n","Epoch: 168/5000, Train Loss: 43.22045204856179, Valid Loss: 44.828469594319664\n","Epoch: 169/5000, Train Loss: 43.29270137440074, Valid Loss: 44.803183237711586\n","Epoch: 170/5000, Train Loss: 43.24155946211381, Valid Loss: 44.69358825683594\n","Epoch: 171/5000, Train Loss: 43.07135564630682, Valid Loss: 44.58746465047201\n","Epoch: 172/5000, Train Loss: 43.08781779896129, Valid Loss: 44.6623903910319\n","Epoch: 173/5000, Train Loss: 42.973758697509766, Valid Loss: 44.649359385172524\n","Epoch: 174/5000, Train Loss: 42.97022247314453, Valid Loss: 44.56066131591797\n","Epoch: 175/5000, Train Loss: 42.91817717118697, Valid Loss: 44.518157958984375\n","Epoch: 176/5000, Train Loss: 42.79152714122426, Valid Loss: 44.419761657714844\n","Epoch: 177/5000, Train Loss: 42.80964279174805, Valid Loss: 44.38284683227539\n","Epoch: 178/5000, Train Loss: 42.74912400679155, Valid Loss: 44.37563451131185\n","Epoch: 179/5000, Train Loss: 42.59626839377663, Valid Loss: 44.19691721598307\n","Epoch: 180/5000, Train Loss: 42.67609058726918, Valid Loss: 44.12902704874674\n","Epoch: 181/5000, Train Loss: 42.59622712568803, Valid Loss: 44.10855611165365\n","Epoch: 182/5000, Train Loss: 42.42794834483754, Valid Loss: 43.9752082824707\n","Epoch: 183/5000, Train Loss: 42.38141458684748, Valid Loss: 43.971649169921875\n","Epoch: 184/5000, Train Loss: 42.49847065318715, Valid Loss: 43.92524210611979\n","Epoch: 185/5000, Train Loss: 42.347360437566586, Valid Loss: 43.857381184895836\n","Epoch: 186/5000, Train Loss: 42.27049498124556, Valid Loss: 43.81233215332031\n","Epoch: 187/5000, Train Loss: 42.10980918190696, Valid Loss: 43.77496083577474\n","Epoch: 188/5000, Train Loss: 42.03698799826882, Valid Loss: 43.79457346598307\n","Epoch: 189/5000, Train Loss: 42.0565958890048, Valid Loss: 43.74580001831055\n","Epoch: 190/5000, Train Loss: 42.030771775679156, Valid Loss: 43.62651570638021\n","Epoch: 191/5000, Train Loss: 42.00741230357777, Valid Loss: 43.62901941935221\n","Epoch: 192/5000, Train Loss: 41.91838906028054, Valid Loss: 43.59940973917643\n","Epoch: 193/5000, Train Loss: 41.79519930752841, Valid Loss: 43.42823282877604\n","Epoch: 194/5000, Train Loss: 41.860987576571375, Valid Loss: 43.40796025594076\n","Epoch: 195/5000, Train Loss: 41.73408092151988, Valid Loss: 43.40105438232422\n","Epoch: 196/5000, Train Loss: 41.64518945867365, Valid Loss: 43.36250686645508\n","Epoch: 197/5000, Train Loss: 41.58481008356268, Valid Loss: 43.27337392171224\n","Epoch: 198/5000, Train Loss: 41.49117868596857, Valid Loss: 43.17913309733073\n","Epoch: 199/5000, Train Loss: 41.41012885353782, Valid Loss: 43.074083964029946\n","Epoch: 200/5000, Train Loss: 41.4179538380016, Valid Loss: 43.07648595174154\n","Epoch: 201/5000, Train Loss: 41.36240907148881, Valid Loss: 43.05320612589518\n","Epoch: 202/5000, Train Loss: 41.38655263727362, Valid Loss: 43.06718317667643\n","Epoch: 203/5000, Train Loss: 41.34154233065519, Valid Loss: 43.00701014200846\n","Epoch: 204/5000, Train Loss: 41.223973360928625, Valid Loss: 42.85095087687174\n","Epoch: 205/5000, Train Loss: 41.24169644442472, Valid Loss: 42.891579945882164\n","Epoch: 206/5000, Train Loss: 41.2142299305309, Valid Loss: 42.79559199015299\n","Epoch: 207/5000, Train Loss: 41.08603980324485, Valid Loss: 42.7617556254069\n","Epoch: 208/5000, Train Loss: 40.99834234064276, Valid Loss: 42.67798868815104\n","Epoch: 209/5000, Train Loss: 40.93574454567649, Valid Loss: 42.598044077555336\n","Epoch: 210/5000, Train Loss: 40.83613274314187, Valid Loss: 42.54350662231445\n","Epoch: 211/5000, Train Loss: 40.92486572265625, Valid Loss: 42.47150166829427\n","Epoch: 212/5000, Train Loss: 40.7170857516202, Valid Loss: 42.48055775960287\n","Epoch: 213/5000, Train Loss: 40.92509391091087, Valid Loss: 42.48655700683594\n","Epoch: 214/5000, Train Loss: 40.64543013139205, Valid Loss: 42.453453063964844\n","Epoch: 215/5000, Train Loss: 40.6265421780673, Valid Loss: 42.38928349812826\n","Epoch: 216/5000, Train Loss: 40.60938158902255, Valid Loss: 42.28482182820638\n","Epoch: 217/5000, Train Loss: 40.47291426225142, Valid Loss: 42.23269780476888\n","Epoch: 218/5000, Train Loss: 40.55696487426758, Valid Loss: 42.139546712239586\n","Epoch: 219/5000, Train Loss: 40.31799593838778, Valid Loss: 42.16688791910807\n","Epoch: 220/5000, Train Loss: 40.2259521484375, Valid Loss: 42.10938262939453\n","Epoch: 221/5000, Train Loss: 40.27615599198775, Valid Loss: 42.016215006510414\n","Epoch: 222/5000, Train Loss: 40.23179938576438, Valid Loss: 42.02826944986979\n","Epoch: 223/5000, Train Loss: 40.13791760531339, Valid Loss: 41.89537811279297\n","Epoch: 224/5000, Train Loss: 40.20406133478338, Valid Loss: 41.88285446166992\n","Epoch: 225/5000, Train Loss: 40.0827751159668, Valid Loss: 41.85230000813802\n","Epoch: 226/5000, Train Loss: 39.918726140802555, Valid Loss: 41.80587514241537\n","Epoch: 227/5000, Train Loss: 39.89118506691673, Valid Loss: 41.629292805989586\n","Epoch: 228/5000, Train Loss: 39.94704263860529, Valid Loss: 41.60881042480469\n","Epoch: 229/5000, Train Loss: 39.87046155062589, Valid Loss: 41.56515757242838\n","Epoch: 230/5000, Train Loss: 39.75587116588246, Valid Loss: 41.64995193481445\n","Epoch: 231/5000, Train Loss: 39.792742642489344, Valid Loss: 41.55369440714518\n","Epoch: 232/5000, Train Loss: 39.6940786188299, Valid Loss: 41.42947006225586\n","Epoch: 233/5000, Train Loss: 39.655486366965555, Valid Loss: 41.432594299316406\n","Epoch: 234/5000, Train Loss: 39.669377760453656, Valid Loss: 41.32765579223633\n","Epoch: 235/5000, Train Loss: 39.515478654341265, Valid Loss: 41.31381861368815\n","Epoch: 236/5000, Train Loss: 39.60308144309304, Valid Loss: 41.27496846516927\n","Epoch: 237/5000, Train Loss: 39.494422219016336, Valid Loss: 41.2771848042806\n","Epoch: 238/5000, Train Loss: 39.37001488425515, Valid Loss: 41.32994079589844\n","Epoch: 239/5000, Train Loss: 39.29250370372426, Valid Loss: 41.18969472249349\n","Epoch: 240/5000, Train Loss: 39.39905548095703, Valid Loss: 41.06748580932617\n","Epoch: 241/5000, Train Loss: 39.24914065274325, Valid Loss: 41.0088742574056\n","Epoch: 242/5000, Train Loss: 39.109642722389914, Valid Loss: 40.91245651245117\n","Epoch: 243/5000, Train Loss: 39.18214589899237, Valid Loss: 40.918958028157554\n","Epoch: 244/5000, Train Loss: 39.03947691483931, Valid Loss: 40.959424336751304\n","Epoch: 245/5000, Train Loss: 39.1063232421875, Valid Loss: 40.86423365275065\n","Epoch: 246/5000, Train Loss: 38.900446805087, Valid Loss: 40.86086654663086\n","Epoch: 247/5000, Train Loss: 38.983338442715734, Valid Loss: 40.82781982421875\n","Epoch: 248/5000, Train Loss: 38.97858845103871, Valid Loss: 40.7131093343099\n","Epoch: 249/5000, Train Loss: 38.756732593883164, Valid Loss: 40.61794408162435\n","Epoch: 250/5000, Train Loss: 38.68360415371981, Valid Loss: 40.58160146077474\n","Epoch: 251/5000, Train Loss: 38.743914517489344, Valid Loss: 40.55611928304037\n","Epoch: 252/5000, Train Loss: 38.81149604103782, Valid Loss: 40.52306238810221\n","Epoch: 253/5000, Train Loss: 38.58897816051137, Valid Loss: 40.48853429158529\n","Epoch: 254/5000, Train Loss: 38.539690884676844, Valid Loss: 40.36656697591146\n","Epoch: 255/5000, Train Loss: 38.48607843572443, Valid Loss: 40.33247502644857\n","Epoch: 256/5000, Train Loss: 38.521305084228516, Valid Loss: 40.31433232625326\n","Epoch: 257/5000, Train Loss: 38.4862306768244, Valid Loss: 40.23343022664388\n","Epoch: 258/5000, Train Loss: 38.35573231090199, Valid Loss: 40.22895177205404\n","Epoch: 259/5000, Train Loss: 38.253516457297586, Valid Loss: 40.235825856526695\n","Epoch: 260/5000, Train Loss: 38.28553459861062, Valid Loss: 40.09841537475586\n","Epoch: 261/5000, Train Loss: 38.23360859264027, Valid Loss: 40.0780143737793\n","Epoch: 262/5000, Train Loss: 38.14054003628817, Valid Loss: 40.089125315348305\n","Epoch: 263/5000, Train Loss: 38.145769292658024, Valid Loss: 39.985914866129555\n","Epoch: 264/5000, Train Loss: 37.885162353515625, Valid Loss: 39.981642405192055\n","Epoch: 265/5000, Train Loss: 37.951294985684484, Valid Loss: 39.9640261332194\n","Epoch: 266/5000, Train Loss: 37.92707894065163, Valid Loss: 39.98020680745443\n","Epoch: 267/5000, Train Loss: 38.01024176857688, Valid Loss: 39.884955088297524\n","Epoch: 268/5000, Train Loss: 38.066347642378375, Valid Loss: 39.786024729410805\n","Epoch: 269/5000, Train Loss: 37.59557446566495, Valid Loss: 39.80687586466471\n","Epoch: 270/5000, Train Loss: 37.90925979614258, Valid Loss: 39.758246103922524\n","Epoch: 271/5000, Train Loss: 37.70845759998668, Valid Loss: 39.68163299560547\n","Epoch: 272/5000, Train Loss: 37.76059861616655, Valid Loss: 39.653577168782554\n","Epoch: 273/5000, Train Loss: 37.61950267444957, Valid Loss: 39.56004969278971\n","Epoch: 274/5000, Train Loss: 37.52335253628817, Valid Loss: 39.56622314453125\n","Epoch: 275/5000, Train Loss: 37.5931958285245, Valid Loss: 39.57723871866862\n","Epoch: 276/5000, Train Loss: 37.36582426591353, Valid Loss: 39.50090535481771\n","Epoch: 277/5000, Train Loss: 37.45285970514471, Valid Loss: 39.42367808024088\n","Epoch: 278/5000, Train Loss: 37.35896266590465, Valid Loss: 39.33639399210612\n","Epoch: 279/5000, Train Loss: 37.20623051036488, Valid Loss: 39.326273600260414\n","Epoch: 280/5000, Train Loss: 37.34538719870827, Valid Loss: 39.38448969523112\n","Epoch: 281/5000, Train Loss: 37.18359860506925, Valid Loss: 39.25839360555013\n","Epoch: 282/5000, Train Loss: 37.32381855357777, Valid Loss: 39.206170399983726\n","Epoch: 283/5000, Train Loss: 37.150687131014735, Valid Loss: 39.18533706665039\n","Epoch: 284/5000, Train Loss: 37.04268888993697, Valid Loss: 39.16367721557617\n","Epoch: 285/5000, Train Loss: 36.94660394841974, Valid Loss: 38.960856119791664\n","Epoch: 286/5000, Train Loss: 37.06337980790572, Valid Loss: 38.995137532552086\n","Epoch: 287/5000, Train Loss: 37.12095468694513, Valid Loss: 38.96384811401367\n","Epoch: 288/5000, Train Loss: 36.816529360684484, Valid Loss: 38.93499883015951\n","Epoch: 289/5000, Train Loss: 36.87470522793856, Valid Loss: 38.9529914855957\n","Epoch: 290/5000, Train Loss: 36.86573548750444, Valid Loss: 38.94305547078451\n","Epoch: 291/5000, Train Loss: 36.79421199451793, Valid Loss: 38.798727671305336\n","Epoch: 292/5000, Train Loss: 36.57968521118164, Valid Loss: 38.83056131998698\n","Epoch: 293/5000, Train Loss: 36.65521378950639, Valid Loss: 38.71135711669922\n","Epoch: 294/5000, Train Loss: 36.62339331886985, Valid Loss: 38.64264424641927\n","Epoch: 295/5000, Train Loss: 36.62001835216176, Valid Loss: 38.63571802775065\n","Epoch: 296/5000, Train Loss: 36.4383395801891, Valid Loss: 38.604043324788414\n","Epoch: 297/5000, Train Loss: 36.622548536820844, Valid Loss: 38.60919698079427\n","Epoch: 298/5000, Train Loss: 36.59683470292525, Valid Loss: 38.53104146321615\n","Epoch: 299/5000, Train Loss: 36.57064645940607, Valid Loss: 38.494378407796226\n","Epoch: 300/5000, Train Loss: 36.422631697221235, Valid Loss: 38.4759890238444\n","Epoch: 301/5000, Train Loss: 36.37470071965998, Valid Loss: 38.37994511922201\n","Epoch: 302/5000, Train Loss: 36.23759807239879, Valid Loss: 38.31473032633463\n","Epoch: 303/5000, Train Loss: 36.30845954201438, Valid Loss: 38.287924448649086\n","Epoch: 304/5000, Train Loss: 36.08026192405007, Valid Loss: 38.2964235941569\n","Epoch: 305/5000, Train Loss: 36.27586607499556, Valid Loss: 38.29041290283203\n","Epoch: 306/5000, Train Loss: 36.25737623734908, Valid Loss: 38.27666727701823\n","Epoch: 307/5000, Train Loss: 36.18454811789773, Valid Loss: 38.16912968953451\n","Epoch: 308/5000, Train Loss: 36.00315198031339, Valid Loss: 38.08448791503906\n","Epoch: 309/5000, Train Loss: 36.04623031616211, Valid Loss: 37.966651916503906\n","Epoch: 310/5000, Train Loss: 35.91931967301802, Valid Loss: 37.92986806233724\n","Epoch: 311/5000, Train Loss: 35.911401922052555, Valid Loss: 37.93076960245768\n","Epoch: 312/5000, Train Loss: 35.882692857222125, Valid Loss: 37.949301401774086\n","Epoch: 313/5000, Train Loss: 35.88990853049538, Valid Loss: 37.885824839274086\n","Epoch: 314/5000, Train Loss: 35.8129997253418, Valid Loss: 37.826525370279946\n","Epoch: 315/5000, Train Loss: 35.8300895690918, Valid Loss: 37.80623118082682\n","Epoch: 316/5000, Train Loss: 35.727295962246984, Valid Loss: 37.829461415608726\n","Epoch: 317/5000, Train Loss: 35.59546175870028, Valid Loss: 37.75413131713867\n","Epoch: 318/5000, Train Loss: 35.70935197310014, Valid Loss: 37.75251134236654\n","Epoch: 319/5000, Train Loss: 35.59145285866477, Valid Loss: 37.70475387573242\n","Epoch: 320/5000, Train Loss: 35.63583269986239, Valid Loss: 37.73472468058268\n","Epoch: 321/5000, Train Loss: 35.33290897716176, Valid Loss: 37.651693979899086\n","Epoch: 322/5000, Train Loss: 35.51212935014205, Valid Loss: 37.55309931437174\n","Epoch: 323/5000, Train Loss: 35.55582636052912, Valid Loss: 37.56793212890625\n","Epoch: 324/5000, Train Loss: 35.58581473610618, Valid Loss: 37.565484364827476\n","Epoch: 325/5000, Train Loss: 35.48289004239169, Valid Loss: 37.43722407023112\n","Epoch: 326/5000, Train Loss: 35.38407030972567, Valid Loss: 37.45841979980469\n","Epoch: 327/5000, Train Loss: 35.45066035877574, Valid Loss: 37.40145492553711\n","Epoch: 328/5000, Train Loss: 35.443985332142226, Valid Loss: 37.39535140991211\n","Epoch: 329/5000, Train Loss: 35.08901804143732, Valid Loss: 37.3071034749349\n","Epoch: 330/5000, Train Loss: 35.186261957341976, Valid Loss: 37.28612518310547\n","Epoch: 331/5000, Train Loss: 35.376770366321914, Valid Loss: 37.16934585571289\n","Epoch: 332/5000, Train Loss: 35.03877674449574, Valid Loss: 37.22667566935221\n","Epoch: 333/5000, Train Loss: 35.08487458662553, Valid Loss: 37.086263020833336\n","Epoch: 334/5000, Train Loss: 35.05118734186346, Valid Loss: 37.025796254475914\n","Epoch: 335/5000, Train Loss: 34.99335445057262, Valid Loss: 37.04577382405599\n","Epoch: 336/5000, Train Loss: 35.10607060519132, Valid Loss: 37.087642669677734\n","Epoch: 337/5000, Train Loss: 34.89903397993608, Valid Loss: 37.01681264241537\n","Epoch: 338/5000, Train Loss: 35.01940189708363, Valid Loss: 36.92359288533529\n","Epoch: 339/5000, Train Loss: 35.062745874578304, Valid Loss: 36.91036605834961\n","Epoch: 340/5000, Train Loss: 34.95926423506303, Valid Loss: 36.88642501831055\n","Epoch: 341/5000, Train Loss: 34.708309867165305, Valid Loss: 36.78824234008789\n","Epoch: 342/5000, Train Loss: 34.65429028597745, Valid Loss: 36.8244260152181\n","Epoch: 343/5000, Train Loss: 34.6932934847745, Valid Loss: 36.84345372517904\n","Epoch: 344/5000, Train Loss: 34.69151011380282, Valid Loss: 36.80408604939779\n","Epoch: 345/5000, Train Loss: 34.572368448430844, Valid Loss: 36.626321156819664\n","Epoch: 346/5000, Train Loss: 34.70626796375621, Valid Loss: 36.67107009887695\n","Epoch: 347/5000, Train Loss: 34.74902967973189, Valid Loss: 36.659043629964195\n","Epoch: 348/5000, Train Loss: 34.468371477994054, Valid Loss: 36.66504923502604\n","Epoch: 349/5000, Train Loss: 34.59812424399636, Valid Loss: 36.661659240722656\n","Epoch: 350/5000, Train Loss: 34.53258219632235, Valid Loss: 36.637655893961586\n","Epoch: 351/5000, Train Loss: 34.20978979630904, Valid Loss: 36.51122919718424\n","Epoch: 352/5000, Train Loss: 34.450116937810726, Valid Loss: 36.44442621866862\n","Epoch: 353/5000, Train Loss: 34.29315896467729, Valid Loss: 36.45252354939779\n","Epoch: 354/5000, Train Loss: 34.224610415371984, Valid Loss: 36.45736312866211\n","Epoch: 355/5000, Train Loss: 34.51286298578436, Valid Loss: 36.37840143839518\n","Epoch: 356/5000, Train Loss: 34.42556103793058, Valid Loss: 36.43792597452799\n","Epoch: 357/5000, Train Loss: 34.3445070440119, Valid Loss: 36.37787628173828\n","Epoch: 358/5000, Train Loss: 34.368926308371805, Valid Loss: 36.37513224283854\n","Epoch: 359/5000, Train Loss: 34.23822038823908, Valid Loss: 36.318976084391274\n","Epoch: 360/5000, Train Loss: 34.09912924333052, Valid Loss: 36.257720947265625\n","Epoch: 361/5000, Train Loss: 33.937801361083984, Valid Loss: 36.16489791870117\n","Epoch: 362/5000, Train Loss: 34.28989791870117, Valid Loss: 36.137657165527344\n","Epoch: 363/5000, Train Loss: 33.98797798156738, Valid Loss: 36.072672526041664\n","Epoch: 364/5000, Train Loss: 33.821623888882726, Valid Loss: 36.157449086507164\n","Epoch: 365/5000, Train Loss: 34.193856326016515, Valid Loss: 36.08197530110677\n","Epoch: 366/5000, Train Loss: 33.96448187394576, Valid Loss: 35.97701517740885\n","Epoch: 367/5000, Train Loss: 33.92954791675914, Valid Loss: 35.97094472249349\n","Epoch: 368/5000, Train Loss: 34.14504970203746, Valid Loss: 35.94538879394531\n","Epoch: 369/5000, Train Loss: 33.97233200073242, Valid Loss: 35.95908864339193\n","Epoch: 370/5000, Train Loss: 33.96231946078214, Valid Loss: 35.87815475463867\n","Epoch: 371/5000, Train Loss: 33.68481930819425, Valid Loss: 35.825243631998696\n","Epoch: 372/5000, Train Loss: 33.89946191961115, Valid Loss: 35.80228042602539\n","Epoch: 373/5000, Train Loss: 33.94031177867543, Valid Loss: 35.89727910359701\n","Epoch: 374/5000, Train Loss: 33.36479967290705, Valid Loss: 35.79273478190104\n","Epoch: 375/5000, Train Loss: 33.70044500177557, Valid Loss: 35.80960210164388\n","Epoch: 376/5000, Train Loss: 33.506431579589844, Valid Loss: 35.8015988667806\n","Epoch: 377/5000, Train Loss: 33.61616689508612, Valid Loss: 35.7752431233724\n","Epoch: 378/5000, Train Loss: 33.80006859519265, Valid Loss: 35.65556844075521\n","Epoch: 379/5000, Train Loss: 33.60720374367454, Valid Loss: 35.66885121663412\n","Epoch: 380/5000, Train Loss: 33.53580925681374, Valid Loss: 35.63278834025065\n","Epoch: 381/5000, Train Loss: 33.54439700733531, Valid Loss: 35.5443967183431\n","Epoch: 382/5000, Train Loss: 33.59425839510831, Valid Loss: 35.582950592041016\n","Epoch: 383/5000, Train Loss: 33.5704213922674, Valid Loss: 35.541882832845054\n","Epoch: 384/5000, Train Loss: 33.40378882668235, Valid Loss: 35.54086430867513\n","Epoch: 385/5000, Train Loss: 33.42495432767001, Valid Loss: 35.54016240437826\n","Epoch: 386/5000, Train Loss: 33.25115672024813, Valid Loss: 35.437127431233726\n","Epoch: 387/5000, Train Loss: 33.498617519031875, Valid Loss: 35.47192891438802\n","Epoch: 388/5000, Train Loss: 33.171865463256836, Valid Loss: 35.40405019124349\n","Epoch: 389/5000, Train Loss: 33.44241610440341, Valid Loss: 35.36919148763021\n","Epoch: 390/5000, Train Loss: 33.23902355540883, Valid Loss: 35.39619572957357\n","Epoch: 391/5000, Train Loss: 33.6545444835316, Valid Loss: 35.44318135579427\n","Epoch: 392/5000, Train Loss: 33.265356584028765, Valid Loss: 35.361437479654946\n","Epoch: 393/5000, Train Loss: 33.14505039561879, Valid Loss: 35.280503590901695\n","Epoch: 394/5000, Train Loss: 33.08641884543679, Valid Loss: 35.28278350830078\n","Epoch: 395/5000, Train Loss: 33.4008657282049, Valid Loss: 35.311370849609375\n","Epoch: 396/5000, Train Loss: 33.162629560990766, Valid Loss: 35.25118509928385\n","Epoch: 397/5000, Train Loss: 33.2423399144953, Valid Loss: 35.23357009887695\n","Epoch: 398/5000, Train Loss: 33.10613718899813, Valid Loss: 35.16600672403971\n","Epoch: 399/5000, Train Loss: 33.452152425592594, Valid Loss: 35.1144053141276\n","Epoch: 400/5000, Train Loss: 32.982260097156875, Valid Loss: 35.13373819986979\n","Epoch: 401/5000, Train Loss: 33.078311226584695, Valid Loss: 35.0834592183431\n","Epoch: 402/5000, Train Loss: 33.00565858320756, Valid Loss: 35.10653940836588\n","Epoch: 403/5000, Train Loss: 32.9572845805775, Valid Loss: 35.07724253336588\n","Epoch: 404/5000, Train Loss: 33.241521488536485, Valid Loss: 35.06512324015299\n","Epoch: 405/5000, Train Loss: 32.725053960626774, Valid Loss: 35.03165817260742\n","Epoch: 406/5000, Train Loss: 32.72061174566095, Valid Loss: 34.981852213541664\n","Epoch: 407/5000, Train Loss: 32.986765428022906, Valid Loss: 34.987298329671226\n","Epoch: 408/5000, Train Loss: 32.8203125, Valid Loss: 34.92629623413086\n","Epoch: 409/5000, Train Loss: 33.107952464710586, Valid Loss: 34.94800313313802\n","Epoch: 410/5000, Train Loss: 32.73850545016202, Valid Loss: 34.91380055745443\n","Epoch: 411/5000, Train Loss: 32.97503610090776, Valid Loss: 34.90782038370768\n","Epoch: 412/5000, Train Loss: 32.83140910755504, Valid Loss: 34.88956069946289\n","Epoch: 413/5000, Train Loss: 32.749989943070844, Valid Loss: 34.79144541422526\n","Epoch: 414/5000, Train Loss: 32.58717016740279, Valid Loss: 34.761454264322914\n","Epoch: 415/5000, Train Loss: 32.677250081842594, Valid Loss: 34.73599370320638\n","Epoch: 416/5000, Train Loss: 32.77700580250133, Valid Loss: 34.74262364705404\n","Epoch: 417/5000, Train Loss: 32.47963263771751, Valid Loss: 34.73158264160156\n","Epoch: 418/5000, Train Loss: 32.55431279269132, Valid Loss: 34.67201360066732\n","Epoch: 419/5000, Train Loss: 32.459562648426406, Valid Loss: 34.66875712076823\n","Epoch: 420/5000, Train Loss: 32.66541567715731, Valid Loss: 34.70398712158203\n","Epoch: 421/5000, Train Loss: 32.652578353881836, Valid Loss: 34.64226150512695\n","Epoch: 422/5000, Train Loss: 32.28975226662376, Valid Loss: 34.69622039794922\n","Epoch: 423/5000, Train Loss: 32.39649633927779, Valid Loss: 34.60013326009115\n","Epoch: 424/5000, Train Loss: 32.571102662519976, Valid Loss: 34.55882771809896\n","Epoch: 425/5000, Train Loss: 32.48103263161399, Valid Loss: 34.6283327738444\n","Epoch: 426/5000, Train Loss: 32.624472704800695, Valid Loss: 34.560805002848305\n","Epoch: 427/5000, Train Loss: 32.505742679942735, Valid Loss: 34.5443483988444\n","Epoch: 428/5000, Train Loss: 32.52566753734242, Valid Loss: 34.52232233683268\n","Epoch: 429/5000, Train Loss: 32.36512305519798, Valid Loss: 34.52898279825846\n","Epoch: 430/5000, Train Loss: 32.559879996559836, Valid Loss: 34.47455724080404\n","Epoch: 431/5000, Train Loss: 32.54804177717729, Valid Loss: 34.383538564046226\n","Epoch: 432/5000, Train Loss: 32.32494215531783, Valid Loss: 34.46892547607422\n","Epoch: 433/5000, Train Loss: 32.280461918223985, Valid Loss: 34.4029286702474\n","Epoch: 434/5000, Train Loss: 32.49196416681463, Valid Loss: 34.4419199625651\n","Epoch: 435/5000, Train Loss: 32.452404195612125, Valid Loss: 34.45038986206055\n","Epoch: 436/5000, Train Loss: 32.292533527721055, Valid Loss: 34.34805425008138\n","Epoch: 437/5000, Train Loss: 32.172333457253195, Valid Loss: 34.267496744791664\n","Epoch: 438/5000, Train Loss: 32.184784629128195, Valid Loss: 34.27490743001302\n","Epoch: 439/5000, Train Loss: 32.23567511818626, Valid Loss: 34.270530700683594\n","Epoch: 440/5000, Train Loss: 32.27314775640314, Valid Loss: 34.21778678894043\n","Epoch: 441/5000, Train Loss: 31.936764283613726, Valid Loss: 34.22969754536947\n","Epoch: 442/5000, Train Loss: 32.444026947021484, Valid Loss: 34.23932329813639\n","Epoch: 443/5000, Train Loss: 31.958867333152078, Valid Loss: 34.20198122660319\n","Epoch: 444/5000, Train Loss: 32.25364026156339, Valid Loss: 34.16009267171224\n","Epoch: 445/5000, Train Loss: 32.157550464976914, Valid Loss: 34.19800059000651\n","Epoch: 446/5000, Train Loss: 32.05571538751776, Valid Loss: 34.18498675028483\n","Epoch: 447/5000, Train Loss: 31.816695300015535, Valid Loss: 34.168466567993164\n","Epoch: 448/5000, Train Loss: 32.113849293101914, Valid Loss: 34.131565729777016\n","Epoch: 449/5000, Train Loss: 31.900096199729226, Valid Loss: 34.12747701009115\n","Epoch: 450/5000, Train Loss: 31.955048821189187, Valid Loss: 34.11590067545573\n","Epoch: 451/5000, Train Loss: 31.97947276722301, Valid Loss: 34.116068522135414\n","Epoch: 452/5000, Train Loss: 31.96883132240989, Valid Loss: 34.07136090596517\n","Epoch: 453/5000, Train Loss: 32.053132664073594, Valid Loss: 34.10725657145182\n","Epoch: 454/5000, Train Loss: 31.987030029296875, Valid Loss: 34.01535224914551\n","Epoch: 455/5000, Train Loss: 32.18886306069114, Valid Loss: 33.98041025797526\n","Epoch: 456/5000, Train Loss: 32.41896369240501, Valid Loss: 34.043829600016274\n","Epoch: 457/5000, Train Loss: 32.051528063687414, Valid Loss: 33.97493108113607\n","Epoch: 458/5000, Train Loss: 31.56524623524059, Valid Loss: 33.97746594746908\n","Epoch: 459/5000, Train Loss: 31.814720327203926, Valid Loss: 33.97859128316244\n","Epoch: 460/5000, Train Loss: 31.70955449884588, Valid Loss: 33.968671798706055\n","Epoch: 461/5000, Train Loss: 31.965825167569246, Valid Loss: 33.91972096761068\n","Epoch: 462/5000, Train Loss: 31.853758205067027, Valid Loss: 33.914999643961586\n","Epoch: 463/5000, Train Loss: 31.821982990611684, Valid Loss: 33.91197204589844\n","Epoch: 464/5000, Train Loss: 31.87568404457786, Valid Loss: 33.91462961832682\n","Epoch: 465/5000, Train Loss: 31.910646438598633, Valid Loss: 33.908071517944336\n","Epoch: 466/5000, Train Loss: 32.060742985118516, Valid Loss: 33.9063663482666\n","Epoch: 467/5000, Train Loss: 31.827091737227008, Valid Loss: 33.85681406656901\n","Epoch: 468/5000, Train Loss: 31.79802669178356, Valid Loss: 33.876047134399414\n","Epoch: 469/5000, Train Loss: 31.47004508972168, Valid Loss: 33.83208465576172\n","Epoch: 470/5000, Train Loss: 31.954791675914418, Valid Loss: 33.82641092936198\n","Epoch: 471/5000, Train Loss: 31.59581323103471, Valid Loss: 33.800624211629234\n","Epoch: 472/5000, Train Loss: 31.607966509732332, Valid Loss: 33.76173528035482\n","Epoch: 473/5000, Train Loss: 31.548275167291816, Valid Loss: 33.79803721110026\n","Epoch: 474/5000, Train Loss: 31.70818363536488, Valid Loss: 33.75679397583008\n","Epoch: 475/5000, Train Loss: 31.79483916542747, Valid Loss: 33.738868713378906\n","Epoch: 476/5000, Train Loss: 31.66799406571822, Valid Loss: 33.742161432902016\n","Epoch: 477/5000, Train Loss: 31.859519611705434, Valid Loss: 33.69052759806315\n","Epoch: 478/5000, Train Loss: 31.37023318897594, Valid Loss: 33.63769022623698\n","Epoch: 479/5000, Train Loss: 31.743736960671164, Valid Loss: 33.6528123219808\n","Epoch: 480/5000, Train Loss: 31.84163613752885, Valid Loss: 33.640801111857094\n","Epoch: 481/5000, Train Loss: 31.447745409878817, Valid Loss: 33.64510854085287\n","Epoch: 482/5000, Train Loss: 31.657161019065164, Valid Loss: 33.66205724080404\n","Epoch: 483/5000, Train Loss: 31.456111734563653, Valid Loss: 33.63762219746908\n","Epoch: 484/5000, Train Loss: 31.418853933160957, Valid Loss: 33.61003748575846\n","Epoch: 485/5000, Train Loss: 31.48481663790616, Valid Loss: 33.58572133382162\n","Epoch: 486/5000, Train Loss: 31.48529780994762, Valid Loss: 33.57160441080729\n","Epoch: 487/5000, Train Loss: 31.47910620949485, Valid Loss: 33.601877212524414\n","Epoch: 488/5000, Train Loss: 31.463852622292258, Valid Loss: 33.55547777811686\n","Epoch: 489/5000, Train Loss: 31.45932960510254, Valid Loss: 33.50628217061361\n","Epoch: 490/5000, Train Loss: 31.586353822187945, Valid Loss: 33.55774815877279\n","Epoch: 491/5000, Train Loss: 31.31853554465554, Valid Loss: 33.56291389465332\n","Epoch: 492/5000, Train Loss: 31.602684887972746, Valid Loss: 33.52534484863281\n","Epoch: 493/5000, Train Loss: 31.623109817504883, Valid Loss: 33.535881678263344\n","Epoch: 494/5000, Train Loss: 31.377698031338777, Valid Loss: 33.508039474487305\n","Epoch: 495/5000, Train Loss: 30.988440947099164, Valid Loss: 33.46638425191244\n","Epoch: 496/5000, Train Loss: 31.737654425881125, Valid Loss: 33.47096633911133\n","Epoch: 497/5000, Train Loss: 31.612179496071555, Valid Loss: 33.48405202229818\n","Epoch: 498/5000, Train Loss: 31.331591172651812, Valid Loss: 33.51630719502767\n","Epoch: 499/5000, Train Loss: 31.587725726040926, Valid Loss: 33.46781539916992\n","Epoch: 500/5000, Train Loss: 31.522078254006125, Valid Loss: 33.44404665629069\n","Epoch: 501/5000, Train Loss: 31.32177699695934, Valid Loss: 33.439108530680336\n","Epoch: 502/5000, Train Loss: 31.563432693481445, Valid Loss: 33.38244946797689\n","Epoch: 503/5000, Train Loss: 31.26802773909135, Valid Loss: 33.3868153889974\n","Epoch: 504/5000, Train Loss: 31.23494495045055, Valid Loss: 33.388078689575195\n","Epoch: 505/5000, Train Loss: 31.364268563010476, Valid Loss: 33.39340400695801\n","Epoch: 506/5000, Train Loss: 31.492672139948066, Valid Loss: 33.40564854939779\n","Epoch: 507/5000, Train Loss: 31.32811338251287, Valid Loss: 33.37344106038412\n","Epoch: 508/5000, Train Loss: 31.02094979719682, Valid Loss: 33.34787686665853\n","Epoch: 509/5000, Train Loss: 31.0374773198908, Valid Loss: 33.37957636515299\n","Epoch: 510/5000, Train Loss: 31.3549392006614, Valid Loss: 33.40271313985189\n","Epoch: 511/5000, Train Loss: 31.056684667413887, Valid Loss: 33.35716692606608\n","Epoch: 512/5000, Train Loss: 31.09857940673828, Valid Loss: 33.3514518737793\n","Epoch: 513/5000, Train Loss: 31.10411765358665, Valid Loss: 33.362905502319336\n","Epoch: 514/5000, Train Loss: 31.195158698342063, Valid Loss: 33.29348055521647\n","Epoch: 515/5000, Train Loss: 31.223529122092508, Valid Loss: 33.26977221171061\n","Epoch: 516/5000, Train Loss: 31.21702714399858, Valid Loss: 33.27523676554362\n","Epoch: 517/5000, Train Loss: 31.131362915039062, Valid Loss: 33.29898770650228\n","Epoch: 518/5000, Train Loss: 31.138704646717418, Valid Loss: 33.33018430074056\n","Epoch: 519/5000, Train Loss: 31.009177988225762, Valid Loss: 33.282463709513344\n","Epoch: 520/5000, Train Loss: 31.30229915272106, Valid Loss: 33.25707817077637\n","Epoch: 521/5000, Train Loss: 31.17159791426225, Valid Loss: 33.23762194315592\n","Epoch: 522/5000, Train Loss: 31.273569280450996, Valid Loss: 33.2453498840332\n","Epoch: 523/5000, Train Loss: 31.348571603948418, Valid Loss: 33.27826118469238\n","Epoch: 524/5000, Train Loss: 31.16406839544123, Valid Loss: 33.25778007507324\n","Epoch: 525/5000, Train Loss: 31.322837482799184, Valid Loss: 33.241387049357094\n","Epoch: 526/5000, Train Loss: 31.201079455288973, Valid Loss: 33.236817042032875\n","Epoch: 527/5000, Train Loss: 31.13467649980025, Valid Loss: 33.221309661865234\n","Epoch: 528/5000, Train Loss: 31.214698271317914, Valid Loss: 33.21027755737305\n","Epoch: 529/5000, Train Loss: 31.013937863436613, Valid Loss: 33.21000544230143\n","Epoch: 530/5000, Train Loss: 31.483806783502754, Valid Loss: 33.20610491434733\n","Epoch: 531/5000, Train Loss: 30.876407623291016, Valid Loss: 33.211021423339844\n","Epoch: 532/5000, Train Loss: 31.091199354691938, Valid Loss: 33.15782801310221\n","Epoch: 533/5000, Train Loss: 30.68895894830877, Valid Loss: 33.178656260172524\n","Epoch: 534/5000, Train Loss: 31.09297665682706, Valid Loss: 33.18974749247233\n","Epoch: 535/5000, Train Loss: 31.205360412597656, Valid Loss: 33.15190633138021\n","Epoch: 536/5000, Train Loss: 31.4217191175981, Valid Loss: 33.17646916707357\n","Epoch: 537/5000, Train Loss: 30.89940019087358, Valid Loss: 33.158159255981445\n","Epoch: 538/5000, Train Loss: 31.262420307506215, Valid Loss: 33.150482177734375\n","Epoch: 539/5000, Train Loss: 30.78966123407537, Valid Loss: 33.14869499206543\n","Epoch: 540/5000, Train Loss: 31.160659790039062, Valid Loss: 33.14545694986979\n","Epoch: 541/5000, Train Loss: 30.892309535633434, Valid Loss: 33.15507443745931\n","Epoch: 542/5000, Train Loss: 30.843759883533824, Valid Loss: 33.16541989644369\n","Epoch: 543/5000, Train Loss: 31.037067066539418, Valid Loss: 33.10476303100586\n","Epoch: 544/5000, Train Loss: 30.867225126786664, Valid Loss: 33.09160359700521\n","Epoch: 545/5000, Train Loss: 31.118154352361504, Valid Loss: 33.11813227335612\n","Epoch: 546/5000, Train Loss: 30.845732255415484, Valid Loss: 33.13459014892578\n","Epoch: 547/5000, Train Loss: 30.879928935657848, Valid Loss: 33.14740498860677\n","Epoch: 548/5000, Train Loss: 31.10331951488148, Valid Loss: 33.12994893391927\n","Epoch: 549/5000, Train Loss: 31.16548694263805, Valid Loss: 33.114203770955406\n","Epoch: 550/5000, Train Loss: 31.069720354947176, Valid Loss: 33.106910705566406\n","Epoch: 551/5000, Train Loss: 31.07080875743519, Valid Loss: 33.066505432128906\n","Epoch: 552/5000, Train Loss: 31.084725640036844, Valid Loss: 33.12225850423177\n","Epoch: 553/5000, Train Loss: 30.725696216930043, Valid Loss: 33.08283233642578\n","Epoch: 554/5000, Train Loss: 30.94923175464977, Valid Loss: 33.054592768351235\n","Epoch: 555/5000, Train Loss: 31.001343293623492, Valid Loss: 33.09015337626139\n","Epoch: 556/5000, Train Loss: 30.92132499001243, Valid Loss: 33.02796745300293\n","Epoch: 557/5000, Train Loss: 30.937970421530984, Valid Loss: 33.02974383036295\n","Epoch: 558/5000, Train Loss: 31.20161836797541, Valid Loss: 33.073811848958336\n","Epoch: 559/5000, Train Loss: 30.963952844793145, Valid Loss: 33.05804443359375\n","Epoch: 560/5000, Train Loss: 30.883128599687055, Valid Loss: 32.99696032206217\n","Epoch: 561/5000, Train Loss: 31.01958552273837, Valid Loss: 32.97000376383463\n","Epoch: 562/5000, Train Loss: 30.822871641679242, Valid Loss: 33.003180185953774\n","Epoch: 563/5000, Train Loss: 30.796199278397992, Valid Loss: 33.01594479878744\n","Epoch: 564/5000, Train Loss: 30.872431321577594, Valid Loss: 33.03383191426595\n","Epoch: 565/5000, Train Loss: 30.722770170731977, Valid Loss: 33.00891876220703\n","Epoch: 566/5000, Train Loss: 30.713417053222656, Valid Loss: 33.00241788228353\n","Epoch: 567/5000, Train Loss: 31.251488425514914, Valid Loss: 33.0003350575765\n","Epoch: 568/5000, Train Loss: 30.95606665177779, Valid Loss: 32.98305002848307\n","Epoch: 569/5000, Train Loss: 31.07720184326172, Valid Loss: 32.98358662923177\n","Epoch: 570/5000, Train Loss: 30.92747827009721, Valid Loss: 32.96058336893717\n","Epoch: 571/5000, Train Loss: 30.850427454168145, Valid Loss: 32.935245513916016\n","Epoch: 572/5000, Train Loss: 30.906249133023348, Valid Loss: 32.9446824391683\n","Epoch: 573/5000, Train Loss: 30.807619615034625, Valid Loss: 32.967031478881836\n","Epoch: 574/5000, Train Loss: 30.94972211664373, Valid Loss: 32.96423149108887\n","Epoch: 575/5000, Train Loss: 30.921952334317293, Valid Loss: 32.942849477132164\n","Epoch: 576/5000, Train Loss: 30.829345529729668, Valid Loss: 32.9454542795817\n","Epoch: 577/5000, Train Loss: 30.734583941372957, Valid Loss: 32.91156260172526\n","Epoch: 578/5000, Train Loss: 30.79292071949352, Valid Loss: 32.899997075398765\n","Epoch: 579/5000, Train Loss: 30.739681764082476, Valid Loss: 32.92854690551758\n","Epoch: 580/5000, Train Loss: 30.787659905173562, Valid Loss: 32.90011342366537\n","Epoch: 581/5000, Train Loss: 31.026322104714133, Valid Loss: 32.928470611572266\n","Epoch: 582/5000, Train Loss: 30.744476318359375, Valid Loss: 32.916660944620766\n","Epoch: 583/5000, Train Loss: 30.761321848089043, Valid Loss: 32.91653696695963\n","Epoch: 584/5000, Train Loss: 30.84181039983576, Valid Loss: 32.90495681762695\n","Epoch: 585/5000, Train Loss: 30.908138275146484, Valid Loss: 32.91322390238444\n","Epoch: 586/5000, Train Loss: 30.89000025662509, Valid Loss: 32.92430114746094\n","Epoch: 587/5000, Train Loss: 31.239229028875176, Valid Loss: 32.94082514444987\n","Epoch: 588/5000, Train Loss: 31.01023465936834, Valid Loss: 32.886505126953125\n","Epoch: 589/5000, Train Loss: 30.999223882501777, Valid Loss: 32.861019134521484\n","Epoch: 590/5000, Train Loss: 30.907651554454457, Valid Loss: 32.893428802490234\n","Epoch: 591/5000, Train Loss: 30.83030926097523, Valid Loss: 32.89850362141927\n","Epoch: 592/5000, Train Loss: 30.960302699695934, Valid Loss: 32.87541389465332\n","Epoch: 593/5000, Train Loss: 30.57655317133123, Valid Loss: 32.879652659098305\n","Epoch: 594/5000, Train Loss: 30.884204864501953, Valid Loss: 32.87560844421387\n","Epoch: 595/5000, Train Loss: 30.737610036676582, Valid Loss: 32.86969820658366\n","Epoch: 596/5000, Train Loss: 30.57059010592374, Valid Loss: 32.89387067159017\n","Epoch: 597/5000, Train Loss: 30.992496490478516, Valid Loss: 32.89410909016927\n","Epoch: 598/5000, Train Loss: 30.663430994207207, Valid Loss: 32.91257413228353\n","Epoch: 599/5000, Train Loss: 30.6383262981068, Valid Loss: 32.90120887756348\n","Epoch: 600/5000, Train Loss: 30.711438265713777, Valid Loss: 32.9184201558431\n","Epoch: 601/5000, Train Loss: 30.868989077481356, Valid Loss: 32.91273053487142\n","Epoch: 602/5000, Train Loss: 30.61262252114036, Valid Loss: 32.861918131510414\n","Epoch: 603/5000, Train Loss: 30.557329177856445, Valid Loss: 32.85907173156738\n","Epoch: 604/5000, Train Loss: 30.669463244351473, Valid Loss: 32.82993125915527\n","Epoch: 605/5000, Train Loss: 30.684951435435902, Valid Loss: 32.858357747395836\n","Epoch: 606/5000, Train Loss: 30.593800284645774, Valid Loss: 32.88760121663412\n","Epoch: 607/5000, Train Loss: 30.688110871748492, Valid Loss: 32.880104064941406\n","Epoch: 608/5000, Train Loss: 30.936448357322, Valid Loss: 32.85301272074381\n","Epoch: 609/5000, Train Loss: 30.8291898207231, Valid Loss: 32.86444218953451\n","Epoch: 610/5000, Train Loss: 30.235687082464043, Valid Loss: 32.84119733174642\n","Epoch: 611/5000, Train Loss: 30.863515507091176, Valid Loss: 32.8701286315918\n","Epoch: 612/5000, Train Loss: 30.626922954212535, Valid Loss: 32.85351371765137\n","Epoch: 613/5000, Train Loss: 30.86859096180309, Valid Loss: 32.85183080037435\n","Epoch: 614/5000, Train Loss: 30.75384625521573, Valid Loss: 32.85148366292318\n","Epoch: 615/5000, Train Loss: 30.83698203346946, Valid Loss: 32.838080724080406\n","Epoch: 616/5000, Train Loss: 30.75716746937145, Valid Loss: 32.82175064086914\n","Epoch: 617/5000, Train Loss: 30.88619041442871, Valid Loss: 32.8186092376709\n","Epoch: 618/5000, Train Loss: 30.528271761807527, Valid Loss: 32.827579498291016\n","Epoch: 619/5000, Train Loss: 30.655812176791105, Valid Loss: 32.8575808207194\n","Epoch: 620/5000, Train Loss: 30.58734668384899, Valid Loss: 32.81105105082194\n","Epoch: 621/5000, Train Loss: 31.04463715986772, Valid Loss: 32.83567682902018\n","Epoch: 622/5000, Train Loss: 30.6610520102761, Valid Loss: 32.80886268615723\n","Epoch: 623/5000, Train Loss: 30.56566654552113, Valid Loss: 32.79545211791992\n","Epoch: 624/5000, Train Loss: 30.87733442133123, Valid Loss: 32.80227851867676\n","Epoch: 625/5000, Train Loss: 30.91918269070712, Valid Loss: 32.822596867879234\n","Epoch: 626/5000, Train Loss: 30.904761574485086, Valid Loss: 32.834302266438804\n","Epoch: 627/5000, Train Loss: 30.918963519009676, Valid Loss: 32.82486915588379\n","Epoch: 628/5000, Train Loss: 30.68888144059615, Valid Loss: 32.827685038248696\n","Epoch: 629/5000, Train Loss: 30.533674413507637, Valid Loss: 32.78197733561198\n","Epoch: 630/5000, Train Loss: 30.888905958695844, Valid Loss: 32.777425130208336\n","Epoch: 631/5000, Train Loss: 30.922437494451348, Valid Loss: 32.793599446614586\n","Epoch: 632/5000, Train Loss: 31.04416743191806, Valid Loss: 32.788506189982094\n","Epoch: 633/5000, Train Loss: 30.791574998335406, Valid Loss: 32.79477055867513\n","Epoch: 634/5000, Train Loss: 30.771160299127754, Valid Loss: 32.77209154764811\n","Epoch: 635/5000, Train Loss: 30.70252765308727, Valid Loss: 32.79064051310221\n","Epoch: 636/5000, Train Loss: 30.82307295365767, Valid Loss: 32.79626592000326\n","Epoch: 637/5000, Train Loss: 30.550389203158293, Valid Loss: 32.80306053161621\n","Epoch: 638/5000, Train Loss: 30.55489002574574, Valid Loss: 32.78823026021322\n","Epoch: 639/5000, Train Loss: 30.366998845880683, Valid Loss: 32.75884183247884\n","Epoch: 640/5000, Train Loss: 31.027897054498847, Valid Loss: 32.7811164855957\n","Epoch: 641/5000, Train Loss: 30.872180591930043, Valid Loss: 32.77247937520345\n","Epoch: 642/5000, Train Loss: 30.631315231323242, Valid Loss: 32.78625742594401\n","Epoch: 643/5000, Train Loss: 30.72499691356312, Valid Loss: 32.809757232666016\n","Epoch: 644/5000, Train Loss: 30.720216057517312, Valid Loss: 32.77787399291992\n","Epoch: 645/5000, Train Loss: 30.636140823364258, Valid Loss: 32.80722872416178\n","Epoch: 646/5000, Train Loss: 30.674184972589668, Valid Loss: 32.782400131225586\n","Epoch: 647/5000, Train Loss: 30.899171135642312, Valid Loss: 32.795976638793945\n","Epoch: 648/5000, Train Loss: 30.41383916681463, Valid Loss: 32.77214241027832\n","Epoch: 649/5000, Train Loss: 30.69321649724787, Valid Loss: 32.77537727355957\n","Epoch: 650/5000, Train Loss: 30.8533597425981, Valid Loss: 32.76312128702799\n","Epoch: 651/5000, Train Loss: 30.495561079545453, Valid Loss: 32.779144287109375\n","Epoch: 652/5000, Train Loss: 30.542526071721856, Valid Loss: 32.75851313273112\n","Epoch: 653/5000, Train Loss: 30.661928350275215, Valid Loss: 32.751200358072914\n","Epoch: 654/5000, Train Loss: 30.546124891801313, Valid Loss: 32.74444707234701\n","Epoch: 655/5000, Train Loss: 30.644532810557973, Valid Loss: 32.71842702229818\n","Epoch: 656/5000, Train Loss: 30.79095528342507, Valid Loss: 32.7362257639567\n","Epoch: 657/5000, Train Loss: 30.37619365345348, Valid Loss: 32.74807929992676\n","Epoch: 658/5000, Train Loss: 30.595729654485528, Valid Loss: 32.71192932128906\n","Epoch: 659/5000, Train Loss: 30.44483028758656, Valid Loss: 32.72536595662435\n","Epoch: 660/5000, Train Loss: 30.53022679415616, Valid Loss: 32.73228200276693\n","Epoch: 661/5000, Train Loss: 30.835498809814453, Valid Loss: 32.730437596639\n","Epoch: 662/5000, Train Loss: 30.62417238408869, Valid Loss: 32.740444819132485\n","Epoch: 663/5000, Train Loss: 30.698442285711113, Valid Loss: 32.75574239095052\n","Epoch: 664/5000, Train Loss: 30.706030758944426, Valid Loss: 32.73467254638672\n","Epoch: 665/5000, Train Loss: 30.521169142289594, Valid Loss: 32.719797134399414\n","Epoch: 666/5000, Train Loss: 30.698334607211027, Valid Loss: 32.71697171529134\n","Epoch: 667/5000, Train Loss: 30.589391881769355, Valid Loss: 32.70694160461426\n","Epoch: 668/5000, Train Loss: 30.599917845292524, Valid Loss: 32.72110811869303\n","Epoch: 669/5000, Train Loss: 30.548690449107777, Valid Loss: 32.72395579020182\n","Epoch: 670/5000, Train Loss: 30.552731947465375, Valid Loss: 32.74166615804037\n","Epoch: 671/5000, Train Loss: 30.720470775257457, Valid Loss: 32.74026362101237\n","Epoch: 672/5000, Train Loss: 30.5033371665261, Valid Loss: 32.73567263285319\n","Epoch: 673/5000, Train Loss: 30.550227598710492, Valid Loss: 32.723416010538735\n","Epoch: 674/5000, Train Loss: 30.857084967873313, Valid Loss: 32.719953536987305\n","Epoch: 675/5000, Train Loss: 30.207837191495027, Valid Loss: 32.719757080078125\n","Epoch: 676/5000, Train Loss: 30.35938228260387, Valid Loss: 32.722250620524086\n","Epoch: 677/5000, Train Loss: 30.44576506181197, Valid Loss: 32.70286178588867\n","Epoch: 678/5000, Train Loss: 30.798035534945402, Valid Loss: 32.710214614868164\n","Epoch: 679/5000, Train Loss: 30.538261240178887, Valid Loss: 32.68831443786621\n","Epoch: 680/5000, Train Loss: 30.473460457541726, Valid Loss: 32.70156796773275\n","Epoch: 681/5000, Train Loss: 30.317041743885387, Valid Loss: 32.68997891743978\n","Epoch: 682/5000, Train Loss: 30.58032850785689, Valid Loss: 32.68506622314453\n","Epoch: 683/5000, Train Loss: 30.4841532273726, Valid Loss: 32.699272791544594\n","Epoch: 684/5000, Train Loss: 30.614456350153144, Valid Loss: 32.688636779785156\n","Epoch: 685/5000, Train Loss: 30.58521877635609, Valid Loss: 32.6811580657959\n","Epoch: 686/5000, Train Loss: 30.824730439619586, Valid Loss: 32.71498235066732\n","Epoch: 687/5000, Train Loss: 30.49171395735307, Valid Loss: 32.702430725097656\n","Epoch: 688/5000, Train Loss: 30.7319410497492, Valid Loss: 32.70414670308431\n","Epoch: 689/5000, Train Loss: 30.50630864230069, Valid Loss: 32.69410959879557\n","Epoch: 690/5000, Train Loss: 30.450917504050516, Valid Loss: 32.71242904663086\n","Epoch: 691/5000, Train Loss: 30.660288897427645, Valid Loss: 32.69519869486491\n","Epoch: 692/5000, Train Loss: 30.4695517800071, Valid Loss: 32.6556142171224\n","Epoch: 693/5000, Train Loss: 30.442053881558504, Valid Loss: 32.68724505106608\n","Epoch: 694/5000, Train Loss: 30.9294771714644, Valid Loss: 32.67358652750651\n","Epoch: 695/5000, Train Loss: 30.455137079412285, Valid Loss: 32.713104248046875\n","Epoch: 696/5000, Train Loss: 30.514508160677824, Valid Loss: 32.683467864990234\n","Epoch: 697/5000, Train Loss: 30.48631338639693, Valid Loss: 32.686102549235024\n","Epoch: 698/5000, Train Loss: 30.714861783114348, Valid Loss: 32.69800821940104\n","Epoch: 699/5000, Train Loss: 30.434740933504973, Valid Loss: 32.67628733317057\n","Epoch: 700/5000, Train Loss: 30.506065715443004, Valid Loss: 32.66141573588053\n","Epoch: 701/5000, Train Loss: 30.402525988492098, Valid Loss: 32.66317494710287\n","Epoch: 702/5000, Train Loss: 30.463598944924094, Valid Loss: 32.67306391398112\n","Epoch: 703/5000, Train Loss: 30.45546999844638, Valid Loss: 32.70884704589844\n","Epoch: 704/5000, Train Loss: 30.3471943248402, Valid Loss: 32.69433848063151\n","Epoch: 705/5000, Train Loss: 30.67187361283736, Valid Loss: 32.69661776224772\n","Epoch: 706/5000, Train Loss: 30.323360443115234, Valid Loss: 32.684292475382485\n","Epoch: 707/5000, Train Loss: 30.768281589854848, Valid Loss: 32.67525545756022\n","Epoch: 708/5000, Train Loss: 30.610246138139203, Valid Loss: 32.70026652018229\n","Epoch: 709/5000, Train Loss: 30.55693938515403, Valid Loss: 32.69372367858887\n","Epoch: 710/5000, Train Loss: 30.670913349498402, Valid Loss: 32.68390337626139\n","Epoch: 711/5000, Train Loss: 30.438397494229402, Valid Loss: 32.702386220296226\n","Epoch: 712/5000, Train Loss: 30.412104519930754, Valid Loss: 32.67725372314453\n","Epoch: 713/5000, Train Loss: 30.444489912553266, Valid Loss: 32.6657772064209\n","Epoch: 714/5000, Train Loss: 30.24242002313787, Valid Loss: 32.66940561930338\n","Epoch: 715/5000, Train Loss: 30.639854257757012, Valid Loss: 32.67565663655599\n","Epoch: 716/5000, Train Loss: 30.531171278520063, Valid Loss: 32.68460909525553\n","Epoch: 717/5000, Train Loss: 30.552866502241656, Valid Loss: 32.67880058288574\n","Epoch: 718/5000, Train Loss: 30.443144884976473, Valid Loss: 32.68453025817871\n","Epoch: 719/5000, Train Loss: 30.32308006286621, Valid Loss: 32.67379951477051\n","Epoch: 720/5000, Train Loss: 30.41345891085538, Valid Loss: 32.669310887654625\n","Epoch: 721/5000, Train Loss: 30.412480267611418, Valid Loss: 32.676726659139\n","Epoch: 722/5000, Train Loss: 30.28271345658736, Valid Loss: 32.670653661092125\n","Epoch: 723/5000, Train Loss: 30.486115715720437, Valid Loss: 32.6612974802653\n","Epoch: 724/5000, Train Loss: 30.51688627763228, Valid Loss: 32.63134447733561\n","Epoch: 725/5000, Train Loss: 30.482947436246004, Valid Loss: 32.63445536295573\n","Epoch: 726/5000, Train Loss: 30.40441183610396, Valid Loss: 32.668233235677086\n","Epoch: 727/5000, Train Loss: 30.432096654718574, Valid Loss: 32.673086166381836\n","Epoch: 728/5000, Train Loss: 30.500818079168145, Valid Loss: 32.65972073872884\n","Epoch: 729/5000, Train Loss: 30.52946056019176, Valid Loss: 32.641395568847656\n","Epoch: 730/5000, Train Loss: 30.3891527002508, Valid Loss: 32.63692156473795\n","Epoch: 731/5000, Train Loss: 30.55982849814675, Valid Loss: 32.64215405782064\n","Epoch: 732/5000, Train Loss: 30.22288599881259, Valid Loss: 32.642303466796875\n","Epoch: 733/5000, Train Loss: 30.359182184392754, Valid Loss: 32.65032768249512\n","Epoch: 734/5000, Train Loss: 30.318205399946734, Valid Loss: 32.66312789916992\n","Epoch: 735/5000, Train Loss: 30.221556750210848, Valid Loss: 32.65810585021973\n","Epoch: 736/5000, Train Loss: 30.27661895751953, Valid Loss: 32.64207522074381\n","Epoch: 737/5000, Train Loss: 30.46410300514915, Valid Loss: 32.64718882242838\n","Epoch: 738/5000, Train Loss: 30.436128963123668, Valid Loss: 32.65139071146647\n","Epoch: 739/5000, Train Loss: 30.400699268687855, Valid Loss: 32.671176274617515\n","Epoch: 740/5000, Train Loss: 30.232858484441582, Valid Loss: 32.65987332661947\n","Epoch: 741/5000, Train Loss: 30.205910075794566, Valid Loss: 32.65413602193197\n","Epoch: 742/5000, Train Loss: 30.615042426369406, Valid Loss: 32.65452257792155\n","Epoch: 743/5000, Train Loss: 30.410801800814543, Valid Loss: 32.648877461751304\n","Epoch: 744/5000, Train Loss: 30.16613041270863, Valid Loss: 32.660182317097984\n","Epoch: 745/5000, Train Loss: 30.03137276389382, Valid Loss: 32.63661003112793\n","Epoch: 746/5000, Train Loss: 30.400068456476387, Valid Loss: 32.632118225097656\n","Epoch: 747/5000, Train Loss: 30.346195394342597, Valid Loss: 32.616360346476235\n","Epoch: 748/5000, Train Loss: 30.46152583035556, Valid Loss: 32.637275060017906\n","Epoch: 749/5000, Train Loss: 30.47248510880904, Valid Loss: 32.66497675577799\n","Epoch: 750/5000, Train Loss: 30.414061633023348, Valid Loss: 32.615199406941734\n","Epoch: 751/5000, Train Loss: 30.615869522094727, Valid Loss: 32.6110585530599\n","Epoch: 752/5000, Train Loss: 30.49885957891291, Valid Loss: 32.60453542073568\n","Epoch: 753/5000, Train Loss: 30.226435574618254, Valid Loss: 32.58779207865397\n","Epoch: 754/5000, Train Loss: 30.46636425365101, Valid Loss: 32.60778935750326\n","Epoch: 755/5000, Train Loss: 30.430491013960406, Valid Loss: 32.62817509969076\n","Epoch: 756/5000, Train Loss: 30.602894869717684, Valid Loss: 32.61219088236491\n","Epoch: 757/5000, Train Loss: 30.43840859153054, Valid Loss: 32.62653732299805\n","Epoch: 758/5000, Train Loss: 30.29642850702459, Valid Loss: 32.60539436340332\n","Epoch: 759/5000, Train Loss: 30.833181381225586, Valid Loss: 32.625928243001304\n","Epoch: 760/5000, Train Loss: 29.966429103504527, Valid Loss: 32.61707369486491\n","Epoch: 761/5000, Train Loss: 29.960715900767934, Valid Loss: 32.6187318166097\n","Epoch: 762/5000, Train Loss: 30.318672700361773, Valid Loss: 32.62576993306478\n","Epoch: 763/5000, Train Loss: 30.06882771578702, Valid Loss: 32.66170565287272\n","Epoch: 764/5000, Train Loss: 30.547164743596856, Valid Loss: 32.62000719706217\n","Epoch: 765/5000, Train Loss: 30.440416509454902, Valid Loss: 32.63169606526693\n","Epoch: 766/5000, Train Loss: 30.239141290838067, Valid Loss: 32.630696614583336\n","Epoch: 767/5000, Train Loss: 30.36728962984952, Valid Loss: 32.633735020955406\n","Epoch: 768/5000, Train Loss: 30.528994300148703, Valid Loss: 32.62791760762533\n","Epoch: 769/5000, Train Loss: 30.280804547396574, Valid Loss: 32.6082394917806\n","Epoch: 770/5000, Train Loss: 30.639115073464133, Valid Loss: 32.59366671244303\n","Epoch: 771/5000, Train Loss: 30.65376767245206, Valid Loss: 32.591065088907875\n","Epoch: 772/5000, Train Loss: 30.491036501797762, Valid Loss: 32.58474349975586\n","Epoch: 773/5000, Train Loss: 30.692047639326617, Valid Loss: 32.59521611531576\n","Epoch: 774/5000, Train Loss: 30.08538644964045, Valid Loss: 32.616233825683594\n","Epoch: 775/5000, Train Loss: 30.171981117942117, Valid Loss: 32.62833913167318\n","Epoch: 776/5000, Train Loss: 30.204146992076527, Valid Loss: 32.61518541971842\n","Epoch: 777/5000, Train Loss: 30.362835277210582, Valid Loss: 32.62960879007975\n","Epoch: 778/5000, Train Loss: 30.53681165521795, Valid Loss: 32.62639300028483\n","Epoch: 779/5000, Train Loss: 30.3290488503196, Valid Loss: 32.602329889933266\n","Epoch: 780/5000, Train Loss: 30.48546687039462, Valid Loss: 32.615416844685875\n","Epoch: 781/5000, Train Loss: 30.46117574518377, Valid Loss: 32.62748781840006\n","Epoch: 782/5000, Train Loss: 30.14655303955078, Valid Loss: 32.62465286254883\n","Epoch: 783/5000, Train Loss: 30.55893915349787, Valid Loss: 32.63484764099121\n","Epoch: 784/5000, Train Loss: 30.549854798750445, Valid Loss: 32.65027745564779\n","Epoch: 785/5000, Train Loss: 30.593605735085227, Valid Loss: 32.66241137186686\n","Epoch: 786/5000, Train Loss: 30.67160589044744, Valid Loss: 32.620330810546875\n","Epoch: 787/5000, Train Loss: 30.44626860185103, Valid Loss: 32.65500259399414\n","Epoch: 788/5000, Train Loss: 30.176266930320047, Valid Loss: 32.64630317687988\n","Epoch: 789/5000, Train Loss: 30.023297916759144, Valid Loss: 32.64111200968424\n","Epoch: 790/5000, Train Loss: 30.68352681940252, Valid Loss: 32.664015452067055\n","Epoch: 791/5000, Train Loss: 30.35151585665616, Valid Loss: 32.64728228251139\n","Epoch: 792/5000, Train Loss: 30.195287704467773, Valid Loss: 32.62904294331869\n","Epoch: 793/5000, Train Loss: 30.5237830768932, Valid Loss: 32.633140563964844\n","Epoch: 794/5000, Train Loss: 30.225243655118074, Valid Loss: 32.619602839152016\n","Epoch: 795/5000, Train Loss: 30.24726139415394, Valid Loss: 32.62722969055176\n","Epoch: 796/5000, Train Loss: 30.34678875316273, Valid Loss: 32.61103312174479\n","Epoch: 797/5000, Train Loss: 30.69316933371804, Valid Loss: 32.61948585510254\n","Epoch: 798/5000, Train Loss: 30.375052885575727, Valid Loss: 32.61102612813314\n","Epoch: 799/5000, Train Loss: 30.433629296042703, Valid Loss: 32.63285573323568\n","Epoch: 800/5000, Train Loss: 30.068272157148883, Valid Loss: 32.642388025919594\n","Epoch: 801/5000, Train Loss: 30.62155671553178, Valid Loss: 32.66271209716797\n","Epoch: 802/5000, Train Loss: 30.40129349448464, Valid Loss: 32.63620567321777\n","Epoch: 803/5000, Train Loss: 30.705705469304863, Valid Loss: 32.63108507792155\n","Epoch: 804/5000, Train Loss: 30.278933785178445, Valid Loss: 32.6402645111084\n","Epoch: 805/5000, Train Loss: 30.300107088955965, Valid Loss: 32.635023752848305\n","Epoch: 806/5000, Train Loss: 30.734292117032137, Valid Loss: 32.629400889078774\n","Epoch: 807/5000, Train Loss: 30.57220424305309, Valid Loss: 32.62914911905924\n","Epoch: 808/5000, Train Loss: 30.289734406904742, Valid Loss: 32.64667892456055\n","Epoch: 809/5000, Train Loss: 30.538909218528055, Valid Loss: 32.598951975504555\n","Epoch: 810/5000, Train Loss: 30.29811009493741, Valid Loss: 32.57975832621256\n","Epoch: 811/5000, Train Loss: 30.534046173095703, Valid Loss: 32.600056966145836\n","Epoch: 812/5000, Train Loss: 30.31707226146351, Valid Loss: 32.58652687072754\n","Epoch: 813/5000, Train Loss: 30.39450922879306, Valid Loss: 32.5919075012207\n","Epoch: 814/5000, Train Loss: 30.444651690396395, Valid Loss: 32.634433110555015\n","Epoch: 815/5000, Train Loss: 30.218828894875266, Valid Loss: 32.62650235493978\n","Epoch: 816/5000, Train Loss: 30.335351943969727, Valid Loss: 32.62276268005371\n","Epoch: 817/5000, Train Loss: 30.23729931224476, Valid Loss: 32.608385721842446\n","Epoch: 818/5000, Train Loss: 30.549661116166547, Valid Loss: 32.62873458862305\n","Epoch: 819/5000, Train Loss: 30.088139100508258, Valid Loss: 32.611349741617836\n","Epoch: 820/5000, Train Loss: 30.175759402188387, Valid Loss: 32.61235745747884\n","Epoch: 821/5000, Train Loss: 30.391782587224785, Valid Loss: 32.61628087361654\n","Epoch: 822/5000, Train Loss: 30.504727623679422, Valid Loss: 32.606419245402016\n","Epoch: 823/5000, Train Loss: 30.317678451538086, Valid Loss: 32.65220959981283\n","Epoch: 824/5000, Train Loss: 30.42383974248713, Valid Loss: 32.6539503733317\n","Epoch: 825/5000, Train Loss: 30.200476559725676, Valid Loss: 32.63252321879069\n","Epoch: 826/5000, Train Loss: 30.48466647755016, Valid Loss: 32.62829907735189\n","Epoch: 827/5000, Train Loss: 30.377978238192473, Valid Loss: 32.63448524475098\n","Epoch: 828/5000, Train Loss: 30.27987324107777, Valid Loss: 32.62538146972656\n","Epoch: 829/5000, Train Loss: 30.301822835748847, Valid Loss: 32.6213181813558\n","Epoch: 830/5000, Train Loss: 30.0906248959628, Valid Loss: 32.607531229654946\n","Epoch: 831/5000, Train Loss: 30.224830107255414, Valid Loss: 32.620787938435875\n","Epoch: 832/5000, Train Loss: 30.518118598244406, Valid Loss: 32.61809158325195\n","Epoch: 833/5000, Train Loss: 30.211471384221856, Valid Loss: 32.61613527933756\n","Epoch: 834/5000, Train Loss: 30.44103171608665, Valid Loss: 32.617621739705406\n","Epoch: 835/5000, Train Loss: 30.40306940945712, Valid Loss: 32.63389205932617\n","Epoch: 836/5000, Train Loss: 30.291079607876863, Valid Loss: 32.608093897501625\n","Epoch: 837/5000, Train Loss: 30.509688290682707, Valid Loss: 32.62012481689453\n","Epoch: 838/5000, Train Loss: 30.456311659379438, Valid Loss: 32.621066411336265\n","Epoch: 839/5000, Train Loss: 30.132668581875887, Valid Loss: 32.64138603210449\n","Epoch: 840/5000, Train Loss: 29.890356757424094, Valid Loss: 32.63760566711426\n","Epoch: 841/5000, Train Loss: 30.34365359219638, Valid Loss: 32.60854975382487\n","Epoch: 842/5000, Train Loss: 30.344404393976387, Valid Loss: 32.61167526245117\n","Epoch: 843/5000, Train Loss: 30.46718389337713, Valid Loss: 32.615532557169594\n","Epoch: 844/5000, Train Loss: 30.246931943026457, Valid Loss: 32.59794616699219\n","Epoch: 845/5000, Train Loss: 30.330487164584074, Valid Loss: 32.62277921040853\n","Epoch: 846/5000, Train Loss: 29.8847937150435, Valid Loss: 32.59580930074056\n","Epoch: 847/5000, Train Loss: 30.03413131020286, Valid Loss: 32.61656824747721\n","Epoch: 848/5000, Train Loss: 30.24269051985307, Valid Loss: 32.61769485473633\n","Epoch: 849/5000, Train Loss: 30.554113908247515, Valid Loss: 32.61858050028483\n","Epoch: 850/5000, Train Loss: 30.50524243441495, Valid Loss: 32.61913172403971\n","Epoch: 851/5000, Train Loss: 30.35603072426536, Valid Loss: 32.606310526529946\n","Epoch: 852/5000, Train Loss: 30.34656992825595, Valid Loss: 32.588637034098305\n","Epoch: 853/5000, Train Loss: 30.237104589288887, Valid Loss: 32.60780588785807\n","Epoch: 854/5000, Train Loss: 30.3953840082342, Valid Loss: 32.601003646850586\n","Epoch: 855/5000, Train Loss: 30.197689749977805, Valid Loss: 32.593573252360024\n","Epoch: 856/5000, Train Loss: 30.339643825184215, Valid Loss: 32.584526697794594\n","Epoch: 857/5000, Train Loss: 30.176427841186523, Valid Loss: 32.59697659810384\n","Epoch: 858/5000, Train Loss: 30.018824317238547, Valid Loss: 32.61285146077474\n","Epoch: 859/5000, Train Loss: 30.480514873157848, Valid Loss: 32.62564595540365\n","Epoch: 860/5000, Train Loss: 30.58900278264826, Valid Loss: 32.63252385457357\n","Epoch: 861/5000, Train Loss: 30.170454372059215, Valid Loss: 32.58898798624674\n","Epoch: 862/5000, Train Loss: 30.545826305042613, Valid Loss: 32.582794189453125\n","Epoch: 863/5000, Train Loss: 30.04106920415705, Valid Loss: 32.59706497192383\n","Epoch: 864/5000, Train Loss: 30.30181850086559, Valid Loss: 32.604496002197266\n","Epoch: 865/5000, Train Loss: 30.328631834550336, Valid Loss: 32.59013875325521\n","Epoch: 866/5000, Train Loss: 30.205829273570668, Valid Loss: 32.56251335144043\n","Epoch: 867/5000, Train Loss: 30.281524658203125, Valid Loss: 32.60073979695638\n","Epoch: 868/5000, Train Loss: 29.756030689586293, Valid Loss: 32.5885124206543\n","Epoch: 869/5000, Train Loss: 30.285070766102184, Valid Loss: 32.57747141520182\n","Epoch: 870/5000, Train Loss: 30.186215834184125, Valid Loss: 32.568970362345375\n","Epoch: 871/5000, Train Loss: 30.489965612238105, Valid Loss: 32.574572245279946\n","Epoch: 872/5000, Train Loss: 30.51379359852184, Valid Loss: 32.57203229268392\n","Epoch: 873/5000, Train Loss: 30.07984040000222, Valid Loss: 32.58860460917155\n","Epoch: 874/5000, Train Loss: 30.113707282326438, Valid Loss: 32.59112675984701\n","Epoch: 875/5000, Train Loss: 30.161158648404207, Valid Loss: 32.60009002685547\n","Epoch: 876/5000, Train Loss: 30.67997169494629, Valid Loss: 32.59963289896647\n","Epoch: 877/5000, Train Loss: 30.324005300348457, Valid Loss: 32.60671615600586\n","Epoch: 878/5000, Train Loss: 30.24093315818093, Valid Loss: 32.6100279490153\n","Epoch: 879/5000, Train Loss: 30.092458031394266, Valid Loss: 32.60124079386393\n","Epoch: 880/5000, Train Loss: 30.265323292125355, Valid Loss: 32.602935791015625\n","Epoch: 881/5000, Train Loss: 30.614057540893555, Valid Loss: 32.58758544921875\n","Epoch: 882/5000, Train Loss: 30.425969557328656, Valid Loss: 32.59701283772787\n","Epoch: 883/5000, Train Loss: 30.039485237815164, Valid Loss: 32.59324709574381\n","Epoch: 884/5000, Train Loss: 30.026802929964933, Valid Loss: 32.57919375101725\n","Epoch: 885/5000, Train Loss: 30.037100358442828, Valid Loss: 32.55356534322103\n","Epoch: 886/5000, Train Loss: 29.687925338745117, Valid Loss: 32.60303560892741\n","Epoch: 887/5000, Train Loss: 30.59104364568537, Valid Loss: 32.590331395467125\n","Epoch: 888/5000, Train Loss: 30.100776845758613, Valid Loss: 32.59259923299154\n","Epoch: 889/5000, Train Loss: 30.356615239923652, Valid Loss: 32.581786473592125\n","Epoch: 890/5000, Train Loss: 30.054980538108133, Valid Loss: 32.57745869954427\n","Epoch: 891/5000, Train Loss: 29.76001583446156, Valid Loss: 32.593348821004234\n","Epoch: 892/5000, Train Loss: 30.23025148565119, Valid Loss: 32.56571833292643\n","Epoch: 893/5000, Train Loss: 30.16697848926891, Valid Loss: 32.55744234720866\n","Epoch: 894/5000, Train Loss: 30.236021388660777, Valid Loss: 32.60028966267904\n","Epoch: 895/5000, Train Loss: 30.10301433910023, Valid Loss: 32.56756337483724\n","Epoch: 896/5000, Train Loss: 30.28208368474787, Valid Loss: 32.558362325032554\n","Epoch: 897/5000, Train Loss: 30.437347238714043, Valid Loss: 32.5854606628418\n","Epoch: 898/5000, Train Loss: 30.394046436656605, Valid Loss: 32.5676638285319\n","Epoch: 899/5000, Train Loss: 30.51182053305886, Valid Loss: 32.57856305440267\n","Epoch: 900/5000, Train Loss: 30.21094478260387, Valid Loss: 32.567291259765625\n","Epoch: 901/5000, Train Loss: 30.230163400823418, Valid Loss: 32.58406448364258\n","Epoch: 902/5000, Train Loss: 30.115792187777433, Valid Loss: 32.58434295654297\n","Epoch: 903/5000, Train Loss: 29.855078263716265, Valid Loss: 32.584115982055664\n","Epoch: 904/5000, Train Loss: 30.301694003018465, Valid Loss: 32.57748540242513\n","Epoch: 905/5000, Train Loss: 30.255600669167258, Valid Loss: 32.565795262654625\n","Epoch: 906/5000, Train Loss: 29.84363989396529, Valid Loss: 32.59777005513509\n","Epoch: 907/5000, Train Loss: 30.435389952226117, Valid Loss: 32.60887018839518\n","Epoch: 908/5000, Train Loss: 30.410961497913707, Valid Loss: 32.600701014200844\n","Epoch: 909/5000, Train Loss: 30.12286949157715, Valid Loss: 32.6145133972168\n","Epoch: 910/5000, Train Loss: 30.0377930727872, Valid Loss: 32.574310302734375\n","Epoch: 911/5000, Train Loss: 30.222754044966265, Valid Loss: 32.56639862060547\n","Epoch: 912/5000, Train Loss: 30.399153796109285, Valid Loss: 32.5824826558431\n","Epoch: 913/5000, Train Loss: 30.57871523770419, Valid Loss: 32.559216817220054\n","Epoch: 914/5000, Train Loss: 30.09487464211204, Valid Loss: 32.55686950683594\n","Epoch: 915/5000, Train Loss: 29.9976113059304, Valid Loss: 32.5687624613444\n","Epoch: 916/5000, Train Loss: 30.146812785755504, Valid Loss: 32.56940714518229\n","Epoch: 917/5000, Train Loss: 29.909786917946555, Valid Loss: 32.56605911254883\n","Epoch: 918/5000, Train Loss: 29.63834675875577, Valid Loss: 32.54977226257324\n","Epoch: 919/5000, Train Loss: 30.251513047651812, Valid Loss: 32.57088406880697\n","Epoch: 920/5000, Train Loss: 29.917741602117363, Valid Loss: 32.608694076538086\n","Epoch: 921/5000, Train Loss: 30.757056496360086, Valid Loss: 32.5952033996582\n","Epoch: 922/5000, Train Loss: 30.390978032892402, Valid Loss: 32.59314092000326\n","Epoch: 923/5000, Train Loss: 30.319901553067293, Valid Loss: 32.57481129964193\n","Epoch: 924/5000, Train Loss: 29.9979173486883, Valid Loss: 32.6004835764567\n","Epoch: 925/5000, Train Loss: 30.08913265575062, Valid Loss: 32.583115895589195\n","Epoch: 926/5000, Train Loss: 29.814800956032492, Valid Loss: 32.580665588378906\n","Epoch: 927/5000, Train Loss: 30.502181833440606, Valid Loss: 32.559617360432945\n","Epoch: 928/5000, Train Loss: 30.543480786410246, Valid Loss: 32.53793907165527\n","Epoch: 929/5000, Train Loss: 30.731277812610973, Valid Loss: 32.540749867757164\n","Epoch: 930/5000, Train Loss: 29.962602268565785, Valid Loss: 32.53671518961588\n","Epoch: 931/5000, Train Loss: 30.055983803488992, Valid Loss: 32.56409708658854\n","Epoch: 932/5000, Train Loss: 30.21152825789018, Valid Loss: 32.53153165181478\n","Epoch: 933/5000, Train Loss: 30.30300591208718, Valid Loss: 32.547306060791016\n","Epoch: 934/5000, Train Loss: 30.14397499778054, Valid Loss: 32.550713221232094\n","Epoch: 935/5000, Train Loss: 30.332494735717773, Valid Loss: 32.54017194112142\n","Epoch: 936/5000, Train Loss: 30.124114296653055, Valid Loss: 32.56603558858236\n","Epoch: 937/5000, Train Loss: 30.068777777931906, Valid Loss: 32.56986045837402\n","Epoch: 938/5000, Train Loss: 30.11488186229359, Valid Loss: 32.60369618733724\n","Epoch: 939/5000, Train Loss: 29.960969404740766, Valid Loss: 32.59941291809082\n","Epoch: 940/5000, Train Loss: 30.123767852783203, Valid Loss: 32.58859507242838\n","Epoch: 941/5000, Train Loss: 29.855778607455168, Valid Loss: 32.585872650146484\n","Epoch: 942/5000, Train Loss: 30.07556880604137, Valid Loss: 32.563205083211265\n","Epoch: 943/5000, Train Loss: 30.110353990034625, Valid Loss: 32.56313959757487\n","Epoch: 944/5000, Train Loss: 30.164322072809394, Valid Loss: 32.54867172241211\n","Epoch: 945/5000, Train Loss: 30.07096914811568, Valid Loss: 32.56831487019857\n","Epoch: 946/5000, Train Loss: 30.068843147971414, Valid Loss: 32.5687255859375\n","Epoch: 947/5000, Train Loss: 30.136789321899414, Valid Loss: 32.603071212768555\n","Epoch: 948/5000, Train Loss: 30.34920449690385, Valid Loss: 32.577819188435875\n","Epoch: 949/5000, Train Loss: 30.142213821411133, Valid Loss: 32.59135309855143\n","Epoch: 950/5000, Train Loss: 30.330985502763227, Valid Loss: 32.59970156351725\n","Epoch: 951/5000, Train Loss: 30.094641251997515, Valid Loss: 32.60148239135742\n","Epoch: 952/5000, Train Loss: 29.968211607499555, Valid Loss: 32.59255854288737\n","Epoch: 953/5000, Train Loss: 30.37465424971147, Valid Loss: 32.586387634277344\n","Epoch: 954/5000, Train Loss: 29.699138988148082, Valid Loss: 32.5874277750651\n","Epoch: 955/5000, Train Loss: 30.31751112504439, Valid Loss: 32.5906867980957\n","Epoch: 956/5000, Train Loss: 29.951704892245207, Valid Loss: 32.58501370747884\n","Epoch: 957/5000, Train Loss: 30.078929380937055, Valid Loss: 32.578969955444336\n","Epoch: 958/5000, Train Loss: 30.19072688709606, Valid Loss: 32.591553370157875\n","Epoch: 959/5000, Train Loss: 30.291908090764824, Valid Loss: 32.600128809611\n","Epoch: 960/5000, Train Loss: 30.040795412930574, Valid Loss: 32.59046490987142\n","Epoch: 961/5000, Train Loss: 30.41078862276944, Valid Loss: 32.55905405680338\n","Epoch: 962/5000, Train Loss: 29.999856775457207, Valid Loss: 32.59071922302246\n","Epoch: 963/5000, Train Loss: 30.19194915077903, Valid Loss: 32.556802113850914\n","Epoch: 964/5000, Train Loss: 30.566783384843305, Valid Loss: 32.55439821879069\n","Epoch: 965/5000, Train Loss: 30.10685903375799, Valid Loss: 32.59291013081869\n","Epoch: 966/5000, Train Loss: 30.485445369373668, Valid Loss: 32.613203048706055\n","Epoch: 967/5000, Train Loss: 30.027941617098723, Valid Loss: 32.62759971618652\n","Epoch: 968/5000, Train Loss: 30.019147179343484, Valid Loss: 32.590538024902344\n","Epoch: 969/5000, Train Loss: 29.921431454745207, Valid Loss: 32.641051610310875\n","Epoch: 970/5000, Train Loss: 29.914629502729937, Valid Loss: 32.605925877889\n","Epoch: 971/5000, Train Loss: 30.045191157947887, Valid Loss: 32.64566230773926\n","Epoch: 972/5000, Train Loss: 29.900129838423297, Valid Loss: 32.60304959615072\n","Epoch: 973/5000, Train Loss: 30.328128294511274, Valid Loss: 32.58885701497396\n","Epoch: 974/5000, Train Loss: 30.388533852317117, Valid Loss: 32.63271458943685\n","Epoch: 975/5000, Train Loss: 30.223004774613813, Valid Loss: 32.61956787109375\n","Epoch: 976/5000, Train Loss: 29.858030145818535, Valid Loss: 32.57749621073405\n","Epoch: 977/5000, Train Loss: 30.191756335171785, Valid Loss: 32.576124827067055\n","Epoch: 978/5000, Train Loss: 30.207990646362305, Valid Loss: 32.57256762186686\n","Epoch: 979/5000, Train Loss: 30.087299173528496, Valid Loss: 32.58394686381022\n","Epoch: 980/5000, Train Loss: 29.955103960904207, Valid Loss: 32.589939753214516\n","Epoch: 981/5000, Train Loss: 29.980741847645152, Valid Loss: 32.60698954264323\n","Epoch: 982/5000, Train Loss: 29.837913166392934, Valid Loss: 32.5893980662028\n","Epoch: 983/5000, Train Loss: 30.134268327192828, Valid Loss: 32.578304290771484\n","Epoch: 984/5000, Train Loss: 30.05808570168235, Valid Loss: 32.574476877848305\n","Epoch: 985/5000, Train Loss: 30.251962141557172, Valid Loss: 32.60447756449381\n","Epoch: 986/5000, Train Loss: 29.963640559803356, Valid Loss: 32.601235707600914\n","Epoch: 987/5000, Train Loss: 30.15231080488725, Valid Loss: 32.565608978271484\n","Epoch: 988/5000, Train Loss: 30.19734712080522, Valid Loss: 32.58875528971354\n","Epoch: 989/5000, Train Loss: 30.02286148071289, Valid Loss: 32.59889793395996\n","Epoch: 990/5000, Train Loss: 30.249727942726828, Valid Loss: 32.5955015818278\n","Epoch: 991/5000, Train Loss: 29.88474533774636, Valid Loss: 32.58179728190104\n","Epoch: 992/5000, Train Loss: 29.97453481500799, Valid Loss: 32.614766438802086\n","Epoch: 993/5000, Train Loss: 29.95219750837846, Valid Loss: 32.6241086324056\n","Epoch: 994/5000, Train Loss: 29.944454019719903, Valid Loss: 32.580461502075195\n","Epoch: 995/5000, Train Loss: 30.116015347567473, Valid Loss: 32.57890510559082\n","Epoch: 996/5000, Train Loss: 30.19429432262074, Valid Loss: 32.57470830281576\n","Epoch: 997/5000, Train Loss: 29.918187748302113, Valid Loss: 32.57542483011881\n","Epoch: 998/5000, Train Loss: 30.271011352539062, Valid Loss: 32.57663599650065\n","Epoch: 999/5000, Train Loss: 30.128773775967684, Valid Loss: 32.60588582356771\n","Epoch: 1000/5000, Train Loss: 30.236719131469727, Valid Loss: 32.57136980692545\n","Epoch: 1001/5000, Train Loss: 30.21843858198686, Valid Loss: 32.5848642985026\n","Epoch: 1002/5000, Train Loss: 30.10199581493031, Valid Loss: 32.59759330749512\n","Epoch: 1003/5000, Train Loss: 30.109056646173652, Valid Loss: 32.576148986816406\n","Epoch: 1004/5000, Train Loss: 30.107349569147285, Valid Loss: 32.580116271972656\n","Epoch: 1005/5000, Train Loss: 30.04960216175426, Valid Loss: 32.569068908691406\n","Epoch: 1006/5000, Train Loss: 30.014009128917348, Valid Loss: 32.56965192159017\n","Epoch: 1007/5000, Train Loss: 29.952951604669746, Valid Loss: 32.604174296061196\n","Epoch: 1008/5000, Train Loss: 30.19555126536976, Valid Loss: 32.61774126688639\n","Epoch: 1009/5000, Train Loss: 29.676200346513227, Valid Loss: 32.598267237345375\n","Epoch: 1010/5000, Train Loss: 29.91830062866211, Valid Loss: 32.57835133870443\n","Epoch: 1011/5000, Train Loss: 30.17732082713734, Valid Loss: 32.57374954223633\n","Epoch: 1012/5000, Train Loss: 30.072145982222125, Valid Loss: 32.56897735595703\n","Epoch: 1013/5000, Train Loss: 30.03586127541282, Valid Loss: 32.55194409688314\n","Epoch: 1014/5000, Train Loss: 30.295272653753106, Valid Loss: 32.58838971455892\n","Epoch: 1015/5000, Train Loss: 30.284249739213422, Valid Loss: 32.56262715657552\n","Epoch: 1016/5000, Train Loss: 30.385331240567293, Valid Loss: 32.569653828938804\n","Epoch: 1017/5000, Train Loss: 30.297645222056996, Valid Loss: 32.57944869995117\n","Epoch: 1018/5000, Train Loss: 30.302471854469992, Valid Loss: 32.56643931070963\n","Epoch: 1019/5000, Train Loss: 29.968642841685902, Valid Loss: 32.58319981892904\n","Epoch: 1020/5000, Train Loss: 29.99618114124645, Valid Loss: 32.587436040242515\n","Epoch: 1021/5000, Train Loss: 30.121883045543324, Valid Loss: 32.59933662414551\n","Epoch: 1022/5000, Train Loss: 29.966326106678356, Valid Loss: 32.572930653889976\n","Epoch: 1023/5000, Train Loss: 29.87185218117454, Valid Loss: 32.559157053629555\n","Epoch: 1024/5000, Train Loss: 29.896150935779918, Valid Loss: 32.58416938781738\n","Epoch: 1025/5000, Train Loss: 30.04062808643688, Valid Loss: 32.58525149027506\n","Epoch: 1026/5000, Train Loss: 30.231793663718484, Valid Loss: 32.58380699157715\n","Epoch: 1027/5000, Train Loss: 30.132982774214312, Valid Loss: 32.56806182861328\n","Epoch: 1028/5000, Train Loss: 29.866973183371805, Valid Loss: 32.56647872924805\n","Epoch: 1029/5000, Train Loss: 29.90738469904119, Valid Loss: 32.60455322265625\n","Epoch: 1030/5000, Train Loss: 29.9740987257524, Valid Loss: 32.59456316630045\n","Epoch: 1031/5000, Train Loss: 29.84976681795987, Valid Loss: 32.595513025919594\n","Epoch: 1032/5000, Train Loss: 30.11693607677113, Valid Loss: 32.59210395812988\n","얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 1032에서 훈련 중단.\n","Training Start: HLM\n","Epoch: 0/5000, Train Loss: 64.12570571899414, Valid Loss: 63.02223205566406\n","Epoch: 1/5000, Train Loss: 64.07910017533736, Valid Loss: 62.97660573323568\n","Epoch: 2/5000, Train Loss: 64.0599347894842, Valid Loss: 62.914981842041016\n","Epoch: 3/5000, Train Loss: 63.99186983975497, Valid Loss: 62.86326599121094\n","Epoch: 4/5000, Train Loss: 63.94896802035245, Valid Loss: 62.82399241129557\n","Epoch: 5/5000, Train Loss: 63.898216247558594, Valid Loss: 62.78518931070963\n","Epoch: 6/5000, Train Loss: 63.881874084472656, Valid Loss: 62.75065358479818\n","Epoch: 7/5000, Train Loss: 63.82588091763583, Valid Loss: 62.70496114095052\n","Epoch: 8/5000, Train Loss: 63.749901511452414, Valid Loss: 62.66338602701823\n","Epoch: 9/5000, Train Loss: 63.75188168612394, Valid Loss: 62.624987284342446\n","Epoch: 10/5000, Train Loss: 63.708687175403945, Valid Loss: 62.58824920654297\n","Epoch: 11/5000, Train Loss: 63.653033169833094, Valid Loss: 62.55043029785156\n","Epoch: 12/5000, Train Loss: 63.596981048583984, Valid Loss: 62.507728576660156\n","Epoch: 13/5000, Train Loss: 63.562115755948156, Valid Loss: 62.45353571573893\n","Epoch: 14/5000, Train Loss: 63.478421644731, Valid Loss: 62.414694468180336\n","Epoch: 15/5000, Train Loss: 63.48154137351296, Valid Loss: 62.35758845011393\n","Epoch: 16/5000, Train Loss: 63.41541151566939, Valid Loss: 62.32262166341146\n","Epoch: 17/5000, Train Loss: 63.37826260653409, Valid Loss: 62.281148274739586\n","Epoch: 18/5000, Train Loss: 63.327213980934836, Valid Loss: 62.21629079182943\n","Epoch: 19/5000, Train Loss: 63.32173295454545, Valid Loss: 62.19119517008463\n","Epoch: 20/5000, Train Loss: 63.25259850241921, Valid Loss: 62.143377939860024\n","Epoch: 21/5000, Train Loss: 63.21838448264382, Valid Loss: 62.064666748046875\n","Epoch: 22/5000, Train Loss: 63.14924482865767, Valid Loss: 62.02758534749349\n","Epoch: 23/5000, Train Loss: 63.1223012750799, Valid Loss: 61.96945571899414\n","Epoch: 24/5000, Train Loss: 63.07897359674627, Valid Loss: 61.92477289835612\n","Epoch: 25/5000, Train Loss: 63.027439464222304, Valid Loss: 61.87159729003906\n","Epoch: 26/5000, Train Loss: 63.007563851096414, Valid Loss: 61.85029602050781\n","Epoch: 27/5000, Train Loss: 62.97399174083363, Valid Loss: 61.805712381998696\n","Epoch: 28/5000, Train Loss: 62.92849558049982, Valid Loss: 61.769195556640625\n","Epoch: 29/5000, Train Loss: 62.90003343061967, Valid Loss: 61.71420033772787\n","Epoch: 30/5000, Train Loss: 62.82364064996893, Valid Loss: 61.672447204589844\n","Epoch: 31/5000, Train Loss: 62.77537710016424, Valid Loss: 61.61722310384115\n","Epoch: 32/5000, Train Loss: 62.75714319402521, Valid Loss: 61.550889333089195\n","Epoch: 33/5000, Train Loss: 62.71318331631747, Valid Loss: 61.4994748433431\n","Epoch: 34/5000, Train Loss: 62.680025274103336, Valid Loss: 61.448219299316406\n","Epoch: 35/5000, Train Loss: 62.619474931196734, Valid Loss: 61.4327278137207\n","Epoch: 36/5000, Train Loss: 62.59154302423651, Valid Loss: 61.38797124226888\n","Epoch: 37/5000, Train Loss: 62.540551619096235, Valid Loss: 61.35431162516276\n","Epoch: 38/5000, Train Loss: 62.47096668590199, Valid Loss: 61.28633244832357\n","Epoch: 39/5000, Train Loss: 62.46724527532404, Valid Loss: 61.29317728678385\n","Epoch: 40/5000, Train Loss: 62.40009619972923, Valid Loss: 61.23542785644531\n","Epoch: 41/5000, Train Loss: 62.30887187610973, Valid Loss: 61.17848459879557\n","Epoch: 42/5000, Train Loss: 62.31877483021129, Valid Loss: 61.12755839029948\n","Epoch: 43/5000, Train Loss: 62.29479564319957, Valid Loss: 61.08311462402344\n","Epoch: 44/5000, Train Loss: 62.224939519708805, Valid Loss: 61.03079096476237\n","Epoch: 45/5000, Train Loss: 62.17965386130593, Valid Loss: 61.01098887125651\n","Epoch: 46/5000, Train Loss: 62.073485981334336, Valid Loss: 60.961018880208336\n","Epoch: 47/5000, Train Loss: 62.0863598910245, Valid Loss: 60.89332580566406\n","Epoch: 48/5000, Train Loss: 62.04286020452326, Valid Loss: 60.854662577311196\n","Epoch: 49/5000, Train Loss: 61.96682219071822, Valid Loss: 60.790032704671226\n","Epoch: 50/5000, Train Loss: 61.96792775934393, Valid Loss: 60.73374557495117\n","Epoch: 51/5000, Train Loss: 61.889186512340196, Valid Loss: 60.72490437825521\n","Epoch: 52/5000, Train Loss: 61.83110601251776, Valid Loss: 60.68818791707357\n","Epoch: 53/5000, Train Loss: 61.80756586248224, Valid Loss: 60.62576675415039\n","Epoch: 54/5000, Train Loss: 61.6990571455522, Valid Loss: 60.57099914550781\n","Epoch: 55/5000, Train Loss: 61.71528174660423, Valid Loss: 60.53312301635742\n","Epoch: 56/5000, Train Loss: 61.65598990700462, Valid Loss: 60.469842274983726\n","Epoch: 57/5000, Train Loss: 61.58271477439187, Valid Loss: 60.435791015625\n","Epoch: 58/5000, Train Loss: 61.53269646384499, Valid Loss: 60.37421544392904\n","Epoch: 59/5000, Train Loss: 61.53858323530717, Valid Loss: 60.3221689860026\n","Epoch: 60/5000, Train Loss: 61.46039373224432, Valid Loss: 60.27795155843099\n","Epoch: 61/5000, Train Loss: 61.38415735418146, Valid Loss: 60.20658874511719\n","Epoch: 62/5000, Train Loss: 61.35015106201172, Valid Loss: 60.16376876831055\n","Epoch: 63/5000, Train Loss: 61.27918174050071, Valid Loss: 60.08731333414713\n","Epoch: 64/5000, Train Loss: 61.25742409446023, Valid Loss: 60.0908203125\n","Epoch: 65/5000, Train Loss: 61.194048794833094, Valid Loss: 60.03057098388672\n","Epoch: 66/5000, Train Loss: 61.13516200672496, Valid Loss: 59.97802607218424\n","Epoch: 67/5000, Train Loss: 61.087181091308594, Valid Loss: 59.92208607991537\n","Epoch: 68/5000, Train Loss: 61.0566291809082, Valid Loss: 59.84463628133138\n","Epoch: 69/5000, Train Loss: 60.998954426158555, Valid Loss: 59.81419245402018\n","Epoch: 70/5000, Train Loss: 60.92352919145064, Valid Loss: 59.785142262776695\n","Epoch: 71/5000, Train Loss: 60.86219718239524, Valid Loss: 59.72594451904297\n","Epoch: 72/5000, Train Loss: 60.81603587757457, Valid Loss: 59.65407689412435\n","Epoch: 73/5000, Train Loss: 60.721976887096055, Valid Loss: 59.59836451212565\n","Epoch: 74/5000, Train Loss: 60.7030424638228, Valid Loss: 59.54773712158203\n","Epoch: 75/5000, Train Loss: 60.653077905828304, Valid Loss: 59.464317321777344\n","Epoch: 76/5000, Train Loss: 60.60268783569336, Valid Loss: 59.42307917277018\n","Epoch: 77/5000, Train Loss: 60.5416505986994, Valid Loss: 59.34323501586914\n","Epoch: 78/5000, Train Loss: 60.44843500310724, Valid Loss: 59.30039978027344\n","Epoch: 79/5000, Train Loss: 60.44677595658736, Valid Loss: 59.274819691975914\n","Epoch: 80/5000, Train Loss: 60.39475527676669, Valid Loss: 59.19767379760742\n","Epoch: 81/5000, Train Loss: 60.33968145197088, Valid Loss: 59.20013173421224\n","Epoch: 82/5000, Train Loss: 60.29945789683949, Valid Loss: 59.119885762532554\n","Epoch: 83/5000, Train Loss: 60.22544305974787, Valid Loss: 59.086358388264976\n","Epoch: 84/5000, Train Loss: 60.167078191583805, Valid Loss: 59.0298105875651\n","Epoch: 85/5000, Train Loss: 60.08392611416903, Valid Loss: 58.960619608561196\n","Epoch: 86/5000, Train Loss: 60.02199381048029, Valid Loss: 58.91066360473633\n","Epoch: 87/5000, Train Loss: 60.05017020485618, Valid Loss: 58.849255879720054\n","Epoch: 88/5000, Train Loss: 59.95333030007102, Valid Loss: 58.780340830485024\n","Epoch: 89/5000, Train Loss: 59.90492560646751, Valid Loss: 58.72172927856445\n","Epoch: 90/5000, Train Loss: 59.796083276922054, Valid Loss: 58.68416213989258\n","Epoch: 91/5000, Train Loss: 59.73979325727983, Valid Loss: 58.60633341471354\n","Epoch: 92/5000, Train Loss: 59.65315107865767, Valid Loss: 58.52356211344401\n","Epoch: 93/5000, Train Loss: 59.60629376498136, Valid Loss: 58.55054728190104\n","Epoch: 94/5000, Train Loss: 59.591583251953125, Valid Loss: 58.49410629272461\n","Epoch: 95/5000, Train Loss: 59.544897599653765, Valid Loss: 58.435646057128906\n","Epoch: 96/5000, Train Loss: 59.52536808360707, Valid Loss: 58.391737620035805\n","Epoch: 97/5000, Train Loss: 59.4481745633212, Valid Loss: 58.31937917073568\n","Epoch: 98/5000, Train Loss: 59.3815637068315, Valid Loss: 58.30222956339518\n","Epoch: 99/5000, Train Loss: 59.3440388766202, Valid Loss: 58.217305501302086\n","Epoch: 100/5000, Train Loss: 59.237226312810726, Valid Loss: 58.125414530436196\n","Epoch: 101/5000, Train Loss: 59.21683536876332, Valid Loss: 58.123400370279946\n","Epoch: 102/5000, Train Loss: 59.15636513449929, Valid Loss: 58.05112075805664\n","Epoch: 103/5000, Train Loss: 59.08521790937944, Valid Loss: 57.97163518269857\n","Epoch: 104/5000, Train Loss: 58.98301800814542, Valid Loss: 57.919647216796875\n","Epoch: 105/5000, Train Loss: 59.04078431562944, Valid Loss: 57.883076985677086\n","Epoch: 106/5000, Train Loss: 58.91280191594904, Valid Loss: 57.79360834757487\n","Epoch: 107/5000, Train Loss: 58.85358810424805, Valid Loss: 57.77987798055013\n","Epoch: 108/5000, Train Loss: 58.79973602294922, Valid Loss: 57.74249776204427\n","Epoch: 109/5000, Train Loss: 58.7263298034668, Valid Loss: 57.684122721354164\n","Epoch: 110/5000, Train Loss: 58.678441827947445, Valid Loss: 57.544151306152344\n","Epoch: 111/5000, Train Loss: 58.55376087535512, Valid Loss: 57.54828770955404\n","Epoch: 112/5000, Train Loss: 58.536433133212, Valid Loss: 57.459832509358726\n","Epoch: 113/5000, Train Loss: 58.4066311229359, Valid Loss: 57.42290115356445\n","Epoch: 114/5000, Train Loss: 58.41159959272905, Valid Loss: 57.34602483113607\n","Epoch: 115/5000, Train Loss: 58.350831812078304, Valid Loss: 57.284707387288414\n","Epoch: 116/5000, Train Loss: 58.28160337968306, Valid Loss: 57.24038060506185\n","Epoch: 117/5000, Train Loss: 58.291682503440164, Valid Loss: 57.16271464029948\n","Epoch: 118/5000, Train Loss: 58.22450984608043, Valid Loss: 57.12334442138672\n","Epoch: 119/5000, Train Loss: 58.152387445623226, Valid Loss: 57.073097229003906\n","Epoch: 120/5000, Train Loss: 58.04901261763139, Valid Loss: 57.00707753499349\n","Epoch: 121/5000, Train Loss: 57.96667688543146, Valid Loss: 56.98263804117838\n","Epoch: 122/5000, Train Loss: 57.919773448597304, Valid Loss: 56.95913060506185\n","Epoch: 123/5000, Train Loss: 57.87792136452415, Valid Loss: 56.863850911458336\n","Epoch: 124/5000, Train Loss: 57.75367910211737, Valid Loss: 56.77211888631185\n","Epoch: 125/5000, Train Loss: 57.73276589133523, Valid Loss: 56.72885513305664\n","Epoch: 126/5000, Train Loss: 57.76010790738192, Valid Loss: 56.70092264811198\n","Epoch: 127/5000, Train Loss: 57.657329906116836, Valid Loss: 56.581251780192055\n","Epoch: 128/5000, Train Loss: 57.51579631458629, Valid Loss: 56.59451039632162\n","Epoch: 129/5000, Train Loss: 57.485332835804336, Valid Loss: 56.530694325764976\n","Epoch: 130/5000, Train Loss: 57.434927853670985, Valid Loss: 56.474507649739586\n","Epoch: 131/5000, Train Loss: 57.38889590176669, Valid Loss: 56.446004231770836\n","Epoch: 132/5000, Train Loss: 57.310238578102805, Valid Loss: 56.3193105061849\n","Epoch: 133/5000, Train Loss: 57.20479410344904, Valid Loss: 56.23987579345703\n","Epoch: 134/5000, Train Loss: 57.20390701293945, Valid Loss: 56.206991831461586\n","Epoch: 135/5000, Train Loss: 57.19642361727628, Valid Loss: 56.17117563883463\n","Epoch: 136/5000, Train Loss: 57.09629301591353, Valid Loss: 56.145591735839844\n","Epoch: 137/5000, Train Loss: 57.05267750133168, Valid Loss: 56.104600270589195\n","Epoch: 138/5000, Train Loss: 56.921146392822266, Valid Loss: 56.023565928141274\n","Epoch: 139/5000, Train Loss: 56.89329979636452, Valid Loss: 55.86733754475912\n","Epoch: 140/5000, Train Loss: 56.915991349653765, Valid Loss: 55.84038162231445\n","Epoch: 141/5000, Train Loss: 56.708816875110976, Valid Loss: 55.861541748046875\n","Epoch: 142/5000, Train Loss: 56.69146659157493, Valid Loss: 55.769083658854164\n","Epoch: 143/5000, Train Loss: 56.64448408647017, Valid Loss: 55.754067738850914\n","Epoch: 144/5000, Train Loss: 56.513602863658555, Valid Loss: 55.64077885945638\n","Epoch: 145/5000, Train Loss: 56.468836697665125, Valid Loss: 55.621087392171226\n","Epoch: 146/5000, Train Loss: 56.51792214133523, Valid Loss: 55.54087702433268\n","Epoch: 147/5000, Train Loss: 56.3500695662065, Valid Loss: 55.45734532674154\n","Epoch: 148/5000, Train Loss: 56.29435764659535, Valid Loss: 55.3844362894694\n","Epoch: 149/5000, Train Loss: 56.31336836381392, Valid Loss: 55.32742563883463\n","Epoch: 150/5000, Train Loss: 56.041840293190695, Valid Loss: 55.19561640421549\n","Epoch: 151/5000, Train Loss: 56.12349319458008, Valid Loss: 55.189517974853516\n","Epoch: 152/5000, Train Loss: 56.09242317893288, Valid Loss: 55.1637077331543\n","Epoch: 153/5000, Train Loss: 56.0429555719549, Valid Loss: 55.11159133911133\n","Epoch: 154/5000, Train Loss: 55.98387076637962, Valid Loss: 55.04873911539713\n","Epoch: 155/5000, Train Loss: 55.84602251919833, Valid Loss: 55.022073109944664\n","Epoch: 156/5000, Train Loss: 55.75208733298562, Valid Loss: 54.950750986735024\n","Epoch: 157/5000, Train Loss: 55.75090304287997, Valid Loss: 54.92025375366211\n","Epoch: 158/5000, Train Loss: 55.73450261896307, Valid Loss: 54.757747650146484\n","Epoch: 159/5000, Train Loss: 55.65068192915483, Valid Loss: 54.73507944742838\n","Epoch: 160/5000, Train Loss: 55.473271109841086, Valid Loss: 54.697198232014976\n","Epoch: 161/5000, Train Loss: 55.51879605379972, Valid Loss: 54.623023986816406\n","Epoch: 162/5000, Train Loss: 55.42443362149325, Valid Loss: 54.591620127360024\n","Epoch: 163/5000, Train Loss: 55.28584046797319, Valid Loss: 54.49014409383138\n","Epoch: 164/5000, Train Loss: 55.220955241810195, Valid Loss: 54.400367736816406\n","Epoch: 165/5000, Train Loss: 55.208731564608485, Valid Loss: 54.31692377726237\n","Epoch: 166/5000, Train Loss: 55.08784658258612, Valid Loss: 54.28130594889323\n","Epoch: 167/5000, Train Loss: 55.010407187721945, Valid Loss: 54.25828297932943\n","Epoch: 168/5000, Train Loss: 54.998291015625, Valid Loss: 54.18152872721354\n","Epoch: 169/5000, Train Loss: 54.96239402077415, Valid Loss: 54.12421162923177\n","Epoch: 170/5000, Train Loss: 54.86202725497159, Valid Loss: 54.1338742574056\n","Epoch: 171/5000, Train Loss: 54.769058574329726, Valid Loss: 54.00872929890951\n","Epoch: 172/5000, Train Loss: 54.77058965509588, Valid Loss: 53.88270823160807\n","Epoch: 173/5000, Train Loss: 54.6981367631392, Valid Loss: 53.855943044026695\n","Epoch: 174/5000, Train Loss: 54.61323373967951, Valid Loss: 53.81025695800781\n","Epoch: 175/5000, Train Loss: 54.55793415416371, Valid Loss: 53.76488240559896\n","Epoch: 176/5000, Train Loss: 54.50139617919922, Valid Loss: 53.6678466796875\n","Epoch: 177/5000, Train Loss: 54.41121430830522, Valid Loss: 53.5496711730957\n","Epoch: 178/5000, Train Loss: 54.30298059636896, Valid Loss: 53.52228673299154\n","Epoch: 179/5000, Train Loss: 54.28517324274237, Valid Loss: 53.43199666341146\n","Epoch: 180/5000, Train Loss: 54.19098663330078, Valid Loss: 53.42251841227213\n","Epoch: 181/5000, Train Loss: 54.14669557051225, Valid Loss: 53.42887878417969\n","Epoch: 182/5000, Train Loss: 54.064285625110976, Valid Loss: 53.3192253112793\n","Epoch: 183/5000, Train Loss: 54.00343842939897, Valid Loss: 53.297149658203125\n","Epoch: 184/5000, Train Loss: 53.94691918113015, Valid Loss: 53.18571472167969\n","Epoch: 185/5000, Train Loss: 53.830834822221235, Valid Loss: 53.097974141438804\n","Epoch: 186/5000, Train Loss: 53.83252785422585, Valid Loss: 53.077894846598305\n","Epoch: 187/5000, Train Loss: 53.77369793978605, Valid Loss: 53.09930547078451\n","Epoch: 188/5000, Train Loss: 53.618154699152164, Valid Loss: 52.93881607055664\n","Epoch: 189/5000, Train Loss: 53.679357355291195, Valid Loss: 52.87269083658854\n","Epoch: 190/5000, Train Loss: 53.468508980490945, Valid Loss: 52.76973342895508\n","Epoch: 191/5000, Train Loss: 53.50463936545632, Valid Loss: 52.73843765258789\n","Epoch: 192/5000, Train Loss: 53.39995262839577, Valid Loss: 52.671966552734375\n","Epoch: 193/5000, Train Loss: 53.37436849420721, Valid Loss: 52.62913513183594\n","Epoch: 194/5000, Train Loss: 53.25653700395064, Valid Loss: 52.51761118570963\n","Epoch: 195/5000, Train Loss: 53.25443371859464, Valid Loss: 52.45573170979818\n","Epoch: 196/5000, Train Loss: 53.06221944635565, Valid Loss: 52.35632069905599\n","Epoch: 197/5000, Train Loss: 53.16473596746271, Valid Loss: 52.33968861897787\n","Epoch: 198/5000, Train Loss: 53.03337860107422, Valid Loss: 52.249454498291016\n","Epoch: 199/5000, Train Loss: 52.8377650867809, Valid Loss: 52.250710805257164\n","Epoch: 200/5000, Train Loss: 52.902936068448156, Valid Loss: 52.14679718017578\n","Epoch: 201/5000, Train Loss: 52.75315267389471, Valid Loss: 52.13125864664713\n","Epoch: 202/5000, Train Loss: 52.709348851984196, Valid Loss: 52.1220957438151\n","Epoch: 203/5000, Train Loss: 52.61100977117365, Valid Loss: 51.9883066813151\n","Epoch: 204/5000, Train Loss: 52.622521140358664, Valid Loss: 51.966897328694664\n","Epoch: 205/5000, Train Loss: 52.58334315906871, Valid Loss: 51.92026901245117\n","Epoch: 206/5000, Train Loss: 52.4644608931108, Valid Loss: 51.770633697509766\n","Epoch: 207/5000, Train Loss: 52.40769715742631, Valid Loss: 51.807909647623696\n","Epoch: 208/5000, Train Loss: 52.400181163441054, Valid Loss: 51.68314997355143\n","Epoch: 209/5000, Train Loss: 52.24177100441673, Valid Loss: 51.58800379435221\n","Epoch: 210/5000, Train Loss: 52.41863805597479, Valid Loss: 51.460453033447266\n","Epoch: 211/5000, Train Loss: 52.041074232621625, Valid Loss: 51.34653854370117\n","Epoch: 212/5000, Train Loss: 52.0441648309881, Valid Loss: 51.38785171508789\n","Epoch: 213/5000, Train Loss: 52.03489095514471, Valid Loss: 51.36742909749349\n","Epoch: 214/5000, Train Loss: 51.931930195201524, Valid Loss: 51.29633458455404\n","Epoch: 215/5000, Train Loss: 51.9074741710316, Valid Loss: 51.263057708740234\n","Epoch: 216/5000, Train Loss: 51.69696322354403, Valid Loss: 51.14307403564453\n","Epoch: 217/5000, Train Loss: 51.81221077658913, Valid Loss: 51.07882944742838\n","Epoch: 218/5000, Train Loss: 51.620330116965555, Valid Loss: 51.07963943481445\n","Epoch: 219/5000, Train Loss: 51.445376656272195, Valid Loss: 50.96847788492838\n","Epoch: 220/5000, Train Loss: 51.56055311723189, Valid Loss: 50.92903518676758\n","Epoch: 221/5000, Train Loss: 51.41667626120827, Valid Loss: 50.91210810343424\n","Epoch: 222/5000, Train Loss: 51.36133367365057, Valid Loss: 50.7974853515625\n","Epoch: 223/5000, Train Loss: 51.40271620316939, Valid Loss: 50.764031728108726\n","Epoch: 224/5000, Train Loss: 51.183928749778055, Valid Loss: 50.7023811340332\n","Epoch: 225/5000, Train Loss: 51.124597029252485, Valid Loss: 50.55133310953776\n","Epoch: 226/5000, Train Loss: 50.9862095225941, Valid Loss: 50.49067687988281\n","Epoch: 227/5000, Train Loss: 50.9876955205744, Valid Loss: 50.428534189860024\n","Epoch: 228/5000, Train Loss: 50.814208637584336, Valid Loss: 50.33023198445638\n","Epoch: 229/5000, Train Loss: 50.85102705522017, Valid Loss: 50.28472900390625\n","Epoch: 230/5000, Train Loss: 50.871123573996805, Valid Loss: 50.193868001302086\n","Epoch: 231/5000, Train Loss: 50.775539051402696, Valid Loss: 50.19829559326172\n","Epoch: 232/5000, Train Loss: 50.651924480091445, Valid Loss: 50.186832427978516\n","Epoch: 233/5000, Train Loss: 50.68674018166282, Valid Loss: 50.154232025146484\n","Epoch: 234/5000, Train Loss: 50.599495280872695, Valid Loss: 50.09170913696289\n","Epoch: 235/5000, Train Loss: 50.61143632368608, Valid Loss: 50.07238642374674\n","Epoch: 236/5000, Train Loss: 50.37558087435636, Valid Loss: 49.96379089355469\n","Epoch: 237/5000, Train Loss: 50.272944016890094, Valid Loss: 49.910301208496094\n","Epoch: 238/5000, Train Loss: 50.28422303633256, Valid Loss: 49.81579844156901\n","Epoch: 239/5000, Train Loss: 50.16343862360174, Valid Loss: 49.83393859863281\n","Epoch: 240/5000, Train Loss: 50.262236161665484, Valid Loss: 49.669568379720054\n","Epoch: 241/5000, Train Loss: 50.04029534079812, Valid Loss: 49.623111724853516\n","Epoch: 242/5000, Train Loss: 49.99094148115678, Valid Loss: 49.52253341674805\n","Epoch: 243/5000, Train Loss: 49.98891761086204, Valid Loss: 49.441202799479164\n","Epoch: 244/5000, Train Loss: 49.889761144464664, Valid Loss: 49.39910761515299\n","Epoch: 245/5000, Train Loss: 49.75458804043856, Valid Loss: 49.31133270263672\n","Epoch: 246/5000, Train Loss: 49.70962142944336, Valid Loss: 49.2245979309082\n","Epoch: 247/5000, Train Loss: 49.661134546453304, Valid Loss: 49.1516965230306\n","Epoch: 248/5000, Train Loss: 49.61711571433327, Valid Loss: 49.14249928792318\n","Epoch: 249/5000, Train Loss: 49.44161536476829, Valid Loss: 49.11329905192057\n","Epoch: 250/5000, Train Loss: 49.54119803688743, Valid Loss: 49.00260798136393\n","Epoch: 251/5000, Train Loss: 49.32296440818093, Valid Loss: 49.03938420613607\n","Epoch: 252/5000, Train Loss: 49.443357641046696, Valid Loss: 48.948689778645836\n","Epoch: 253/5000, Train Loss: 49.31382369995117, Valid Loss: 48.85418446858724\n","Epoch: 254/5000, Train Loss: 49.15024740045721, Valid Loss: 48.834476470947266\n","Epoch: 255/5000, Train Loss: 49.10133049704812, Valid Loss: 48.74560419718424\n","Epoch: 256/5000, Train Loss: 48.97273497147994, Valid Loss: 48.66127268473307\n","Epoch: 257/5000, Train Loss: 48.93667706576261, Valid Loss: 48.58227030436198\n","Epoch: 258/5000, Train Loss: 49.12026526711204, Valid Loss: 48.503378550211586\n","Epoch: 259/5000, Train Loss: 48.912996465509586, Valid Loss: 48.51226933797201\n","Epoch: 260/5000, Train Loss: 48.87424538352273, Valid Loss: 48.39206059773763\n","Epoch: 261/5000, Train Loss: 48.664246645840734, Valid Loss: 48.430032094319664\n","Epoch: 262/5000, Train Loss: 48.77399444580078, Valid Loss: 48.31562932332357\n","Epoch: 263/5000, Train Loss: 48.569156646728516, Valid Loss: 48.22827402750651\n","Epoch: 264/5000, Train Loss: 48.43608544089577, Valid Loss: 48.1194216410319\n","Epoch: 265/5000, Train Loss: 48.552434401078656, Valid Loss: 48.18271255493164\n","Epoch: 266/5000, Train Loss: 48.41053009033203, Valid Loss: 48.04149881998698\n","Epoch: 267/5000, Train Loss: 48.30875084616921, Valid Loss: 48.00945536295573\n","Epoch: 268/5000, Train Loss: 48.26128595525568, Valid Loss: 47.91804631551107\n","Epoch: 269/5000, Train Loss: 48.20307020707564, Valid Loss: 47.89227040608724\n","Epoch: 270/5000, Train Loss: 48.09654790704901, Valid Loss: 47.79047520955404\n","Epoch: 271/5000, Train Loss: 47.99871826171875, Valid Loss: 47.74899673461914\n","Epoch: 272/5000, Train Loss: 48.01963875510476, Valid Loss: 47.689276377360024\n","Epoch: 273/5000, Train Loss: 48.00604456121271, Valid Loss: 47.55319595336914\n","Epoch: 274/5000, Train Loss: 47.677308862859554, Valid Loss: 47.472947438557945\n","Epoch: 275/5000, Train Loss: 47.75420934503729, Valid Loss: 47.481727600097656\n","Epoch: 276/5000, Train Loss: 47.70899616588246, Valid Loss: 47.3978525797526\n","Epoch: 277/5000, Train Loss: 47.697081479159266, Valid Loss: 47.427389780680336\n","Epoch: 278/5000, Train Loss: 47.53915752064098, Valid Loss: 47.259361267089844\n","Epoch: 279/5000, Train Loss: 47.46912869540128, Valid Loss: 47.242139180501304\n","Epoch: 280/5000, Train Loss: 47.58217482133345, Valid Loss: 47.16032028198242\n","Epoch: 281/5000, Train Loss: 47.230791958895594, Valid Loss: 47.20757039388021\n","Epoch: 282/5000, Train Loss: 47.339839241721414, Valid Loss: 47.11816660563151\n","Epoch: 283/5000, Train Loss: 47.26526295055043, Valid Loss: 47.0386708577474\n","Epoch: 284/5000, Train Loss: 47.032440879128195, Valid Loss: 46.98133850097656\n","Epoch: 285/5000, Train Loss: 47.23427893898704, Valid Loss: 46.88772837320963\n","Epoch: 286/5000, Train Loss: 47.004154205322266, Valid Loss: 46.81567891438802\n","Epoch: 287/5000, Train Loss: 47.15556370128285, Valid Loss: 46.93388112386068\n","Epoch: 288/5000, Train Loss: 46.87198985706676, Valid Loss: 46.86799875895182\n","Epoch: 289/5000, Train Loss: 46.67483832619407, Valid Loss: 46.70559819539388\n","Epoch: 290/5000, Train Loss: 46.784421400590375, Valid Loss: 46.613442738850914\n","Epoch: 291/5000, Train Loss: 46.51004895296964, Valid Loss: 46.51018524169922\n","Epoch: 292/5000, Train Loss: 46.65391471169212, Valid Loss: 46.43513743082682\n","Epoch: 293/5000, Train Loss: 46.625941883433946, Valid Loss: 46.44624455769857\n","Epoch: 294/5000, Train Loss: 46.56525629216974, Valid Loss: 46.50500233968099\n","Epoch: 295/5000, Train Loss: 46.45840558138761, Valid Loss: 46.422515869140625\n","Epoch: 296/5000, Train Loss: 46.4856615933505, Valid Loss: 46.18915939331055\n","Epoch: 297/5000, Train Loss: 46.45067284323952, Valid Loss: 46.12575149536133\n","Epoch: 298/5000, Train Loss: 46.231996362859554, Valid Loss: 46.14977010091146\n","Epoch: 299/5000, Train Loss: 46.172305367209695, Valid Loss: 46.04200108846029\n","Epoch: 300/5000, Train Loss: 46.10356556285512, Valid Loss: 46.08335876464844\n","Epoch: 301/5000, Train Loss: 46.087244900790125, Valid Loss: 45.98477554321289\n","Epoch: 302/5000, Train Loss: 46.03040521795099, Valid Loss: 45.9507802327474\n","Epoch: 303/5000, Train Loss: 45.8730635209517, Valid Loss: 45.86938222249349\n","Epoch: 304/5000, Train Loss: 46.034602425315164, Valid Loss: 45.7458610534668\n","Epoch: 305/5000, Train Loss: 45.74576187133789, Valid Loss: 45.75915273030599\n","Epoch: 306/5000, Train Loss: 45.74784955111417, Valid Loss: 45.69746526082357\n","Epoch: 307/5000, Train Loss: 45.823613600297406, Valid Loss: 45.69190979003906\n","Epoch: 308/5000, Train Loss: 45.62057321721857, Valid Loss: 45.63344065348307\n","Epoch: 309/5000, Train Loss: 45.30540154196999, Valid Loss: 45.533565521240234\n","Epoch: 310/5000, Train Loss: 45.54451508955522, Valid Loss: 45.490821838378906\n","Epoch: 311/5000, Train Loss: 45.36065535111861, Valid Loss: 45.41526412963867\n","Epoch: 312/5000, Train Loss: 45.19435570456765, Valid Loss: 45.3070068359375\n","Epoch: 313/5000, Train Loss: 45.352401386607774, Valid Loss: 45.25800450642904\n","Epoch: 314/5000, Train Loss: 45.34719675237482, Valid Loss: 45.22779846191406\n","Epoch: 315/5000, Train Loss: 45.16059632734819, Valid Loss: 45.147098541259766\n","Epoch: 316/5000, Train Loss: 45.09717906605113, Valid Loss: 45.124396006266274\n","Epoch: 317/5000, Train Loss: 44.87586108121005, Valid Loss: 45.061909993489586\n","Epoch: 318/5000, Train Loss: 45.0050496188077, Valid Loss: 45.031533559163414\n","Epoch: 319/5000, Train Loss: 44.99572164362127, Valid Loss: 44.863118489583336\n","Epoch: 320/5000, Train Loss: 44.81573902476918, Valid Loss: 44.77768198649088\n","Epoch: 321/5000, Train Loss: 44.716671683571555, Valid Loss: 44.77636845906576\n","Epoch: 322/5000, Train Loss: 44.62249409068715, Valid Loss: 44.659566243489586\n","Epoch: 323/5000, Train Loss: 44.79142795909535, Valid Loss: 44.57634735107422\n","Epoch: 324/5000, Train Loss: 44.46791735562411, Valid Loss: 44.634700775146484\n","Epoch: 325/5000, Train Loss: 44.70010480013761, Valid Loss: 44.58208211263021\n","Epoch: 326/5000, Train Loss: 44.501800537109375, Valid Loss: 44.5030517578125\n","Epoch: 327/5000, Train Loss: 44.43765570900657, Valid Loss: 44.467141469319664\n","Epoch: 328/5000, Train Loss: 44.460453033447266, Valid Loss: 44.33183161417643\n","Epoch: 329/5000, Train Loss: 44.31972746415572, Valid Loss: 44.331406911214195\n","Epoch: 330/5000, Train Loss: 44.12248507412997, Valid Loss: 44.16957219441732\n","Epoch: 331/5000, Train Loss: 44.101905822753906, Valid Loss: 44.14147822062174\n","Epoch: 332/5000, Train Loss: 43.974837910045274, Valid Loss: 44.18537521362305\n","Epoch: 333/5000, Train Loss: 44.17836692116477, Valid Loss: 44.168025970458984\n","Epoch: 334/5000, Train Loss: 44.06890244917436, Valid Loss: 44.117767333984375\n","Epoch: 335/5000, Train Loss: 43.919489773837, Valid Loss: 44.101837158203125\n","Epoch: 336/5000, Train Loss: 43.79080859097567, Valid Loss: 43.984580993652344\n","Epoch: 337/5000, Train Loss: 43.74406051635742, Valid Loss: 43.87887064615885\n","Epoch: 338/5000, Train Loss: 43.60967323996804, Valid Loss: 43.877288818359375\n","Epoch: 339/5000, Train Loss: 43.760346152565695, Valid Loss: 43.776650746663414\n","Epoch: 340/5000, Train Loss: 43.459318681196734, Valid Loss: 43.70072555541992\n","Epoch: 341/5000, Train Loss: 43.6451582475142, Valid Loss: 43.57957331339518\n","Epoch: 342/5000, Train Loss: 43.323523087935015, Valid Loss: 43.62162780761719\n","Epoch: 343/5000, Train Loss: 43.30549933693626, Valid Loss: 43.46998087565104\n","Epoch: 344/5000, Train Loss: 43.38286590576172, Valid Loss: 43.405836741129555\n","Epoch: 345/5000, Train Loss: 43.360590501265094, Valid Loss: 43.43841552734375\n","Epoch: 346/5000, Train Loss: 43.25114510276101, Valid Loss: 43.428460439046226\n","Epoch: 347/5000, Train Loss: 43.07567769830877, Valid Loss: 43.229496002197266\n","Epoch: 348/5000, Train Loss: 43.10576629638672, Valid Loss: 43.137621561686196\n","Epoch: 349/5000, Train Loss: 43.129544691606, Valid Loss: 43.25284322102865\n","Epoch: 350/5000, Train Loss: 42.90416890924627, Valid Loss: 43.14438501993815\n","Epoch: 351/5000, Train Loss: 43.074236089533024, Valid Loss: 43.10757573445638\n","Epoch: 352/5000, Train Loss: 43.03623442216353, Valid Loss: 43.05251693725586\n","Epoch: 353/5000, Train Loss: 42.94480583884499, Valid Loss: 43.052154541015625\n","Epoch: 354/5000, Train Loss: 42.47990729592063, Valid Loss: 42.86480840047201\n","Epoch: 355/5000, Train Loss: 42.64390009099787, Valid Loss: 42.913065592447914\n","Epoch: 356/5000, Train Loss: 42.66255153309215, Valid Loss: 42.817858378092446\n","Epoch: 357/5000, Train Loss: 42.61347545276988, Valid Loss: 42.75503921508789\n","Epoch: 358/5000, Train Loss: 42.498863913796164, Valid Loss: 42.73373031616211\n","Epoch: 359/5000, Train Loss: 42.5683559070934, Valid Loss: 42.633235931396484\n","Epoch: 360/5000, Train Loss: 42.34873442216353, Valid Loss: 42.617899576822914\n","Epoch: 361/5000, Train Loss: 42.296330885453656, Valid Loss: 42.549068450927734\n","Epoch: 362/5000, Train Loss: 42.111075314608485, Valid Loss: 42.495094299316406\n","Epoch: 363/5000, Train Loss: 42.33607136119496, Valid Loss: 42.45141728719076\n","Epoch: 364/5000, Train Loss: 42.235552701083094, Valid Loss: 42.350274403889976\n","Epoch: 365/5000, Train Loss: 42.24761338667436, Valid Loss: 42.37015024820963\n","Epoch: 366/5000, Train Loss: 41.997787128795274, Valid Loss: 42.325705210367836\n","Epoch: 367/5000, Train Loss: 42.13268834894354, Valid Loss: 42.190687815348305\n","Epoch: 368/5000, Train Loss: 42.00352755459872, Valid Loss: 42.13718287150065\n","Epoch: 369/5000, Train Loss: 41.89019636674361, Valid Loss: 42.10758717854818\n","Epoch: 370/5000, Train Loss: 41.71854990178888, Valid Loss: 42.15425237019857\n","Epoch: 371/5000, Train Loss: 41.91509732333097, Valid Loss: 42.08115259806315\n","Epoch: 372/5000, Train Loss: 41.72953310879794, Valid Loss: 42.01341120402018\n","Epoch: 373/5000, Train Loss: 41.78877084905451, Valid Loss: 41.93365732828776\n","Epoch: 374/5000, Train Loss: 41.691533522172406, Valid Loss: 41.80958048502604\n","Epoch: 375/5000, Train Loss: 41.642654765735976, Valid Loss: 41.905226389567055\n","Epoch: 376/5000, Train Loss: 41.53399727561257, Valid Loss: 41.892251332600914\n","Epoch: 377/5000, Train Loss: 41.52752824263139, Valid Loss: 41.86050542195638\n","Epoch: 378/5000, Train Loss: 41.33098220825195, Valid Loss: 41.76959482828776\n","Epoch: 379/5000, Train Loss: 41.353241660378195, Valid Loss: 41.67120869954427\n","Epoch: 380/5000, Train Loss: 41.40624167702415, Valid Loss: 41.68200556437174\n","Epoch: 381/5000, Train Loss: 41.34775855324485, Valid Loss: 41.55732345581055\n","Epoch: 382/5000, Train Loss: 41.23614155162465, Valid Loss: 41.39635976155599\n","Epoch: 383/5000, Train Loss: 41.21120383522727, Valid Loss: 41.46706008911133\n","Epoch: 384/5000, Train Loss: 41.032051433216445, Valid Loss: 41.33475240071615\n","Epoch: 385/5000, Train Loss: 41.21901737559926, Valid Loss: 41.41098276774088\n","Epoch: 386/5000, Train Loss: 40.93717956542969, Valid Loss: 41.260918935139976\n","Epoch: 387/5000, Train Loss: 40.800249966708094, Valid Loss: 41.2144406636556\n","Epoch: 388/5000, Train Loss: 40.86212366277521, Valid Loss: 41.17997360229492\n","Epoch: 389/5000, Train Loss: 41.13181270252574, Valid Loss: 41.20559310913086\n","Epoch: 390/5000, Train Loss: 40.89288260719993, Valid Loss: 41.06198755900065\n","Epoch: 391/5000, Train Loss: 40.62971253828569, Valid Loss: 41.07119369506836\n","Epoch: 392/5000, Train Loss: 40.73849591341886, Valid Loss: 41.045815785725914\n","Epoch: 393/5000, Train Loss: 40.75642013549805, Valid Loss: 40.90286382039388\n","Epoch: 394/5000, Train Loss: 40.64251084761186, Valid Loss: 40.842793782552086\n","Epoch: 395/5000, Train Loss: 40.31700966574929, Valid Loss: 40.8094367980957\n","Epoch: 396/5000, Train Loss: 40.52779423106801, Valid Loss: 40.743176778157554\n","Epoch: 397/5000, Train Loss: 40.47090807828036, Valid Loss: 40.688741048177086\n","Epoch: 398/5000, Train Loss: 40.25035892833363, Valid Loss: 40.70659891764323\n","Epoch: 399/5000, Train Loss: 40.57408696954901, Valid Loss: 40.70791880289713\n","Epoch: 400/5000, Train Loss: 40.21428264271129, Valid Loss: 40.69024785359701\n","Epoch: 401/5000, Train Loss: 40.16256471113725, Valid Loss: 40.57266108194987\n","Epoch: 402/5000, Train Loss: 40.27883807095614, Valid Loss: 40.54501978556315\n","Epoch: 403/5000, Train Loss: 40.18005440451882, Valid Loss: 40.501200358072914\n","Epoch: 404/5000, Train Loss: 40.11741534146395, Valid Loss: 40.46560287475586\n","Epoch: 405/5000, Train Loss: 40.17375391179865, Valid Loss: 40.408660888671875\n","Epoch: 406/5000, Train Loss: 39.78015067360618, Valid Loss: 40.388580322265625\n","Epoch: 407/5000, Train Loss: 40.00293697010387, Valid Loss: 40.301657358805336\n","Epoch: 408/5000, Train Loss: 39.891693115234375, Valid Loss: 40.244486490885414\n","Epoch: 409/5000, Train Loss: 39.70341561057351, Valid Loss: 40.16414133707682\n","Epoch: 410/5000, Train Loss: 40.01644654707475, Valid Loss: 40.151014963785805\n","Epoch: 411/5000, Train Loss: 39.70739329944957, Valid Loss: 40.17036437988281\n","Epoch: 412/5000, Train Loss: 39.563961722634055, Valid Loss: 40.128763834635414\n","Epoch: 413/5000, Train Loss: 39.760691209272906, Valid Loss: 39.87168629964193\n","Epoch: 414/5000, Train Loss: 39.734651045365766, Valid Loss: 39.96562576293945\n","Epoch: 415/5000, Train Loss: 39.5637172352184, Valid Loss: 39.90797678629557\n","Epoch: 416/5000, Train Loss: 39.47723249955611, Valid Loss: 39.90135192871094\n","Epoch: 417/5000, Train Loss: 39.432002327658914, Valid Loss: 39.86170196533203\n","Epoch: 418/5000, Train Loss: 39.41907327825373, Valid Loss: 39.79342269897461\n","Epoch: 419/5000, Train Loss: 39.37281521883878, Valid Loss: 39.78384272257487\n","Epoch: 420/5000, Train Loss: 39.20755039561879, Valid Loss: 39.83709208170573\n","Epoch: 421/5000, Train Loss: 39.249868566339664, Valid Loss: 39.75956471761068\n","Epoch: 422/5000, Train Loss: 39.35621088201349, Valid Loss: 39.627950032552086\n","Epoch: 423/5000, Train Loss: 39.21222548051314, Valid Loss: 39.52909851074219\n","Epoch: 424/5000, Train Loss: 39.09174936467951, Valid Loss: 39.36788813273112\n","Epoch: 425/5000, Train Loss: 38.84616955843839, Valid Loss: 39.417747497558594\n","Epoch: 426/5000, Train Loss: 38.97587897560813, Valid Loss: 39.43372599283854\n","Epoch: 427/5000, Train Loss: 38.83883354880593, Valid Loss: 39.29285685221354\n","Epoch: 428/5000, Train Loss: 38.772034731778234, Valid Loss: 39.286816914876304\n","Epoch: 429/5000, Train Loss: 38.787776947021484, Valid Loss: 39.27887725830078\n","Epoch: 430/5000, Train Loss: 38.67484872991388, Valid Loss: 39.23037083943685\n","Epoch: 431/5000, Train Loss: 38.62269696322355, Valid Loss: 39.19410959879557\n","Epoch: 432/5000, Train Loss: 38.806387121027164, Valid Loss: 39.041751861572266\n","Epoch: 433/5000, Train Loss: 38.57380537553267, Valid Loss: 39.0754280090332\n","Epoch: 434/5000, Train Loss: 38.846323533491656, Valid Loss: 39.062451680501304\n","Epoch: 435/5000, Train Loss: 38.60749782215465, Valid Loss: 39.04622904459635\n","Epoch: 436/5000, Train Loss: 38.719933943314985, Valid Loss: 38.94388961791992\n","Epoch: 437/5000, Train Loss: 38.27941825173118, Valid Loss: 38.96784464518229\n","Epoch: 438/5000, Train Loss: 38.515778281471945, Valid Loss: 38.84704335530599\n","Epoch: 439/5000, Train Loss: 38.421890605579726, Valid Loss: 38.89672978719076\n","Epoch: 440/5000, Train Loss: 38.308079806241125, Valid Loss: 38.745749155680336\n","Epoch: 441/5000, Train Loss: 38.25088396939364, Valid Loss: 38.788308461507164\n","Epoch: 442/5000, Train Loss: 38.21968390724876, Valid Loss: 38.68776830037435\n","Epoch: 443/5000, Train Loss: 38.309266177090734, Valid Loss: 38.67082722981771\n","Epoch: 444/5000, Train Loss: 38.2133955522017, Valid Loss: 38.6482899983724\n","Epoch: 445/5000, Train Loss: 38.145497755570844, Valid Loss: 38.535196940104164\n","Epoch: 446/5000, Train Loss: 38.039335771040484, Valid Loss: 38.48466364542643\n","Epoch: 447/5000, Train Loss: 38.142718922008164, Valid Loss: 38.511348724365234\n","Epoch: 448/5000, Train Loss: 37.98696240511808, Valid Loss: 38.426005045572914\n","Epoch: 449/5000, Train Loss: 37.830026453191586, Valid Loss: 38.41536204020182\n","Epoch: 450/5000, Train Loss: 37.876014015891336, Valid Loss: 38.35737228393555\n","Epoch: 451/5000, Train Loss: 37.92490491000089, Valid Loss: 38.287986755371094\n","Epoch: 452/5000, Train Loss: 37.74799554998224, Valid Loss: 38.26668039957682\n","Epoch: 453/5000, Train Loss: 37.837515744295985, Valid Loss: 38.33239618937174\n","Epoch: 454/5000, Train Loss: 37.955428730357774, Valid Loss: 38.23408762613932\n","Epoch: 455/5000, Train Loss: 37.734769300981, Valid Loss: 38.130863189697266\n","Epoch: 456/5000, Train Loss: 37.466764970259234, Valid Loss: 38.038552602132164\n","Epoch: 457/5000, Train Loss: 37.83999911221591, Valid Loss: 38.00335947672526\n","Epoch: 458/5000, Train Loss: 37.65171016346324, Valid Loss: 38.01983642578125\n","Epoch: 459/5000, Train Loss: 37.67365126176314, Valid Loss: 38.009002685546875\n","Epoch: 460/5000, Train Loss: 37.66735007546165, Valid Loss: 38.02456283569336\n","Epoch: 461/5000, Train Loss: 37.54859230735085, Valid Loss: 37.99811299641927\n","Epoch: 462/5000, Train Loss: 37.496789412064985, Valid Loss: 37.936875661214195\n","Epoch: 463/5000, Train Loss: 37.6565714749423, Valid Loss: 37.86639277140299\n","Epoch: 464/5000, Train Loss: 37.28886586969549, Valid Loss: 37.81968688964844\n","Epoch: 465/5000, Train Loss: 37.187082117254086, Valid Loss: 37.808135986328125\n","Epoch: 466/5000, Train Loss: 37.48820252852006, Valid Loss: 37.729960123697914\n","Epoch: 467/5000, Train Loss: 36.91740902987394, Valid Loss: 37.74574661254883\n","Epoch: 468/5000, Train Loss: 37.18123418634588, Valid Loss: 37.767303466796875\n","Epoch: 469/5000, Train Loss: 37.19747647372159, Valid Loss: 37.61831792195638\n","Epoch: 470/5000, Train Loss: 37.19776118885387, Valid Loss: 37.582831064860024\n","Epoch: 471/5000, Train Loss: 36.95213629982688, Valid Loss: 37.591470082600914\n","Epoch: 472/5000, Train Loss: 36.943723851984196, Valid Loss: 37.53697967529297\n","Epoch: 473/5000, Train Loss: 36.86752215298739, Valid Loss: 37.57992808024088\n","Epoch: 474/5000, Train Loss: 37.04513445767489, Valid Loss: 37.52538299560547\n","Epoch: 475/5000, Train Loss: 36.997986880215734, Valid Loss: 37.421390533447266\n","Epoch: 476/5000, Train Loss: 36.707702289928086, Valid Loss: 37.41683832804362\n","Epoch: 477/5000, Train Loss: 36.61963514848189, Valid Loss: 37.38151931762695\n","Epoch: 478/5000, Train Loss: 36.98124625466087, Valid Loss: 37.28479894002279\n","Epoch: 479/5000, Train Loss: 36.61336760087447, Valid Loss: 37.2083485921224\n","Epoch: 480/5000, Train Loss: 36.974268132990055, Valid Loss: 37.159924825032554\n","Epoch: 481/5000, Train Loss: 36.952537536621094, Valid Loss: 37.175740559895836\n","Epoch: 482/5000, Train Loss: 36.492685838179156, Valid Loss: 37.156551361083984\n","Epoch: 483/5000, Train Loss: 36.48501690951261, Valid Loss: 37.098199208577476\n","Epoch: 484/5000, Train Loss: 36.455839677290484, Valid Loss: 37.036930084228516\n","Epoch: 485/5000, Train Loss: 36.75652729381215, Valid Loss: 36.98366038004557\n","Epoch: 486/5000, Train Loss: 36.65553283691406, Valid Loss: 36.98728052775065\n","Epoch: 487/5000, Train Loss: 36.39932563088157, Valid Loss: 37.029500325520836\n","Epoch: 488/5000, Train Loss: 36.533232948996805, Valid Loss: 36.931374867757164\n","Epoch: 489/5000, Train Loss: 36.3399807323109, Valid Loss: 36.856459299723305\n","Epoch: 490/5000, Train Loss: 36.283235723322086, Valid Loss: 36.73741022745768\n","Epoch: 491/5000, Train Loss: 36.483266310258344, Valid Loss: 36.771793365478516\n","Epoch: 492/5000, Train Loss: 36.29799582741477, Valid Loss: 36.722601572672524\n","Epoch: 493/5000, Train Loss: 36.00979336825284, Valid Loss: 36.71438852945963\n","Epoch: 494/5000, Train Loss: 36.28099753639915, Valid Loss: 36.5920295715332\n","Epoch: 495/5000, Train Loss: 36.22037748856978, Valid Loss: 36.62202580769857\n","Epoch: 496/5000, Train Loss: 36.10636728460138, Valid Loss: 36.536258697509766\n","Epoch: 497/5000, Train Loss: 35.97452510486949, Valid Loss: 36.542564392089844\n","Epoch: 498/5000, Train Loss: 36.11275031349876, Valid Loss: 36.52412668863932\n","Epoch: 499/5000, Train Loss: 36.1221792047674, Valid Loss: 36.496283213297524\n","Epoch: 500/5000, Train Loss: 35.91620844060724, Valid Loss: 36.435648600260414\n","Epoch: 501/5000, Train Loss: 36.07629949396307, Valid Loss: 36.45679601033529\n","Epoch: 502/5000, Train Loss: 35.83775468306108, Valid Loss: 36.342393239339195\n","Epoch: 503/5000, Train Loss: 35.7513427734375, Valid Loss: 36.26158014933268\n","Epoch: 504/5000, Train Loss: 35.91472140225497, Valid Loss: 36.27484130859375\n","Epoch: 505/5000, Train Loss: 35.934011632745914, Valid Loss: 36.2579231262207\n","Epoch: 506/5000, Train Loss: 35.95389140735973, Valid Loss: 36.17686080932617\n","Epoch: 507/5000, Train Loss: 35.871347947554156, Valid Loss: 36.19014994303385\n","Epoch: 508/5000, Train Loss: 35.729954806241125, Valid Loss: 36.20353698730469\n","Epoch: 509/5000, Train Loss: 35.60390299016779, Valid Loss: 36.12948735555013\n","Epoch: 510/5000, Train Loss: 35.62085931951349, Valid Loss: 36.05132166544596\n","Epoch: 511/5000, Train Loss: 35.75004785711115, Valid Loss: 36.047603607177734\n","Epoch: 512/5000, Train Loss: 35.41194222190163, Valid Loss: 36.03341929117838\n","Epoch: 513/5000, Train Loss: 35.65856205333363, Valid Loss: 35.992600758870445\n","Epoch: 514/5000, Train Loss: 35.318885109641336, Valid Loss: 35.94513448079427\n","Epoch: 515/5000, Train Loss: 35.40350757945668, Valid Loss: 35.823891957600914\n","Epoch: 516/5000, Train Loss: 35.438967964865945, Valid Loss: 35.810218811035156\n","Epoch: 517/5000, Train Loss: 35.48337797685103, Valid Loss: 35.81668345133463\n","Epoch: 518/5000, Train Loss: 35.6069204157049, Valid Loss: 35.723279317220054\n","Epoch: 519/5000, Train Loss: 35.40140221335671, Valid Loss: 35.7257194519043\n","Epoch: 520/5000, Train Loss: 35.53916410966353, Valid Loss: 35.77161153157552\n","Epoch: 521/5000, Train Loss: 35.154694990678266, Valid Loss: 35.76610565185547\n","Epoch: 522/5000, Train Loss: 35.58476638793945, Valid Loss: 35.71143595377604\n","Epoch: 523/5000, Train Loss: 35.05785439231179, Valid Loss: 35.55677159627279\n","Epoch: 524/5000, Train Loss: 35.40828635475852, Valid Loss: 35.5219980875651\n","Epoch: 525/5000, Train Loss: 35.186973571777344, Valid Loss: 35.57797495524088\n","Epoch: 526/5000, Train Loss: 35.099425229159266, Valid Loss: 35.559888203938804\n","Epoch: 527/5000, Train Loss: 35.243588881059125, Valid Loss: 35.46082051595052\n","Epoch: 528/5000, Train Loss: 35.34950672496449, Valid Loss: 35.528160095214844\n","Epoch: 529/5000, Train Loss: 34.99949819391424, Valid Loss: 35.48662185668945\n","Epoch: 530/5000, Train Loss: 34.81601264260032, Valid Loss: 35.43372599283854\n","Epoch: 531/5000, Train Loss: 34.928607420487836, Valid Loss: 35.402320861816406\n","Epoch: 532/5000, Train Loss: 34.68979055231268, Valid Loss: 35.35071818033854\n","Epoch: 533/5000, Train Loss: 34.94462030584162, Valid Loss: 35.34888203938802\n","Epoch: 534/5000, Train Loss: 34.89023867520419, Valid Loss: 35.28828811645508\n","Epoch: 535/5000, Train Loss: 34.87286585027521, Valid Loss: 35.324728647867836\n","Epoch: 536/5000, Train Loss: 34.95318464799361, Valid Loss: 35.21099726359049\n","Epoch: 537/5000, Train Loss: 34.97620253129439, Valid Loss: 35.22406768798828\n","Epoch: 538/5000, Train Loss: 34.89175102927468, Valid Loss: 35.19403203328451\n","Epoch: 539/5000, Train Loss: 34.802061947909266, Valid Loss: 35.1953379313151\n","Epoch: 540/5000, Train Loss: 34.71558518843217, Valid Loss: 35.16972732543945\n","Epoch: 541/5000, Train Loss: 34.89093364368785, Valid Loss: 35.10077667236328\n","Epoch: 542/5000, Train Loss: 34.918425820090555, Valid Loss: 35.138292948404946\n","Epoch: 543/5000, Train Loss: 34.742970726706766, Valid Loss: 35.090579986572266\n","Epoch: 544/5000, Train Loss: 34.78613974831321, Valid Loss: 35.029833475748696\n","Epoch: 545/5000, Train Loss: 34.61770595203746, Valid Loss: 34.986166636149086\n","Epoch: 546/5000, Train Loss: 34.6841215653853, Valid Loss: 35.01247024536133\n","Epoch: 547/5000, Train Loss: 34.6774621443315, Valid Loss: 35.03413009643555\n","Epoch: 548/5000, Train Loss: 34.47577025673606, Valid Loss: 34.973323822021484\n","Epoch: 549/5000, Train Loss: 34.79987161809748, Valid Loss: 34.90258280436198\n","Epoch: 550/5000, Train Loss: 34.34520374644887, Valid Loss: 34.9373410542806\n","Epoch: 551/5000, Train Loss: 34.745729966597125, Valid Loss: 34.93229675292969\n","Epoch: 552/5000, Train Loss: 34.52904493158514, Valid Loss: 34.90913009643555\n","Epoch: 553/5000, Train Loss: 34.42227207530629, Valid Loss: 34.796852111816406\n","Epoch: 554/5000, Train Loss: 34.04405108365145, Valid Loss: 34.767435709635414\n","Epoch: 555/5000, Train Loss: 34.07822487571023, Valid Loss: 34.765769958496094\n","Epoch: 556/5000, Train Loss: 34.3012001731179, Valid Loss: 34.727603912353516\n","Epoch: 557/5000, Train Loss: 34.30872622403231, Valid Loss: 34.788228352864586\n","Epoch: 558/5000, Train Loss: 34.23416519165039, Valid Loss: 34.73798751831055\n","Epoch: 559/5000, Train Loss: 34.534027966586024, Valid Loss: 34.68151982625326\n","Epoch: 560/5000, Train Loss: 34.43547612970526, Valid Loss: 34.734238942464195\n","Epoch: 561/5000, Train Loss: 34.37058500810103, Valid Loss: 34.645729064941406\n","Epoch: 562/5000, Train Loss: 34.1909403367476, Valid Loss: 34.600844065348305\n","Epoch: 563/5000, Train Loss: 33.95620744878595, Valid Loss: 34.536111195882164\n","Epoch: 564/5000, Train Loss: 34.24608334628019, Valid Loss: 34.51372528076172\n","Epoch: 565/5000, Train Loss: 34.366605585271664, Valid Loss: 34.5492197672526\n","Epoch: 566/5000, Train Loss: 34.00771921331232, Valid Loss: 34.50768152872721\n","Epoch: 567/5000, Train Loss: 34.16798002069647, Valid Loss: 34.449598948160805\n","Epoch: 568/5000, Train Loss: 34.28754563765092, Valid Loss: 34.448813120524086\n","Epoch: 569/5000, Train Loss: 34.26683044433594, Valid Loss: 34.47888692220052\n","Epoch: 570/5000, Train Loss: 33.893084265969016, Valid Loss: 34.384873708089195\n","Epoch: 571/5000, Train Loss: 34.10270552201705, Valid Loss: 34.48473866780599\n","Epoch: 572/5000, Train Loss: 34.26245880126953, Valid Loss: 34.455065409342446\n","Epoch: 573/5000, Train Loss: 34.12894491715865, Valid Loss: 34.479871114095054\n","Epoch: 574/5000, Train Loss: 34.27244741266424, Valid Loss: 34.398125966389976\n","Epoch: 575/5000, Train Loss: 33.8702449798584, Valid Loss: 34.43870417277018\n","Epoch: 576/5000, Train Loss: 34.205957586115055, Valid Loss: 34.34681193033854\n","Epoch: 577/5000, Train Loss: 33.86924500898881, Valid Loss: 34.35239919026693\n","Epoch: 578/5000, Train Loss: 33.97088189558549, Valid Loss: 34.3113047281901\n","Epoch: 579/5000, Train Loss: 33.97284698486328, Valid Loss: 34.28314971923828\n","Epoch: 580/5000, Train Loss: 34.24532144719904, Valid Loss: 34.27013397216797\n","Epoch: 581/5000, Train Loss: 33.7169962796298, Valid Loss: 34.22479120890299\n","Epoch: 582/5000, Train Loss: 33.90464262528853, Valid Loss: 34.2539914449056\n","Epoch: 583/5000, Train Loss: 33.80189670215953, Valid Loss: 34.252419789632164\n","Epoch: 584/5000, Train Loss: 34.06640035455877, Valid Loss: 34.25668716430664\n","Epoch: 585/5000, Train Loss: 33.75269629738548, Valid Loss: 34.2372792561849\n","Epoch: 586/5000, Train Loss: 33.87856535478072, Valid Loss: 34.1737314860026\n","Epoch: 587/5000, Train Loss: 33.58692828091708, Valid Loss: 34.131483713785805\n","Epoch: 588/5000, Train Loss: 33.59792223843661, Valid Loss: 34.11390813191732\n","Epoch: 589/5000, Train Loss: 33.79523953524503, Valid Loss: 34.10238774617513\n","Epoch: 590/5000, Train Loss: 33.915967594493516, Valid Loss: 34.06560389200846\n","Epoch: 591/5000, Train Loss: 33.60584571144798, Valid Loss: 34.06935246785482\n","Epoch: 592/5000, Train Loss: 33.69470370899547, Valid Loss: 34.03481801350912\n","Epoch: 593/5000, Train Loss: 33.542368628761984, Valid Loss: 34.041449228922524\n","Epoch: 594/5000, Train Loss: 33.683646115389735, Valid Loss: 34.069619496663414\n","Epoch: 595/5000, Train Loss: 33.587413267655805, Valid Loss: 34.018121083577476\n","Epoch: 596/5000, Train Loss: 33.51681327819824, Valid Loss: 33.969865798950195\n","Epoch: 597/5000, Train Loss: 33.876846833662555, Valid Loss: 33.93762715657552\n","Epoch: 598/5000, Train Loss: 33.76750460537997, Valid Loss: 33.9393622080485\n","Epoch: 599/5000, Train Loss: 33.87734049016779, Valid Loss: 33.9462095896403\n","Epoch: 600/5000, Train Loss: 33.339819648049094, Valid Loss: 33.93924077351888\n","Epoch: 601/5000, Train Loss: 33.496641332452946, Valid Loss: 33.87753105163574\n","Epoch: 602/5000, Train Loss: 33.70979135686701, Valid Loss: 33.88584200541178\n","Epoch: 603/5000, Train Loss: 33.60096203197133, Valid Loss: 33.82987276713053\n","Epoch: 604/5000, Train Loss: 33.78598906777122, Valid Loss: 33.80669911702474\n","Epoch: 605/5000, Train Loss: 33.59012707796964, Valid Loss: 33.792669932047524\n","Epoch: 606/5000, Train Loss: 33.561770872636274, Valid Loss: 33.79546928405762\n","Epoch: 607/5000, Train Loss: 33.390030254017226, Valid Loss: 33.81769688924154\n","Epoch: 608/5000, Train Loss: 33.14222370494496, Valid Loss: 33.797078450520836\n","Epoch: 609/5000, Train Loss: 33.22567211497914, Valid Loss: 33.78522046407064\n","Epoch: 610/5000, Train Loss: 33.633592952381484, Valid Loss: 33.78828430175781\n","Epoch: 611/5000, Train Loss: 33.66346914117987, Valid Loss: 33.778782526652016\n","Epoch: 612/5000, Train Loss: 33.52514856511896, Valid Loss: 33.782859802246094\n","Epoch: 613/5000, Train Loss: 33.73860896717418, Valid Loss: 33.72790400187174\n","Epoch: 614/5000, Train Loss: 33.25094829906117, Valid Loss: 33.623236338297524\n","Epoch: 615/5000, Train Loss: 33.301866878162734, Valid Loss: 33.67292149861654\n","Epoch: 616/5000, Train Loss: 33.53721375898881, Valid Loss: 33.66157595316569\n","Epoch: 617/5000, Train Loss: 33.29902804981578, Valid Loss: 33.63664182027181\n","Epoch: 618/5000, Train Loss: 33.18878746032715, Valid Loss: 33.627424240112305\n","Epoch: 619/5000, Train Loss: 33.436250339854844, Valid Loss: 33.586744944254555\n","Epoch: 620/5000, Train Loss: 33.60407534512606, Valid Loss: 33.6022834777832\n","Epoch: 621/5000, Train Loss: 33.22952738675204, Valid Loss: 33.601372400919594\n","Epoch: 622/5000, Train Loss: 33.40473053672097, Valid Loss: 33.5987294514974\n","Epoch: 623/5000, Train Loss: 33.679441625421696, Valid Loss: 33.58920923868815\n","Epoch: 624/5000, Train Loss: 33.25530728426847, Valid Loss: 33.58179219563802\n","Epoch: 625/5000, Train Loss: 33.091196753761984, Valid Loss: 33.6124636332194\n","Epoch: 626/5000, Train Loss: 33.417681607333094, Valid Loss: 33.532060623168945\n","Epoch: 627/5000, Train Loss: 33.146398370916195, Valid Loss: 33.51856931050619\n","Epoch: 628/5000, Train Loss: 32.91784650629217, Valid Loss: 33.52028846740723\n","Epoch: 629/5000, Train Loss: 33.45644708113237, Valid Loss: 33.517796198527016\n","Epoch: 630/5000, Train Loss: 33.24748784845526, Valid Loss: 33.543357849121094\n","Epoch: 631/5000, Train Loss: 33.30867229808461, Valid Loss: 33.526767094930015\n","Epoch: 632/5000, Train Loss: 33.27517977627841, Valid Loss: 33.526266733805336\n","Epoch: 633/5000, Train Loss: 32.830800316550516, Valid Loss: 33.523441314697266\n","Epoch: 634/5000, Train Loss: 33.22919828241522, Valid Loss: 33.46270942687988\n","Epoch: 635/5000, Train Loss: 33.034365914084695, Valid Loss: 33.500404357910156\n","Epoch: 636/5000, Train Loss: 33.346527099609375, Valid Loss: 33.44143104553223\n","Epoch: 637/5000, Train Loss: 33.021975083784625, Valid Loss: 33.42350196838379\n","Epoch: 638/5000, Train Loss: 33.11078314347701, Valid Loss: 33.41889254252116\n","Epoch: 639/5000, Train Loss: 32.80040897022594, Valid Loss: 33.42149543762207\n","Epoch: 640/5000, Train Loss: 32.89645957946777, Valid Loss: 33.38722165425619\n","Epoch: 641/5000, Train Loss: 33.30695932561701, Valid Loss: 33.37776883443197\n","Epoch: 642/5000, Train Loss: 33.48198075727983, Valid Loss: 33.385908126831055\n","Epoch: 643/5000, Train Loss: 33.046860608187586, Valid Loss: 33.36203257242838\n","Epoch: 644/5000, Train Loss: 33.38546978343617, Valid Loss: 33.38387235005697\n","Epoch: 645/5000, Train Loss: 33.17759288441051, Valid Loss: 33.40980784098307\n","Epoch: 646/5000, Train Loss: 33.00234014337713, Valid Loss: 33.37796401977539\n","Epoch: 647/5000, Train Loss: 32.74824818697843, Valid Loss: 33.30276616414388\n","Epoch: 648/5000, Train Loss: 32.870193828235976, Valid Loss: 33.34425036112467\n","Epoch: 649/5000, Train Loss: 33.066698421131484, Valid Loss: 33.33545811971029\n","Epoch: 650/5000, Train Loss: 33.107575850053266, Valid Loss: 33.33776346842448\n","Epoch: 651/5000, Train Loss: 32.9806320884011, Valid Loss: 33.28122011820475\n","Epoch: 652/5000, Train Loss: 32.80304631319913, Valid Loss: 33.30027643839518\n","Epoch: 653/5000, Train Loss: 32.91344139792702, Valid Loss: 33.26921780904134\n","Epoch: 654/5000, Train Loss: 32.69334532997825, Valid Loss: 33.31408437093099\n","Epoch: 655/5000, Train Loss: 32.95576893199574, Valid Loss: 33.265790939331055\n","Epoch: 656/5000, Train Loss: 33.05927085876465, Valid Loss: 33.2860361735026\n","Epoch: 657/5000, Train Loss: 32.916381489146836, Valid Loss: 33.25290552775065\n","Epoch: 658/5000, Train Loss: 33.481650439175695, Valid Loss: 33.242771784464516\n","Epoch: 659/5000, Train Loss: 32.99704707752574, Valid Loss: 33.23314539591471\n","Epoch: 660/5000, Train Loss: 32.791793303056195, Valid Loss: 33.240607579549156\n","Epoch: 661/5000, Train Loss: 32.864994222467594, Valid Loss: 33.22901280721029\n","Epoch: 662/5000, Train Loss: 33.001778862693094, Valid Loss: 33.19969940185547\n","Epoch: 663/5000, Train Loss: 33.242752595381305, Valid Loss: 33.16036605834961\n","Epoch: 664/5000, Train Loss: 32.83487285267223, Valid Loss: 33.19410705566406\n","Epoch: 665/5000, Train Loss: 32.97527798739347, Valid Loss: 33.11078898111979\n","Epoch: 666/5000, Train Loss: 32.90961265563965, Valid Loss: 33.12361526489258\n","Epoch: 667/5000, Train Loss: 33.317869359796696, Valid Loss: 33.18194897969564\n","Epoch: 668/5000, Train Loss: 32.8533900867809, Valid Loss: 33.135786056518555\n","Epoch: 669/5000, Train Loss: 32.7902809489857, Valid Loss: 33.12407430013021\n","Epoch: 670/5000, Train Loss: 32.9732912236994, Valid Loss: 33.101977030436196\n","Epoch: 671/5000, Train Loss: 32.848590504039414, Valid Loss: 33.127875645955406\n","Epoch: 672/5000, Train Loss: 32.698975649746984, Valid Loss: 33.10543314615885\n","Epoch: 673/5000, Train Loss: 32.834564902565695, Valid Loss: 33.07372919718424\n","Epoch: 674/5000, Train Loss: 33.05804651433771, Valid Loss: 33.09298133850098\n","Epoch: 675/5000, Train Loss: 32.854047081687234, Valid Loss: 33.08795992533366\n","Epoch: 676/5000, Train Loss: 32.92663556879217, Valid Loss: 33.09122530619303\n","Epoch: 677/5000, Train Loss: 32.905730507590555, Valid Loss: 33.14303652445475\n","Epoch: 678/5000, Train Loss: 32.95611381530762, Valid Loss: 33.09714444478353\n","Epoch: 679/5000, Train Loss: 32.72777054526589, Valid Loss: 33.107608795166016\n","Epoch: 680/5000, Train Loss: 32.8399904424494, Valid Loss: 33.085571924845375\n","Epoch: 681/5000, Train Loss: 32.716215480457656, Valid Loss: 33.047057469685875\n","Epoch: 682/5000, Train Loss: 32.75244747508656, Valid Loss: 33.03677622477213\n","Epoch: 683/5000, Train Loss: 32.67606648531827, Valid Loss: 33.08131090799967\n","Epoch: 684/5000, Train Loss: 33.05420684814453, Valid Loss: 33.02110036214193\n","Epoch: 685/5000, Train Loss: 32.49075352061879, Valid Loss: 33.02883084615072\n","Epoch: 686/5000, Train Loss: 32.758526368574664, Valid Loss: 33.012315114339195\n","Epoch: 687/5000, Train Loss: 32.954290043223985, Valid Loss: 33.03872934977213\n","Epoch: 688/5000, Train Loss: 32.80467362837358, Valid Loss: 33.00800704956055\n","Epoch: 689/5000, Train Loss: 32.80613049593839, Valid Loss: 32.981236139933266\n","Epoch: 690/5000, Train Loss: 32.76181151650169, Valid Loss: 32.98403358459473\n","Epoch: 691/5000, Train Loss: 32.7167309847745, Valid Loss: 32.94420496622721\n","Epoch: 692/5000, Train Loss: 32.441117546775125, Valid Loss: 32.9712823232015\n","Epoch: 693/5000, Train Loss: 32.61514629017223, Valid Loss: 32.96709632873535\n","Epoch: 694/5000, Train Loss: 32.69711338390004, Valid Loss: 32.974724451700844\n","Epoch: 695/5000, Train Loss: 32.80786046114835, Valid Loss: 33.00004577636719\n","Epoch: 696/5000, Train Loss: 32.5218001278964, Valid Loss: 32.982727686564125\n","Epoch: 697/5000, Train Loss: 32.669321060180664, Valid Loss: 32.97502072652181\n","Epoch: 698/5000, Train Loss: 32.95664561878551, Valid Loss: 32.94611104329427\n","Epoch: 699/5000, Train Loss: 32.78513873707164, Valid Loss: 32.95654296875\n","Epoch: 700/5000, Train Loss: 32.19084566289728, Valid Loss: 32.9080499013265\n","Epoch: 701/5000, Train Loss: 32.40934025157582, Valid Loss: 32.91072527567545\n","Epoch: 702/5000, Train Loss: 32.84049693020907, Valid Loss: 32.87593205769857\n","Epoch: 703/5000, Train Loss: 32.616378437389024, Valid Loss: 32.91816838582357\n","Epoch: 704/5000, Train Loss: 32.72501824118874, Valid Loss: 32.88154602050781\n","Epoch: 705/5000, Train Loss: 32.43345988880504, Valid Loss: 32.901149113972984\n","Epoch: 706/5000, Train Loss: 32.57545627247203, Valid Loss: 32.9284299214681\n","Epoch: 707/5000, Train Loss: 32.75928792086515, Valid Loss: 32.94526290893555\n","Epoch: 708/5000, Train Loss: 32.970721678300336, Valid Loss: 32.92855962117513\n","Epoch: 709/5000, Train Loss: 32.53369435397062, Valid Loss: 32.905726750691734\n","Epoch: 710/5000, Train Loss: 32.44344867359508, Valid Loss: 32.88102467854818\n","Epoch: 711/5000, Train Loss: 32.59196818958629, Valid Loss: 32.87039057413737\n","Epoch: 712/5000, Train Loss: 32.63008065657182, Valid Loss: 32.86697959899902\n","Epoch: 713/5000, Train Loss: 32.69003122503107, Valid Loss: 32.8469607035319\n","Epoch: 714/5000, Train Loss: 32.763425306840375, Valid Loss: 32.85158920288086\n","Epoch: 715/5000, Train Loss: 32.856148979880594, Valid Loss: 32.87252680460612\n","Epoch: 716/5000, Train Loss: 32.46818715875799, Valid Loss: 32.82791074117025\n","Epoch: 717/5000, Train Loss: 32.75604178688743, Valid Loss: 32.873359044392906\n","Epoch: 718/5000, Train Loss: 32.56768798828125, Valid Loss: 32.86558469136556\n","Epoch: 719/5000, Train Loss: 32.48923267017711, Valid Loss: 32.87328656514486\n","Epoch: 720/5000, Train Loss: 32.42801770296964, Valid Loss: 32.84467124938965\n","Epoch: 721/5000, Train Loss: 32.13407655195756, Valid Loss: 32.861000061035156\n","Epoch: 722/5000, Train Loss: 32.46664133938876, Valid Loss: 32.8235232035319\n","Epoch: 723/5000, Train Loss: 32.72646661238237, Valid Loss: 32.807926177978516\n","Epoch: 724/5000, Train Loss: 32.53295603665438, Valid Loss: 32.842702865600586\n","Epoch: 725/5000, Train Loss: 32.63015469637784, Valid Loss: 32.83309364318848\n","Epoch: 726/5000, Train Loss: 32.604189439253375, Valid Loss: 32.85907236735026\n","Epoch: 727/5000, Train Loss: 32.67039905894887, Valid Loss: 32.783365885416664\n","Epoch: 728/5000, Train Loss: 32.82053219188344, Valid Loss: 32.850074768066406\n","Epoch: 729/5000, Train Loss: 32.505145853216, Valid Loss: 32.847683588663735\n","Epoch: 730/5000, Train Loss: 32.55794975974343, Valid Loss: 32.86442629496256\n","Epoch: 731/5000, Train Loss: 32.60978629372337, Valid Loss: 32.84069697062174\n","Epoch: 732/5000, Train Loss: 32.486526836048476, Valid Loss: 32.83191998799642\n","Epoch: 733/5000, Train Loss: 32.40947792746804, Valid Loss: 32.832574208577476\n","Epoch: 734/5000, Train Loss: 32.96126365661621, Valid Loss: 32.82326062520345\n","Epoch: 735/5000, Train Loss: 32.32470009543679, Valid Loss: 32.78998247782389\n","Epoch: 736/5000, Train Loss: 32.25238591974432, Valid Loss: 32.77923838297526\n","Epoch: 737/5000, Train Loss: 32.55871096524325, Valid Loss: 32.75196075439453\n","Epoch: 738/5000, Train Loss: 32.39156497608531, Valid Loss: 32.754638036092125\n","Epoch: 739/5000, Train Loss: 32.16397337480025, Valid Loss: 32.750998179117836\n","Epoch: 740/5000, Train Loss: 32.41246847672896, Valid Loss: 32.74336369832357\n","Epoch: 741/5000, Train Loss: 32.71107430891557, Valid Loss: 32.75922648111979\n","Epoch: 742/5000, Train Loss: 32.50236753983931, Valid Loss: 32.78130976359049\n","Epoch: 743/5000, Train Loss: 32.4443192915483, Valid Loss: 32.77419662475586\n","Epoch: 744/5000, Train Loss: 32.51098372719505, Valid Loss: 32.791741053263344\n","Epoch: 745/5000, Train Loss: 32.45254204489968, Valid Loss: 32.75607490539551\n","Epoch: 746/5000, Train Loss: 32.5008676702326, Valid Loss: 32.80037625630697\n","Epoch: 747/5000, Train Loss: 32.589952815662734, Valid Loss: 32.778781255086265\n","Epoch: 748/5000, Train Loss: 32.21172731572931, Valid Loss: 32.75010108947754\n","Epoch: 749/5000, Train Loss: 32.541810122403234, Valid Loss: 32.741966247558594\n","Epoch: 750/5000, Train Loss: 32.70237783952193, Valid Loss: 32.72065862019857\n","Epoch: 751/5000, Train Loss: 32.56902885437012, Valid Loss: 32.69848505655924\n","Epoch: 752/5000, Train Loss: 32.56048566644842, Valid Loss: 32.709601084391274\n","Epoch: 753/5000, Train Loss: 32.675155292857774, Valid Loss: 32.688097635904946\n","Epoch: 754/5000, Train Loss: 32.401893789117985, Valid Loss: 32.73057619730631\n","Epoch: 755/5000, Train Loss: 32.57931171764027, Valid Loss: 32.68459955851237\n","Epoch: 756/5000, Train Loss: 32.44921268116344, Valid Loss: 32.66125170389811\n","Epoch: 757/5000, Train Loss: 32.76071149652655, Valid Loss: 32.665414810180664\n","Epoch: 758/5000, Train Loss: 32.93734845248136, Valid Loss: 32.685078938802086\n","Epoch: 759/5000, Train Loss: 32.34330662814054, Valid Loss: 32.75362586975098\n","Epoch: 760/5000, Train Loss: 32.35622978210449, Valid Loss: 32.712921142578125\n","Epoch: 761/5000, Train Loss: 32.612285787409, Valid Loss: 32.6806214650472\n","Epoch: 762/5000, Train Loss: 32.251199375499375, Valid Loss: 32.70957565307617\n","Epoch: 763/5000, Train Loss: 32.275394439697266, Valid Loss: 32.71205838521322\n","Epoch: 764/5000, Train Loss: 32.55010362104936, Valid Loss: 32.68728065490723\n","Epoch: 765/5000, Train Loss: 32.21652828563344, Valid Loss: 32.655832290649414\n","Epoch: 766/5000, Train Loss: 32.465144937688656, Valid Loss: 32.6550490061442\n","Epoch: 767/5000, Train Loss: 32.21284779635343, Valid Loss: 32.6653855641683\n","Epoch: 768/5000, Train Loss: 32.37059281089089, Valid Loss: 32.66242535909017\n","Epoch: 769/5000, Train Loss: 32.46051649613814, Valid Loss: 32.6979185740153\n","Epoch: 770/5000, Train Loss: 32.22253903475675, Valid Loss: 32.690938313802086\n","Epoch: 771/5000, Train Loss: 32.07784930142489, Valid Loss: 32.69666290283203\n","Epoch: 772/5000, Train Loss: 32.60327772660689, Valid Loss: 32.70397122701009\n","Epoch: 773/5000, Train Loss: 32.678469917990945, Valid Loss: 32.69166946411133\n","Epoch: 774/5000, Train Loss: 31.966537822376598, Valid Loss: 32.67031796773275\n","Epoch: 775/5000, Train Loss: 32.55460253628817, Valid Loss: 32.65500259399414\n","Epoch: 776/5000, Train Loss: 32.508743632923476, Valid Loss: 32.7099412282308\n","Epoch: 777/5000, Train Loss: 32.215274984186344, Valid Loss: 32.700018564860024\n","Epoch: 778/5000, Train Loss: 32.275701002641156, Valid Loss: 32.68832143147787\n","Epoch: 779/5000, Train Loss: 32.58083742315119, Valid Loss: 32.67988522847494\n","Epoch: 780/5000, Train Loss: 32.309922131625086, Valid Loss: 32.6800651550293\n","Epoch: 781/5000, Train Loss: 32.47200653769753, Valid Loss: 32.66212463378906\n","Epoch: 782/5000, Train Loss: 32.34861807389693, Valid Loss: 32.6557191212972\n","Epoch: 783/5000, Train Loss: 32.247476230968125, Valid Loss: 32.68458493550619\n","Epoch: 784/5000, Train Loss: 32.18536793101918, Valid Loss: 32.653814951578774\n","Epoch: 785/5000, Train Loss: 32.26374071294611, Valid Loss: 32.653419494628906\n","Epoch: 786/5000, Train Loss: 32.521983580155805, Valid Loss: 32.65465863545736\n","Epoch: 787/5000, Train Loss: 32.06094238974831, Valid Loss: 32.6655470530192\n","Epoch: 788/5000, Train Loss: 32.55944685502486, Valid Loss: 32.6810105641683\n","Epoch: 789/5000, Train Loss: 32.72923590920188, Valid Loss: 32.66775894165039\n","Epoch: 790/5000, Train Loss: 32.21788371693004, Valid Loss: 32.661430994669594\n","Epoch: 791/5000, Train Loss: 31.991287751631305, Valid Loss: 32.657030741373696\n","Epoch: 792/5000, Train Loss: 32.55856219204989, Valid Loss: 32.647087732950844\n","Epoch: 793/5000, Train Loss: 32.27397363836115, Valid Loss: 32.67260551452637\n","Epoch: 794/5000, Train Loss: 32.21485068581321, Valid Loss: 32.67029698689779\n","Epoch: 795/5000, Train Loss: 32.534366607666016, Valid Loss: 32.650328954060875\n","Epoch: 796/5000, Train Loss: 32.1400501944802, Valid Loss: 32.65902773539225\n","Epoch: 797/5000, Train Loss: 32.447261810302734, Valid Loss: 32.65380414326986\n","Epoch: 798/5000, Train Loss: 32.06613818081942, Valid Loss: 32.65701230367025\n","Epoch: 799/5000, Train Loss: 32.336030439897016, Valid Loss: 32.638633728027344\n","Epoch: 800/5000, Train Loss: 32.69002550298517, Valid Loss: 32.64344596862793\n","Epoch: 801/5000, Train Loss: 32.41372108459473, Valid Loss: 32.612941106160484\n","Epoch: 802/5000, Train Loss: 32.17534550753507, Valid Loss: 32.6291389465332\n","Epoch: 803/5000, Train Loss: 32.21787036548961, Valid Loss: 32.65771484375\n","Epoch: 804/5000, Train Loss: 32.40536204251376, Valid Loss: 32.62879880269369\n","Epoch: 805/5000, Train Loss: 32.87770097905939, Valid Loss: 32.67323684692383\n","Epoch: 806/5000, Train Loss: 32.297218496149235, Valid Loss: 32.634023666381836\n","Epoch: 807/5000, Train Loss: 32.56236579201438, Valid Loss: 32.624314626057945\n","Epoch: 808/5000, Train Loss: 32.30487806146795, Valid Loss: 32.641218185424805\n","Epoch: 809/5000, Train Loss: 32.165706461126156, Valid Loss: 32.680803298950195\n","Epoch: 810/5000, Train Loss: 32.48954218084162, Valid Loss: 32.64831797281901\n","Epoch: 811/5000, Train Loss: 32.08596819097345, Valid Loss: 32.63257153828939\n","Epoch: 812/5000, Train Loss: 32.510986674915664, Valid Loss: 32.631049474080406\n","Epoch: 813/5000, Train Loss: 32.201702638105914, Valid Loss: 32.626661936442055\n","Epoch: 814/5000, Train Loss: 32.39259026267312, Valid Loss: 32.67335192362467\n","Epoch: 815/5000, Train Loss: 32.28806200894442, Valid Loss: 32.62416203816732\n","Epoch: 816/5000, Train Loss: 32.223266254771836, Valid Loss: 32.635890324910484\n","Epoch: 817/5000, Train Loss: 32.28471617265181, Valid Loss: 32.608577728271484\n","Epoch: 818/5000, Train Loss: 32.4008112820712, Valid Loss: 32.62433751424154\n","Epoch: 819/5000, Train Loss: 32.02395820617676, Valid Loss: 32.61965751647949\n","Epoch: 820/5000, Train Loss: 32.053143587979406, Valid Loss: 32.60669263203939\n","Epoch: 821/5000, Train Loss: 32.02851451526988, Valid Loss: 32.58289019266764\n","Epoch: 822/5000, Train Loss: 32.18291161277077, Valid Loss: 32.630330403645836\n","Epoch: 823/5000, Train Loss: 31.835744337602094, Valid Loss: 32.62571779886881\n","Epoch: 824/5000, Train Loss: 32.09605581110174, Valid Loss: 32.62013943990072\n","Epoch: 825/5000, Train Loss: 32.22430558638139, Valid Loss: 32.63186264038086\n","Epoch: 826/5000, Train Loss: 32.63963942094283, Valid Loss: 32.62973848978678\n","Epoch: 827/5000, Train Loss: 32.39077411998402, Valid Loss: 32.64712142944336\n","Epoch: 828/5000, Train Loss: 32.1077846180309, Valid Loss: 32.63714726765951\n","Epoch: 829/5000, Train Loss: 32.30072593688965, Valid Loss: 32.64808654785156\n","Epoch: 830/5000, Train Loss: 32.365177327936344, Valid Loss: 32.620784759521484\n","Epoch: 831/5000, Train Loss: 32.25190179998224, Valid Loss: 32.61071650187174\n","Epoch: 832/5000, Train Loss: 32.29769307916815, Valid Loss: 32.589524586995445\n","Epoch: 833/5000, Train Loss: 32.11857084794478, Valid Loss: 32.61481221516927\n","Epoch: 834/5000, Train Loss: 32.14573652094061, Valid Loss: 32.61289024353027\n","Epoch: 835/5000, Train Loss: 32.17761282487349, Valid Loss: 32.64936955769857\n","Epoch: 836/5000, Train Loss: 32.557503786954015, Valid Loss: 32.648380279541016\n","Epoch: 837/5000, Train Loss: 32.24225460399281, Valid Loss: 32.616242090861\n","Epoch: 838/5000, Train Loss: 32.184221267700195, Valid Loss: 32.63985252380371\n","Epoch: 839/5000, Train Loss: 32.3286436254328, Valid Loss: 32.63446935017904\n","Epoch: 840/5000, Train Loss: 32.1223780892112, Valid Loss: 32.60939407348633\n","Epoch: 841/5000, Train Loss: 32.00892292369496, Valid Loss: 32.603458404541016\n","Epoch: 842/5000, Train Loss: 32.27246769991788, Valid Loss: 32.58745574951172\n","Epoch: 843/5000, Train Loss: 32.09217002175071, Valid Loss: 32.63320668538412\n","Epoch: 844/5000, Train Loss: 32.42963409423828, Valid Loss: 32.63710149129232\n","Epoch: 845/5000, Train Loss: 32.296283375133164, Valid Loss: 32.60501480102539\n","Epoch: 846/5000, Train Loss: 32.73698633367365, Valid Loss: 32.6175111134847\n","Epoch: 847/5000, Train Loss: 32.0485392483798, Valid Loss: 32.620055516560875\n","Epoch: 848/5000, Train Loss: 32.372101523659445, Valid Loss: 32.63911247253418\n","Epoch: 849/5000, Train Loss: 32.16964114796031, Valid Loss: 32.64541753133138\n","Epoch: 850/5000, Train Loss: 31.795394550670277, Valid Loss: 32.632012049357094\n","Epoch: 851/5000, Train Loss: 32.34558729691939, Valid Loss: 32.59575271606445\n","Epoch: 852/5000, Train Loss: 32.18216375871138, Valid Loss: 32.59979756673177\n","Epoch: 853/5000, Train Loss: 32.43666874278676, Valid Loss: 32.598398208618164\n","Epoch: 854/5000, Train Loss: 31.921387238935992, Valid Loss: 32.59811210632324\n","Epoch: 855/5000, Train Loss: 32.120149785822086, Valid Loss: 32.62843004862467\n","Epoch: 856/5000, Train Loss: 32.62686105207963, Valid Loss: 32.58761088053385\n","Epoch: 857/5000, Train Loss: 32.568926724520594, Valid Loss: 32.61881319681803\n","Epoch: 858/5000, Train Loss: 31.83191022005948, Valid Loss: 32.56380716959635\n","Epoch: 859/5000, Train Loss: 32.18956877968528, Valid Loss: 32.594998041788735\n","Epoch: 860/5000, Train Loss: 31.751845273104582, Valid Loss: 32.59822463989258\n","Epoch: 861/5000, Train Loss: 31.7563493902033, Valid Loss: 32.58863639831543\n","Epoch: 862/5000, Train Loss: 32.32460802251642, Valid Loss: 32.60001564025879\n","Epoch: 863/5000, Train Loss: 32.16695906899192, Valid Loss: 32.5932191212972\n","Epoch: 864/5000, Train Loss: 32.47185065529563, Valid Loss: 32.58775965372721\n","Epoch: 865/5000, Train Loss: 32.077178261496805, Valid Loss: 32.601020177205406\n","Epoch: 866/5000, Train Loss: 32.023524197665125, Valid Loss: 32.572638193766274\n","Epoch: 867/5000, Train Loss: 32.16899785128507, Valid Loss: 32.58171081542969\n","Epoch: 868/5000, Train Loss: 32.175327647816054, Valid Loss: 32.567450841267906\n","Epoch: 869/5000, Train Loss: 32.16509420221502, Valid Loss: 32.59295082092285\n","Epoch: 870/5000, Train Loss: 31.868030894886363, Valid Loss: 32.59248924255371\n","Epoch: 871/5000, Train Loss: 31.972935936667703, Valid Loss: 32.59541130065918\n","Epoch: 872/5000, Train Loss: 32.41140226884322, Valid Loss: 32.59766960144043\n","Epoch: 873/5000, Train Loss: 32.43543867631392, Valid Loss: 32.588626861572266\n","Epoch: 874/5000, Train Loss: 32.21646343577992, Valid Loss: 32.544712702433266\n","Epoch: 875/5000, Train Loss: 32.30169539018111, Valid Loss: 32.539707819620766\n","Epoch: 876/5000, Train Loss: 32.292891935868695, Valid Loss: 32.53270403544108\n","Epoch: 877/5000, Train Loss: 32.14327031915838, Valid Loss: 32.56451098124186\n","Epoch: 878/5000, Train Loss: 31.923329960216176, Valid Loss: 32.5621395111084\n","Epoch: 879/5000, Train Loss: 32.2967322956432, Valid Loss: 32.557820638020836\n","Epoch: 880/5000, Train Loss: 32.07325276461515, Valid Loss: 32.5767567952474\n","Epoch: 881/5000, Train Loss: 32.28132230585272, Valid Loss: 32.56621805826823\n","Epoch: 882/5000, Train Loss: 31.881175127896395, Valid Loss: 32.55886014302572\n","Epoch: 883/5000, Train Loss: 32.44832784479315, Valid Loss: 32.56714948018392\n","Epoch: 884/5000, Train Loss: 31.982580878517844, Valid Loss: 32.589368184407554\n","Epoch: 885/5000, Train Loss: 32.56018759987571, Valid Loss: 32.581658045450844\n","Epoch: 886/5000, Train Loss: 32.030926791104406, Valid Loss: 32.579468409220375\n","Epoch: 887/5000, Train Loss: 32.160273812033914, Valid Loss: 32.59938430786133\n","Epoch: 888/5000, Train Loss: 32.504285465587266, Valid Loss: 32.588635762532554\n","Epoch: 889/5000, Train Loss: 32.159866159612484, Valid Loss: 32.568385442097984\n","Epoch: 890/5000, Train Loss: 32.052983890880235, Valid Loss: 32.51890500386556\n","Epoch: 891/5000, Train Loss: 32.07211008938876, Valid Loss: 32.504459381103516\n","Epoch: 892/5000, Train Loss: 32.43936035849831, Valid Loss: 32.50668716430664\n","Epoch: 893/5000, Train Loss: 32.53899869051847, Valid Loss: 32.52141316731771\n","Epoch: 894/5000, Train Loss: 32.142874457619406, Valid Loss: 32.50508817036947\n","Epoch: 895/5000, Train Loss: 32.33926495638761, Valid Loss: 32.52587636311849\n","Epoch: 896/5000, Train Loss: 32.111938129771836, Valid Loss: 32.53318087259928\n","Epoch: 897/5000, Train Loss: 32.39005661010742, Valid Loss: 32.556182861328125\n","Epoch: 898/5000, Train Loss: 32.28730617869984, Valid Loss: 32.53005599975586\n","Epoch: 899/5000, Train Loss: 31.686786478215996, Valid Loss: 32.544310887654625\n","Epoch: 900/5000, Train Loss: 32.358877875588156, Valid Loss: 32.528425216674805\n","Epoch: 901/5000, Train Loss: 32.29565568403764, Valid Loss: 32.56934102376302\n","Epoch: 902/5000, Train Loss: 31.897728659889914, Valid Loss: 32.53002166748047\n","Epoch: 903/5000, Train Loss: 32.235179380937055, Valid Loss: 32.56212870279948\n","Epoch: 904/5000, Train Loss: 32.40502877668901, Valid Loss: 32.54342842102051\n","Epoch: 905/5000, Train Loss: 32.43826900828969, Valid Loss: 32.50256856282552\n","Epoch: 906/5000, Train Loss: 31.91483532298695, Valid Loss: 32.56822967529297\n","Epoch: 907/5000, Train Loss: 32.233716617931016, Valid Loss: 32.54991467793783\n","Epoch: 908/5000, Train Loss: 32.1690089485862, Valid Loss: 32.554365158081055\n","Epoch: 909/5000, Train Loss: 32.095929405905984, Valid Loss: 32.546609242757164\n","Epoch: 910/5000, Train Loss: 32.14115593650124, Valid Loss: 32.54712994893392\n","Epoch: 911/5000, Train Loss: 32.041756716641515, Valid Loss: 32.512800216674805\n","Epoch: 912/5000, Train Loss: 32.07081621343439, Valid Loss: 32.54646555582682\n","Epoch: 913/5000, Train Loss: 31.83681175925515, Valid Loss: 32.58443133036295\n","Epoch: 914/5000, Train Loss: 32.3808616291393, Valid Loss: 32.538159688313804\n","Epoch: 915/5000, Train Loss: 32.11751504377885, Valid Loss: 32.57317860921224\n","Epoch: 916/5000, Train Loss: 32.015411376953125, Valid Loss: 32.59822781880697\n","Epoch: 917/5000, Train Loss: 32.704453381625086, Valid Loss: 32.55417251586914\n","Epoch: 918/5000, Train Loss: 32.62951365384188, Valid Loss: 32.596078872680664\n","Epoch: 919/5000, Train Loss: 32.28302695534446, Valid Loss: 32.57171885172526\n","Epoch: 920/5000, Train Loss: 32.15867510708895, Valid Loss: 32.56676355997721\n","Epoch: 921/5000, Train Loss: 32.32344141873446, Valid Loss: 32.5510196685791\n","Epoch: 922/5000, Train Loss: 31.693721077658914, Valid Loss: 32.56747182210287\n","Epoch: 923/5000, Train Loss: 32.113422567194156, Valid Loss: 32.524614334106445\n","Epoch: 924/5000, Train Loss: 32.07874315435236, Valid Loss: 32.54569753011068\n","Epoch: 925/5000, Train Loss: 32.31534680453214, Valid Loss: 32.56098874409994\n","Epoch: 926/5000, Train Loss: 32.19172425703569, Valid Loss: 32.55889765421549\n","Epoch: 927/5000, Train Loss: 32.47004526311701, Valid Loss: 32.56212552388509\n","Epoch: 928/5000, Train Loss: 32.05746113170277, Valid Loss: 32.61534436543783\n","Epoch: 929/5000, Train Loss: 31.999189376831055, Valid Loss: 32.63843599955241\n","Epoch: 930/5000, Train Loss: 32.062553058971055, Valid Loss: 32.5929012298584\n","Epoch: 931/5000, Train Loss: 32.15362496809526, Valid Loss: 32.598082860310875\n","Epoch: 932/5000, Train Loss: 32.32617603648793, Valid Loss: 32.61338424682617\n","Epoch: 933/5000, Train Loss: 31.95027732849121, Valid Loss: 32.59899457295736\n","Epoch: 934/5000, Train Loss: 32.19638460332697, Valid Loss: 32.57308260599772\n","Epoch: 935/5000, Train Loss: 32.02992820739746, Valid Loss: 32.60281944274902\n","Epoch: 936/5000, Train Loss: 31.93523563038219, Valid Loss: 32.574727376302086\n","Epoch: 937/5000, Train Loss: 32.26689824191007, Valid Loss: 32.57382837931315\n","Epoch: 938/5000, Train Loss: 32.357913970947266, Valid Loss: 32.575316747029625\n","Epoch: 939/5000, Train Loss: 32.064967068758875, Valid Loss: 32.592734018961586\n","Epoch: 940/5000, Train Loss: 31.88512490012429, Valid Loss: 32.58233324686686\n","Epoch: 941/5000, Train Loss: 32.114206834272906, Valid Loss: 32.55730756123861\n","Epoch: 942/5000, Train Loss: 31.82008188421076, Valid Loss: 32.558064778645836\n","Epoch: 943/5000, Train Loss: 32.209966312755235, Valid Loss: 32.549655278523765\n","Epoch: 944/5000, Train Loss: 31.923505089499734, Valid Loss: 32.5829283396403\n","Epoch: 945/5000, Train Loss: 32.15313651344993, Valid Loss: 32.56781323750814\n","Epoch: 946/5000, Train Loss: 32.23595896634188, Valid Loss: 32.57466061909994\n","Epoch: 947/5000, Train Loss: 32.18343769420277, Valid Loss: 32.5320192972819\n","Epoch: 948/5000, Train Loss: 32.182652560147375, Valid Loss: 32.51792017618815\n","Epoch: 949/5000, Train Loss: 32.238976045088336, Valid Loss: 32.58283551534017\n","Epoch: 950/5000, Train Loss: 31.9437800320712, Valid Loss: 32.573676427205406\n","Epoch: 951/5000, Train Loss: 32.31219534440474, Valid Loss: 32.56023979187012\n","Epoch: 952/5000, Train Loss: 32.174861907958984, Valid Loss: 32.555589040120445\n","Epoch: 953/5000, Train Loss: 32.137412157925695, Valid Loss: 32.59698359171549\n","Epoch: 954/5000, Train Loss: 32.39659292047674, Valid Loss: 32.6002254486084\n","Epoch: 955/5000, Train Loss: 32.436730818314985, Valid Loss: 32.55112393697103\n","Epoch: 956/5000, Train Loss: 31.781803304498847, Valid Loss: 32.54809315999349\n","Epoch: 957/5000, Train Loss: 31.948284322565254, Valid Loss: 32.54151217142741\n","Epoch: 958/5000, Train Loss: 32.01969129388983, Valid Loss: 32.53856341044108\n","Epoch: 959/5000, Train Loss: 32.422825206409804, Valid Loss: 32.55923271179199\n","Epoch: 960/5000, Train Loss: 32.29568256031383, Valid Loss: 32.55669975280762\n","Epoch: 961/5000, Train Loss: 32.10984490134499, Valid Loss: 32.567806243896484\n","Epoch: 962/5000, Train Loss: 32.10887041958895, Valid Loss: 32.54957580566406\n","Epoch: 963/5000, Train Loss: 31.929346778176047, Valid Loss: 32.57752927144369\n","Epoch: 964/5000, Train Loss: 32.35760567405007, Valid Loss: 32.53298123677572\n","Epoch: 965/5000, Train Loss: 32.39020902460272, Valid Loss: 32.54898007710775\n","Epoch: 966/5000, Train Loss: 32.02766834605824, Valid Loss: 32.51753298441569\n","Epoch: 967/5000, Train Loss: 32.003409125588156, Valid Loss: 32.55943743387858\n","Epoch: 968/5000, Train Loss: 31.962879874489524, Valid Loss: 32.587706883748375\n","Epoch: 969/5000, Train Loss: 32.21696246754039, Valid Loss: 32.60569763183594\n","Epoch: 970/5000, Train Loss: 32.10123408924449, Valid Loss: 32.61749458312988\n","Epoch: 971/5000, Train Loss: 32.301660711115055, Valid Loss: 32.596204121907554\n","Epoch: 972/5000, Train Loss: 32.44793285023082, Valid Loss: 32.58080736796061\n","Epoch: 973/5000, Train Loss: 32.06385612487793, Valid Loss: 32.5824171702067\n","Epoch: 974/5000, Train Loss: 32.2689512426203, Valid Loss: 32.58556938171387\n","Epoch: 975/5000, Train Loss: 31.790299675681375, Valid Loss: 32.60884920756022\n","Epoch: 976/5000, Train Loss: 32.066208059137516, Valid Loss: 32.552821477254234\n","Epoch: 977/5000, Train Loss: 32.168432929299094, Valid Loss: 32.60161844889323\n","Epoch: 978/5000, Train Loss: 32.32802685824308, Valid Loss: 32.59400622049967\n","Epoch: 979/5000, Train Loss: 31.887759121981535, Valid Loss: 32.566676457722984\n","Epoch: 980/5000, Train Loss: 32.087159937078304, Valid Loss: 32.5830078125\n","Epoch: 981/5000, Train Loss: 32.04393421519887, Valid Loss: 32.56749407450358\n","Epoch: 982/5000, Train Loss: 31.764379674738105, Valid Loss: 32.56942240397135\n","Epoch: 983/5000, Train Loss: 31.765877983786844, Valid Loss: 32.55510584513346\n","Epoch: 984/5000, Train Loss: 32.21994382684881, Valid Loss: 32.5740966796875\n","Epoch: 985/5000, Train Loss: 31.78289257396351, Valid Loss: 32.58663431803385\n","Epoch: 986/5000, Train Loss: 32.24287934736772, Valid Loss: 32.63383674621582\n","Epoch: 987/5000, Train Loss: 32.098433234474875, Valid Loss: 32.60539118448893\n","Epoch: 988/5000, Train Loss: 32.44623305580833, Valid Loss: 32.60362688700358\n","Epoch: 989/5000, Train Loss: 31.79846226085316, Valid Loss: 32.58467165629069\n","Epoch: 990/5000, Train Loss: 32.00640314275568, Valid Loss: 32.57239214579264\n","Epoch: 991/5000, Train Loss: 31.652352246371183, Valid Loss: 32.59833780924479\n","Epoch: 992/5000, Train Loss: 32.011665691028945, Valid Loss: 32.60233243306478\n","Epoch: 993/5000, Train Loss: 32.02232187444513, Valid Loss: 32.59150950113932\n","Epoch: 994/5000, Train Loss: 31.88682608170943, Valid Loss: 32.58944320678711\n","Epoch: 995/5000, Train Loss: 32.14895907315341, Valid Loss: 32.57926368713379\n","Epoch: 996/5000, Train Loss: 31.983800541270863, Valid Loss: 32.5952574412028\n","Epoch: 997/5000, Train Loss: 31.783316005359996, Valid Loss: 32.59975051879883\n","Epoch: 998/5000, Train Loss: 31.935881181196734, Valid Loss: 32.594360987345375\n","Epoch: 999/5000, Train Loss: 32.03251491893422, Valid Loss: 32.5651003519694\n","Epoch: 1000/5000, Train Loss: 32.498138427734375, Valid Loss: 32.53310330708822\n","Epoch: 1001/5000, Train Loss: 31.775001872669566, Valid Loss: 32.54729652404785\n","Epoch: 1002/5000, Train Loss: 31.950369228016246, Valid Loss: 32.572348276774086\n","Epoch: 1003/5000, Train Loss: 32.02751350402832, Valid Loss: 32.56919161478678\n","Epoch: 1004/5000, Train Loss: 32.078453063964844, Valid Loss: 32.594326655069985\n","Epoch: 1005/5000, Train Loss: 32.33998107910156, Valid Loss: 32.57290077209473\n","얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 1005에서 훈련 중단.\n"]}],"source":["print(\"Training Start: MLM\")\n","featuresModel_MLM = train(train_MLM_loader, valid_MLM_loader, featuresModel_MLM, criterion, optimizer_MLM, epochs=CFG['EPOCHS'])\n","\n","print(\"Training Start: HLM\")\n","featuresModel_HLM = train(train_HLM_loader, valid_HLM_loader, featuresModel_HLM, criterion, optimizer_HLM, epochs=CFG['EPOCHS'])"]},{"cell_type":"code","execution_count":20,"id":"0fa8976d","metadata":{"id":"0fa8976d","executionInfo":{"status":"ok","timestamp":1693229547994,"user_tz":-540,"elapsed":915,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["torch.save(featuresModel_MLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_MLM_2.pth')  # 모델 객체의 state_dict 저장\n","torch.save(featuresModel_HLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_HLM_2.pth')"]},{"cell_type":"code","execution_count":21,"id":"1e894642","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":613,"status":"ok","timestamp":1693229666602,"user":{"displayName":"임송재","userId":"10220915962739075092"},"user_tz":-540},"id":"1e894642","outputId":"888548b7-899e-44a6-c12d-5d9a252fda24"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":21}],"source":["featuresModel_MLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_MLM_2.pth'))\n","featuresModel_HLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_HLM_2.pth'))"]},{"cell_type":"code","execution_count":22,"id":"f8054263","metadata":{"id":"f8054263","executionInfo":{"status":"ok","timestamp":1693229667163,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["test_MLM = NewFeatureDataset(test, target_col=None, transform=transform, is_test=True)\n","test_HLM = NewFeatureDataset(test, target_col=None, transform=transform, is_test=True)\n","\n","test_MLM_loader = DataLoader(dataset=test_MLM,\n","                             batch_size=CFG['BATCH_SIZE'],\n","                             shuffle=False)\n","\n","test_HLM_loader = DataLoader(dataset=test_HLM,\n","                             batch_size=CFG['BATCH_SIZE'],\n","                             shuffle=False)"]},{"cell_type":"code","execution_count":23,"id":"5c7701c7","metadata":{"id":"5c7701c7","executionInfo":{"status":"ok","timestamp":1693229667920,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["def inference(test_loader, model):\n","    model.eval()\n","    preds = []\n","\n","    with torch.no_grad():\n","        for inputs in test_loader:\n","            output = model(inputs)\n","            preds.extend(output.cpu().numpy().flatten().tolist())\n","\n","    return preds"]},{"cell_type":"code","execution_count":24,"id":"5c713f79","metadata":{"id":"5c713f79","executionInfo":{"status":"ok","timestamp":1693229667920,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["newFeature_predictions_MLM = inference(test_MLM_loader, featuresModel_MLM)\n","newFeature_predictions_HLM = inference(test_HLM_loader, featuresModel_HLM)\n","# newFeature 예측값들"]},{"cell_type":"code","source":["### 여기서부턴 Morgan_Model"],"metadata":{"id":"XBlqtT3Qhhaf","executionInfo":{"status":"ok","timestamp":1693229667920,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"id":"XBlqtT3Qhhaf","execution_count":25,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/train.csv')\n","test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/test.csv')"],"metadata":{"id":"1EnG51eC4VQF","executionInfo":{"status":"ok","timestamp":1693229668469,"user_tz":-540,"elapsed":2,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"id":"1EnG51eC4VQF","execution_count":26,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from rdkit import Chem, DataStructs\n","from rdkit.Chem import AllChem, Descriptors\n","\n","def calculate_metabolic_stability_descriptors(smiles):\n","    mol = Chem.MolFromSmiles(smiles)\n","    logP = Descriptors.MolLogP(mol)\n","    num_rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n","    num_heteroatoms = Descriptors.NumHeteroatoms(mol)\n","    num_hydrogen_acceptors = Descriptors.NumHAcceptors(mol)\n","    num_hydrogen_donors = Descriptors.NumHDonors(mol)\n","    morgan_fingerprint = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n","    morgan_array = np.zeros((1,), dtype=np.int8)\n","    DataStructs.ConvertToNumpyArray(morgan_fingerprint, morgan_array)\n","    aromatic_rings = mol.GetRingInfo().NumRings()\n","    tpsa = Descriptors.TPSA(mol)\n","\n","    return logP, num_rotatable_bonds, num_heteroatoms, num_hydrogen_acceptors, num_hydrogen_donors, morgan_array, aromatic_rings, tpsa\n","\n","train[[\n","    'logP', 'num_rotatable_bonds', 'num_heteroatoms',\n","    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint', 'aromatic_rings',\n","    'tpsa'\n","]] = train['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n","\n","test[[\n","    'logP', 'num_rotatable_bonds', 'num_heteroatoms',\n","    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint', 'aromatic_rings',\n","    'tpsa'\n","]] = test['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n","\n","train\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"rH2hNCKhhl9f","executionInfo":{"status":"ok","timestamp":1693229677480,"user_tz":-540,"elapsed":9012,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"75fc9d55-3e21-4a5d-81e0-b3e603045385"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["              id                                             SMILES     MLM  \\\n","0     TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n","1     TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n","2     TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n","3     TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n","4     TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n","...          ...                                                ...     ...   \n","3493  TRAIN_3493     Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl   1.556   \n","3494  TRAIN_3494  CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...  35.560   \n","3495  TRAIN_3495                       CCOC(=O)CCCc1nc2cc(N)ccc2n1C  56.150   \n","3496  TRAIN_3496                     Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl   0.030   \n","3497  TRAIN_3497                   COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1   0.450   \n","\n","         HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n","0     50.680  3.259           400.495                5             2   \n","1     50.590  2.169           301.407                2             1   \n","2     80.892  1.593           297.358                5             0   \n","3      2.000  4.771           494.652                6             0   \n","4     99.990  2.335           268.310                3             0   \n","...      ...    ...               ...              ...           ...   \n","3493   3.079  3.409           396.195                3             1   \n","3494  47.630  1.912           359.381                4             1   \n","3495   1.790  1.941           261.320                3             1   \n","3496   2.770  0.989           284.696                5             1   \n","3497   2.650  4.321           295.399                2             0   \n","\n","      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea     logP  \\\n","0                      8  3.259                      117.37  3.87744   \n","1                      2  2.172                       73.47  3.35474   \n","2                      3  1.585                       62.45  1.20450   \n","3                      5  3.475                       92.60  3.89356   \n","4                      1  2.337                       42.43  2.81772   \n","...                  ...    ...                         ...      ...   \n","3493                   5  3.409                       64.74  2.74730   \n","3494                   3  1.844                       77.37  2.27630   \n","3495                   6  2.124                       70.14  2.04130   \n","3496                   5  0.989                       91.51  1.42720   \n","3497                   4  4.321                       50.36  4.71792   \n","\n","      num_rotatable_bonds  num_heteroatoms  num_hydrogen_acceptors  \\\n","0                       8                8                       6   \n","1                       2                5                       4   \n","2                       3                7                       7   \n","3                       5                9                       7   \n","4                       1                4                       3   \n","...                   ...              ...                     ...   \n","3493                    4               11                       5   \n","3494                    3                7                       5   \n","3495                    5                5                       5   \n","3496                    4                7                       6   \n","3497                    4                3                       3   \n","\n","      num_hydrogen_donors                                 morgan_fingerprint  \\\n","0                       2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","1                       1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","2                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","3                       0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","4                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","...                   ...                                                ...   \n","3493                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","3494                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","3495                    1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","3496                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n","3497                    0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n","\n","      aromatic_rings   tpsa  \n","0                  3  89.13  \n","1                  3  45.23  \n","2                  4  62.45  \n","3                  5  84.22  \n","4                  3  42.43  \n","...              ...    ...  \n","3493               3  64.74  \n","3494               4  85.04  \n","3495               2  70.14  \n","3496               2  91.51  \n","3497               3  22.12  \n","\n","[3498 rows x 19 columns]"],"text/html":["\n","  <div id=\"df-4803d37e-4a86-4555-93af-502c6f561360\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>SMILES</th>\n","      <th>MLM</th>\n","      <th>HLM</th>\n","      <th>AlogP</th>\n","      <th>Molecular_Weight</th>\n","      <th>Num_H_Acceptors</th>\n","      <th>Num_H_Donors</th>\n","      <th>Num_RotatableBonds</th>\n","      <th>LogD</th>\n","      <th>Molecular_PolarSurfaceArea</th>\n","      <th>logP</th>\n","      <th>num_rotatable_bonds</th>\n","      <th>num_heteroatoms</th>\n","      <th>num_hydrogen_acceptors</th>\n","      <th>num_hydrogen_donors</th>\n","      <th>morgan_fingerprint</th>\n","      <th>aromatic_rings</th>\n","      <th>tpsa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TRAIN_0000</td>\n","      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n","      <td>26.010</td>\n","      <td>50.680</td>\n","      <td>3.259</td>\n","      <td>400.495</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>3.259</td>\n","      <td>117.37</td>\n","      <td>3.87744</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>6</td>\n","      <td>2</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>3</td>\n","      <td>89.13</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TRAIN_0001</td>\n","      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n","      <td>29.270</td>\n","      <td>50.590</td>\n","      <td>2.169</td>\n","      <td>301.407</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2.172</td>\n","      <td>73.47</td>\n","      <td>3.35474</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>3</td>\n","      <td>45.23</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TRAIN_0002</td>\n","      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n","      <td>5.586</td>\n","      <td>80.892</td>\n","      <td>1.593</td>\n","      <td>297.358</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1.585</td>\n","      <td>62.45</td>\n","      <td>1.20450</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>4</td>\n","      <td>62.45</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TRAIN_0003</td>\n","      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n","      <td>5.710</td>\n","      <td>2.000</td>\n","      <td>4.771</td>\n","      <td>494.652</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>3.475</td>\n","      <td>92.60</td>\n","      <td>3.89356</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>5</td>\n","      <td>84.22</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TRAIN_0004</td>\n","      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n","      <td>93.270</td>\n","      <td>99.990</td>\n","      <td>2.335</td>\n","      <td>268.310</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2.337</td>\n","      <td>42.43</td>\n","      <td>2.81772</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>3</td>\n","      <td>42.43</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3493</th>\n","      <td>TRAIN_3493</td>\n","      <td>Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl</td>\n","      <td>1.556</td>\n","      <td>3.079</td>\n","      <td>3.409</td>\n","      <td>396.195</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>3.409</td>\n","      <td>64.74</td>\n","      <td>2.74730</td>\n","      <td>4</td>\n","      <td>11</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>3</td>\n","      <td>64.74</td>\n","    </tr>\n","    <tr>\n","      <th>3494</th>\n","      <td>TRAIN_3494</td>\n","      <td>CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...</td>\n","      <td>35.560</td>\n","      <td>47.630</td>\n","      <td>1.912</td>\n","      <td>359.381</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1.844</td>\n","      <td>77.37</td>\n","      <td>2.27630</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>4</td>\n","      <td>85.04</td>\n","    </tr>\n","    <tr>\n","      <th>3495</th>\n","      <td>TRAIN_3495</td>\n","      <td>CCOC(=O)CCCc1nc2cc(N)ccc2n1C</td>\n","      <td>56.150</td>\n","      <td>1.790</td>\n","      <td>1.941</td>\n","      <td>261.320</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>2.124</td>\n","      <td>70.14</td>\n","      <td>2.04130</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>2</td>\n","      <td>70.14</td>\n","    </tr>\n","    <tr>\n","      <th>3496</th>\n","      <td>TRAIN_3496</td>\n","      <td>Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl</td>\n","      <td>0.030</td>\n","      <td>2.770</td>\n","      <td>0.989</td>\n","      <td>284.696</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>0.989</td>\n","      <td>91.51</td>\n","      <td>1.42720</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n","      <td>2</td>\n","      <td>91.51</td>\n","    </tr>\n","    <tr>\n","      <th>3497</th>\n","      <td>TRAIN_3497</td>\n","      <td>COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1</td>\n","      <td>0.450</td>\n","      <td>2.650</td>\n","      <td>4.321</td>\n","      <td>295.399</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4.321</td>\n","      <td>50.36</td>\n","      <td>4.71792</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n","      <td>3</td>\n","      <td>22.12</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3498 rows × 19 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4803d37e-4a86-4555-93af-502c6f561360')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4803d37e-4a86-4555-93af-502c6f561360 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4803d37e-4a86-4555-93af-502c6f561360');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-0d4e0fc2-b0fa-4cf2-a9ee-3e13c2f66c8a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0d4e0fc2-b0fa-4cf2-a9ee-3e13c2f66c8a')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-0d4e0fc2-b0fa-4cf2-a9ee-3e13c2f66c8a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":27}],"id":"rH2hNCKhhl9f"},{"cell_type":"code","source":["train['AlogP'].fillna(train['AlogP'].median(), inplace=True)\n","test['AlogP'].fillna(test['AlogP'].median(), inplace=True)"],"metadata":{"id":"vzonJnrwhmF2","executionInfo":{"status":"ok","timestamp":1693229677480,"user_tz":-540,"elapsed":7,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":28,"outputs":[],"id":"vzonJnrwhmF2"},{"cell_type":"code","source":["class MorganDataset(Dataset):\n","    def __init__(self, data, target_col=None, transform=None, is_test=False):\n","        self.is_test = is_test\n","        self.transform = transform\n","        self.is_test = is_test\n","\n","        if not self.is_test:\n","            self.data = self.transform.fit_transform(np.stack(data['morgan_fingerprint']))\n","        else: # test\n","            self.data = self.transform.transform(np.stack(data['morgan_fingerprint']))\n","\n","        if target_col is not None and not self.is_test:\n","            self.target = data[target_col]\n","\n","    def __getitem__(self, index):\n","        features = self.data[index]\n","\n","        if hasattr(self, 'target'):\n","            target = self.target[index]\n","            return torch.tensor(features).to(device).float(), torch.tensor(target).to(device).float().unsqueeze(dim=-1)\n","        else:\n","            return torch.tensor(features).to(device).float()\n","\n","    def __len__(self):\n","        return len(self.data)\n"],"metadata":{"id":"jP3vRcO_hmIw","executionInfo":{"status":"ok","timestamp":1693229677481,"user_tz":-540,"elapsed":7,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":29,"outputs":[],"id":"jP3vRcO_hmIw"},{"cell_type":"code","source":["transform = VarianceThreshold(threshold=0.05)\n","\n","train_MLM = MorganDataset(train, target_col='MLM', transform=transform, is_test=False)\n","train_HLM = MorganDataset(train, target_col='HLM', transform=transform, is_test=False)\n","\n","input_size = train_MLM.data.shape[1]\n","input_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iJkTGsp8hm88","executionInfo":{"status":"ok","timestamp":1693229678102,"user_tz":-540,"elapsed":627,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"9ad00539-7ea9-45ef-87c1-c4d066304425"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["251"]},"metadata":{},"execution_count":30}],"id":"iJkTGsp8hm88"},{"cell_type":"code","source":["train_HLM.data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee5HuI_Ohm_Q","executionInfo":{"status":"ok","timestamp":1693229678103,"user_tz":-540,"elapsed":6,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"9e3c46b7-fb42-4930-c424-65fafa8b13a4"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3498, 251)"]},"metadata":{},"execution_count":31}],"id":"ee5HuI_Ohm_Q"},{"cell_type":"code","source":["# train,valid split\n","train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42)\n","train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42)"],"metadata":{"id":"HWFJ4waNhnBQ","executionInfo":{"status":"ok","timestamp":1693229678564,"user_tz":-540,"elapsed":464,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":32,"outputs":[],"id":"HWFJ4waNhnBQ"},{"cell_type":"code","source":["torch.tensor(train_MLM.data[1]).shape, torch.tensor(train['MLM'][1]).float().unsqueeze(dim=-1).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0ftnFBzhnJH","executionInfo":{"status":"ok","timestamp":1693229678564,"user_tz":-540,"elapsed":8,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"edfb8211-0273-43cf-f997-a8f5adc1f265"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([251]), torch.Size([1]))"]},"metadata":{},"execution_count":33}],"id":"g0ftnFBzhnJH"},{"cell_type":"code","source":["# Hyperparameter\n","CFG = {'BATCH_SIZE': 256, # 200 과적합일 시에 낮추기\n","       'EPOCHS': 10000,\n","       'INPUT_SIZE': input_size,\n","       'HIDDEN_SIZE': 1024,\n","       'OUTPUT_SIZE': 1,\n","       'DROPOUT_RATE': 0.8, # 0.8 과적합일 시에 높이기\n","       'LEARNING_RATE': 1e-1}"],"metadata":{"id":"0jwJi04NhnKz","executionInfo":{"status":"ok","timestamp":1693229678564,"user_tz":-540,"elapsed":6,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":34,"outputs":[],"id":"0jwJi04NhnKz"},{"cell_type":"code","source":["train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=True)\n","\n","valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=False)\n","\n","\n","train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=True)\n","\n","valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n","                              batch_size=CFG['BATCH_SIZE'],\n","                              shuffle=False)"],"metadata":{"id":"f-XSz3AThnMH","executionInfo":{"status":"ok","timestamp":1693229678565,"user_tz":-540,"elapsed":7,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":35,"outputs":[],"id":"f-XSz3AThnMH"},{"cell_type":"code","source":["X_train, y_train = next(iter(train_MLM_loader))\n","print (X_train.shape, y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSHoOMoihnNg","executionInfo":{"status":"ok","timestamp":1693229678565,"user_tz":-540,"elapsed":7,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"a4d50eb9-1ae5-447b-9c68-d6397ced8324"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256, 251]) torch.Size([256, 1])\n"]}],"id":"HSHoOMoihnNg"},{"cell_type":"code","source":["class MorganModel(nn.Module): # 이게 제출한 전 코드임!!!!!\n","    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n","        super(MorganModel, self).__init__()\n","\n","        self.fc_layers = nn.Sequential(\n","            nn.Linear(input_size, hidden_size),\n","            nn.LayerNorm(hidden_size),\n","            nn.LeakyReLU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.LayerNorm(hidden_size),\n","            nn.LeakyReLU(),\n","            nn.Dropout(dropout_rate),\n","\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.LayerNorm(hidden_size),\n","            nn.LeakyReLU(),\n","            nn.Dropout(dropout_rate),\n","        )\n","\n","        self.fc_out = nn.Linear(hidden_size, out_size)\n","\n","    def forward(self, x):\n","        out = self.fc_layers(x)\n","        out = self.fc_out(out)\n","        return out"],"metadata":{"id":"LYUHQri2hnOy","executionInfo":{"status":"ok","timestamp":1693229678565,"user_tz":-540,"elapsed":5,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":37,"outputs":[],"id":"LYUHQri2hnOy"},{"cell_type":"code","source":["morganModel_MLM = MorganModel(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])\n","morganModel_HLM = MorganModel(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])"],"metadata":{"id":"uwSZJsuMhnQF","executionInfo":{"status":"ok","timestamp":1693229678565,"user_tz":-540,"elapsed":5,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":38,"outputs":[],"id":"uwSZJsuMhnQF"},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super(RMSELoss, self).__init__()\n","        self.mse = nn.MSELoss()  # 기존의 MSELoss 함수 사용\n","\n","    def forward(self, output, target):\n","        mse_loss = self.mse(output, target)  # 기존의 MSELoss를 계산\n","        rmse_loss = torch.sqrt(mse_loss)  # MSE에 제곱근 씌워 RMSE 계산\n","        return rmse_loss\n","\n","criterion = RMSELoss()\n","optimizer_MLM = torch.optim.Adam(morganModel_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n","optimizer_HLM = torch.optim.Adam(morganModel_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n"],"metadata":{"id":"bg3cSRiZhnRc","executionInfo":{"status":"ok","timestamp":1693229678566,"user_tz":-540,"elapsed":6,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":39,"outputs":[],"id":"bg3cSRiZhnRc"},{"cell_type":"code","source":["def train(train_loader, valid_loader, model, criterion, optimizer, epochs, patience=100):\n","    best_valid_loss = float('inf')\n","    no_improvement_count = 0\n","\n","    for epoch in range(epochs):\n","        model.train()  # 모델을 훈련 모드로 설정\n","        running_loss = 0\n","        for inputs, targets in train_loader:\n","            optimizer.zero_grad()\n","\n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        model.eval()  # 모델을 검증 모드로 설정\n","        valid_loss = 0\n","        with torch.no_grad():\n","          for inputs, targets in valid_loader:\n","            output = model(inputs)\n","            loss = criterion(output, targets)\n","            valid_loss += loss.item()\n","\n","        avg_train_loss = running_loss / len(train_loader)\n","        avg_valid_loss = valid_loss / len(valid_loader)\n","        print(f'Epoch: {epoch}/{epochs}, Train Loss: {avg_train_loss}, Valid Loss: {avg_valid_loss}')\n","\n","        if avg_valid_loss < best_valid_loss:\n","          best_valid_loss = avg_valid_loss\n","          no_improvement_count = 0\n","          best_model_state = model.state_dict()\n","        else:\n","          no_improvement_count += 1\n","          if no_improvement_count >= patience:\n","            print(f'얼리 스토핑: {patience} 에포크 동안 검증 손실이 향상되지 않음. 에포크 {epoch}에서 훈련 중단.')\n","            break\n","\n","    # 최적의 모델 상태 불러오기\n","    model.load_state_dict(best_model_state)\n","    return model"],"metadata":{"id":"Wk2cBX_phnSu","executionInfo":{"status":"ok","timestamp":1693229678566,"user_tz":-540,"elapsed":5,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":40,"outputs":[],"id":"Wk2cBX_phnSu"},{"cell_type":"code","source":["print(\"Training Start: MLM\")\n","morganModel_MLM = train(train_MLM_loader, valid_MLM_loader, morganModel_MLM, criterion, optimizer_MLM, epochs=CFG['EPOCHS'])\n","\n","print(\"Training Start: HLM\")\n","morganModel_HLM = train(train_HLM_loader, valid_HLM_loader, morganModel_HLM, criterion, optimizer_HLM, epochs=CFG['EPOCHS'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B61MVSEnhnUC","executionInfo":{"status":"ok","timestamp":1693229920977,"user_tz":-540,"elapsed":242416,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"428d64a6-1cbd-43ee-a1ad-e723c42e573e"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Start: MLM\n","Epoch: 0/10000, Train Loss: 39.60606869784269, Valid Loss: 37.37712605794271\n","Epoch: 1/10000, Train Loss: 36.50003780018199, Valid Loss: 36.68722788492838\n","Epoch: 2/10000, Train Loss: 36.00373979048295, Valid Loss: 36.6932258605957\n","Epoch: 3/10000, Train Loss: 36.29834816672585, Valid Loss: 36.67222340901693\n","Epoch: 4/10000, Train Loss: 35.14621908014471, Valid Loss: 36.27057902018229\n","Epoch: 5/10000, Train Loss: 34.39154260808771, Valid Loss: 34.22482426961263\n","Epoch: 6/10000, Train Loss: 33.61454287442294, Valid Loss: 33.68545150756836\n","Epoch: 7/10000, Train Loss: 33.178049087524414, Valid Loss: 34.550366719563804\n","Epoch: 8/10000, Train Loss: 33.39829913052645, Valid Loss: 36.13474909464518\n","Epoch: 9/10000, Train Loss: 32.78080662814054, Valid Loss: 34.476731618245445\n","Epoch: 10/10000, Train Loss: 33.1479400287975, Valid Loss: 34.55092748006185\n","Epoch: 11/10000, Train Loss: 32.54441937533292, Valid Loss: 34.28586196899414\n","Epoch: 12/10000, Train Loss: 31.836274233731356, Valid Loss: 34.01641972859701\n","Epoch: 13/10000, Train Loss: 31.997943878173828, Valid Loss: 34.152687072753906\n","Epoch: 14/10000, Train Loss: 31.244748028841887, Valid Loss: 33.78042984008789\n","Epoch: 15/10000, Train Loss: 31.395803624933418, Valid Loss: 34.181050618489586\n","Epoch: 16/10000, Train Loss: 31.542171131480824, Valid Loss: 34.79485321044922\n","Epoch: 17/10000, Train Loss: 31.19940220225941, Valid Loss: 34.47821807861328\n","Epoch: 18/10000, Train Loss: 30.63918703252619, Valid Loss: 34.85046513875326\n","Epoch: 19/10000, Train Loss: 30.72850868918679, Valid Loss: 33.93655649820963\n","Epoch: 20/10000, Train Loss: 31.000350778753106, Valid Loss: 34.1379648844401\n","Epoch: 21/10000, Train Loss: 30.647592024369672, Valid Loss: 34.574475606282554\n","Epoch: 22/10000, Train Loss: 30.228586717085406, Valid Loss: 35.03458023071289\n","Epoch: 23/10000, Train Loss: 30.238516374067828, Valid Loss: 34.59213892618815\n","Epoch: 24/10000, Train Loss: 29.949405323375355, Valid Loss: 35.318702697753906\n","Epoch: 25/10000, Train Loss: 30.12308623573997, Valid Loss: 34.63377380371094\n","Epoch: 26/10000, Train Loss: 29.71522938121449, Valid Loss: 34.94622802734375\n","Epoch: 27/10000, Train Loss: 29.596011421897195, Valid Loss: 34.78930155436198\n","Epoch: 28/10000, Train Loss: 30.05525120821866, Valid Loss: 35.23598098754883\n","Epoch: 29/10000, Train Loss: 29.492846402254973, Valid Loss: 34.87893422444662\n","Epoch: 30/10000, Train Loss: 29.054623690518465, Valid Loss: 35.63385772705078\n","Epoch: 31/10000, Train Loss: 28.895937486128375, Valid Loss: 34.819052378336586\n","Epoch: 32/10000, Train Loss: 29.545608693903144, Valid Loss: 35.4123420715332\n","Epoch: 33/10000, Train Loss: 28.460658160122957, Valid Loss: 34.987561543782554\n","Epoch: 34/10000, Train Loss: 29.16756716641513, Valid Loss: 34.56779861450195\n","Epoch: 35/10000, Train Loss: 28.56499533219771, Valid Loss: 35.351924896240234\n","Epoch: 36/10000, Train Loss: 28.470523314042524, Valid Loss: 35.11305363972982\n","Epoch: 37/10000, Train Loss: 28.023598410866477, Valid Loss: 35.56573740641276\n","Epoch: 38/10000, Train Loss: 28.374173771251332, Valid Loss: 35.19040552775065\n","Epoch: 39/10000, Train Loss: 28.00698540427468, Valid Loss: 34.83450063069662\n","Epoch: 40/10000, Train Loss: 27.79482650756836, Valid Loss: 35.25148137410482\n","Epoch: 41/10000, Train Loss: 27.905526247891512, Valid Loss: 35.56509272257487\n","Epoch: 42/10000, Train Loss: 27.91309946233576, Valid Loss: 35.036301930745445\n","Epoch: 43/10000, Train Loss: 28.02224384654652, Valid Loss: 35.1670290629069\n","Epoch: 44/10000, Train Loss: 27.81679933721369, Valid Loss: 35.56734720865885\n","Epoch: 45/10000, Train Loss: 27.822490692138672, Valid Loss: 35.222302754720054\n","Epoch: 46/10000, Train Loss: 27.85091278769753, Valid Loss: 35.5140012105306\n","Epoch: 47/10000, Train Loss: 27.638015747070312, Valid Loss: 34.894280751546226\n","Epoch: 48/10000, Train Loss: 27.56932241266424, Valid Loss: 35.61828867594401\n","Epoch: 49/10000, Train Loss: 27.032045364379883, Valid Loss: 36.08853658040365\n","Epoch: 50/10000, Train Loss: 27.048993717540394, Valid Loss: 36.41150665283203\n","Epoch: 51/10000, Train Loss: 26.98493905500932, Valid Loss: 35.55601247151693\n","Epoch: 52/10000, Train Loss: 26.565384257923473, Valid Loss: 35.09075164794922\n","Epoch: 53/10000, Train Loss: 26.71574540571733, Valid Loss: 35.653404235839844\n","Epoch: 54/10000, Train Loss: 27.04805547540838, Valid Loss: 35.262261708577476\n","Epoch: 55/10000, Train Loss: 26.852576689286664, Valid Loss: 35.89757283528646\n","Epoch: 56/10000, Train Loss: 26.578554673628375, Valid Loss: 35.40733846028646\n","Epoch: 57/10000, Train Loss: 26.43036512895064, Valid Loss: 35.64380009969076\n","Epoch: 58/10000, Train Loss: 26.235999367453836, Valid Loss: 35.52200698852539\n","Epoch: 59/10000, Train Loss: 26.529733484441582, Valid Loss: 36.05740229288737\n","Epoch: 60/10000, Train Loss: 26.767329302701082, Valid Loss: 36.16321309407552\n","Epoch: 61/10000, Train Loss: 25.541115847500887, Valid Loss: 36.0299924214681\n","Epoch: 62/10000, Train Loss: 25.9366793199019, Valid Loss: 36.56476084391276\n","Epoch: 63/10000, Train Loss: 26.01595514470881, Valid Loss: 35.72357432047526\n","Epoch: 64/10000, Train Loss: 25.939638658003375, Valid Loss: 35.6789919535319\n","Epoch: 65/10000, Train Loss: 25.615548047152433, Valid Loss: 36.45375315348307\n","Epoch: 66/10000, Train Loss: 25.65064672990279, Valid Loss: 35.892626444498696\n","Epoch: 67/10000, Train Loss: 25.94662128795277, Valid Loss: 35.81690470377604\n","Epoch: 68/10000, Train Loss: 25.13970253684304, Valid Loss: 36.367018381754555\n","Epoch: 69/10000, Train Loss: 25.106748407537285, Valid Loss: 36.153175354003906\n","Epoch: 70/10000, Train Loss: 25.46588776328347, Valid Loss: 36.214090983072914\n","Epoch: 71/10000, Train Loss: 25.092065811157227, Valid Loss: 36.16572189331055\n","Epoch: 72/10000, Train Loss: 24.84423134543679, Valid Loss: 36.50007629394531\n","Epoch: 73/10000, Train Loss: 25.014703924005683, Valid Loss: 36.65031178792318\n","Epoch: 74/10000, Train Loss: 25.03214454650879, Valid Loss: 35.41645431518555\n","Epoch: 75/10000, Train Loss: 25.195052927190606, Valid Loss: 35.601776123046875\n","Epoch: 76/10000, Train Loss: 24.49977527965199, Valid Loss: 36.68239212036133\n","Epoch: 77/10000, Train Loss: 25.04643336209384, Valid Loss: 36.17025883992513\n","Epoch: 78/10000, Train Loss: 25.092975443059746, Valid Loss: 36.066978454589844\n","Epoch: 79/10000, Train Loss: 24.417828819968484, Valid Loss: 36.69523366292318\n","Epoch: 80/10000, Train Loss: 24.346647609363902, Valid Loss: 36.432210286458336\n","Epoch: 81/10000, Train Loss: 24.182031631469727, Valid Loss: 36.5767567952474\n","Epoch: 82/10000, Train Loss: 23.953330993652344, Valid Loss: 36.92817306518555\n","Epoch: 83/10000, Train Loss: 24.49657578901811, Valid Loss: 36.91635767618815\n","Epoch: 84/10000, Train Loss: 24.413908524946734, Valid Loss: 36.62009302775065\n","Epoch: 85/10000, Train Loss: 24.591488057916816, Valid Loss: 36.14340591430664\n","Epoch: 86/10000, Train Loss: 24.259478308937766, Valid Loss: 36.31278991699219\n","Epoch: 87/10000, Train Loss: 24.83443104137074, Valid Loss: 36.64483515421549\n","Epoch: 88/10000, Train Loss: 24.266862869262695, Valid Loss: 36.338217417399086\n","Epoch: 89/10000, Train Loss: 23.47333925420588, Valid Loss: 36.59526824951172\n","Epoch: 90/10000, Train Loss: 24.207183837890625, Valid Loss: 36.24351501464844\n","Epoch: 91/10000, Train Loss: 24.09367561340332, Valid Loss: 36.76280975341797\n","Epoch: 92/10000, Train Loss: 24.005396582863547, Valid Loss: 36.42593256632487\n","Epoch: 93/10000, Train Loss: 23.93917603926225, Valid Loss: 35.97424189249674\n","Epoch: 94/10000, Train Loss: 23.721881693059746, Valid Loss: 36.67276255289713\n","Epoch: 95/10000, Train Loss: 23.717518199573863, Valid Loss: 36.36886723836263\n","Epoch: 96/10000, Train Loss: 23.312391107732598, Valid Loss: 36.585748036702476\n","Epoch: 97/10000, Train Loss: 23.531037764115766, Valid Loss: 36.25975799560547\n","Epoch: 98/10000, Train Loss: 24.371524984186347, Valid Loss: 36.475589752197266\n","Epoch: 99/10000, Train Loss: 23.586760434237394, Valid Loss: 35.98719914754232\n","Epoch: 100/10000, Train Loss: 22.816346428611062, Valid Loss: 36.82568359375\n","Epoch: 101/10000, Train Loss: 23.24111383611506, Valid Loss: 37.32014465332031\n","Epoch: 102/10000, Train Loss: 23.504146575927734, Valid Loss: 36.17310078938802\n","Epoch: 103/10000, Train Loss: 23.092328505082563, Valid Loss: 36.66315841674805\n","Epoch: 104/10000, Train Loss: 22.50652902776545, Valid Loss: 36.538771311442055\n","Epoch: 105/10000, Train Loss: 22.690027063543145, Valid Loss: 36.43649419148763\n","Epoch: 106/10000, Train Loss: 22.613456205888227, Valid Loss: 36.478624979654946\n","얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 106에서 훈련 중단.\n","Training Start: HLM\n","Epoch: 0/10000, Train Loss: 40.5802397294478, Valid Loss: 36.77040100097656\n","Epoch: 1/10000, Train Loss: 37.70854568481445, Valid Loss: 36.376608530680336\n","Epoch: 2/10000, Train Loss: 37.55617939342152, Valid Loss: 35.823402404785156\n","Epoch: 3/10000, Train Loss: 37.252991069446914, Valid Loss: 35.609055836995445\n","Epoch: 4/10000, Train Loss: 36.795863758433946, Valid Loss: 34.8497200012207\n","Epoch: 5/10000, Train Loss: 35.98309950395064, Valid Loss: 33.81472524007162\n","Epoch: 6/10000, Train Loss: 35.611141204833984, Valid Loss: 33.86689249674479\n","Epoch: 7/10000, Train Loss: 34.61227174238725, Valid Loss: 33.45368576049805\n","Epoch: 8/10000, Train Loss: 34.86442427201705, Valid Loss: 34.018758138020836\n","Epoch: 9/10000, Train Loss: 34.48422067815607, Valid Loss: 33.536869049072266\n","Epoch: 10/10000, Train Loss: 34.1249587319114, Valid Loss: 34.00328572591146\n","Epoch: 11/10000, Train Loss: 34.528474981134586, Valid Loss: 33.48714955647787\n","Epoch: 12/10000, Train Loss: 33.74152512983842, Valid Loss: 34.429274241129555\n","Epoch: 13/10000, Train Loss: 33.790842576460406, Valid Loss: 33.38237762451172\n","Epoch: 14/10000, Train Loss: 33.51964204961603, Valid Loss: 33.719469706217446\n","Epoch: 15/10000, Train Loss: 33.18889045715332, Valid Loss: 33.83241399129232\n","Epoch: 16/10000, Train Loss: 33.500670346346766, Valid Loss: 33.683615366617836\n","Epoch: 17/10000, Train Loss: 33.98163535378196, Valid Loss: 33.780354817708336\n","Epoch: 18/10000, Train Loss: 32.851933219216086, Valid Loss: 33.829185485839844\n","Epoch: 19/10000, Train Loss: 32.57975873080167, Valid Loss: 33.98253885904948\n","Epoch: 20/10000, Train Loss: 32.42229028181596, Valid Loss: 34.25434494018555\n","Epoch: 21/10000, Train Loss: 32.588876550847836, Valid Loss: 33.47316360473633\n","Epoch: 22/10000, Train Loss: 32.781444029374555, Valid Loss: 33.95512390136719\n","Epoch: 23/10000, Train Loss: 31.970183459195223, Valid Loss: 33.78271230061849\n","Epoch: 24/10000, Train Loss: 32.229786439375445, Valid Loss: 33.937033335367836\n","Epoch: 25/10000, Train Loss: 32.07264934886586, Valid Loss: 33.896888732910156\n","Epoch: 26/10000, Train Loss: 31.840212215076793, Valid Loss: 34.24269994099935\n","Epoch: 27/10000, Train Loss: 31.768050627274945, Valid Loss: 33.803513844807945\n","Epoch: 28/10000, Train Loss: 31.584616921164773, Valid Loss: 34.374829610188804\n","Epoch: 29/10000, Train Loss: 32.11623833396218, Valid Loss: 34.78767013549805\n","Epoch: 30/10000, Train Loss: 31.503503452647816, Valid Loss: 34.55479303995768\n","Epoch: 31/10000, Train Loss: 31.177973313765094, Valid Loss: 34.17495854695638\n","Epoch: 32/10000, Train Loss: 31.25013472817161, Valid Loss: 34.10181681315104\n","Epoch: 33/10000, Train Loss: 31.01865595037287, Valid Loss: 34.39369201660156\n","Epoch: 34/10000, Train Loss: 30.856814471158113, Valid Loss: 34.53565343221029\n","Epoch: 35/10000, Train Loss: 31.087548689408735, Valid Loss: 34.70119094848633\n","Epoch: 36/10000, Train Loss: 30.437009464610707, Valid Loss: 33.99085489908854\n","Epoch: 37/10000, Train Loss: 30.578459652987394, Valid Loss: 34.0437266031901\n","Epoch: 38/10000, Train Loss: 29.96726538918235, Valid Loss: 35.35920715332031\n","Epoch: 39/10000, Train Loss: 30.57270656932484, Valid Loss: 34.34044901529948\n","Epoch: 40/10000, Train Loss: 29.786046635020863, Valid Loss: 34.387261708577476\n","Epoch: 41/10000, Train Loss: 30.12651894309304, Valid Loss: 34.05766932169596\n","Epoch: 42/10000, Train Loss: 30.13604112104936, Valid Loss: 35.16564814249674\n","Epoch: 43/10000, Train Loss: 30.217212850397285, Valid Loss: 34.33930969238281\n","Epoch: 44/10000, Train Loss: 29.653393832120027, Valid Loss: 34.45355097452799\n","Epoch: 45/10000, Train Loss: 29.476774389093574, Valid Loss: 35.69188435872396\n","Epoch: 46/10000, Train Loss: 29.786236502907492, Valid Loss: 34.25088628133138\n","Epoch: 47/10000, Train Loss: 29.50116573680531, Valid Loss: 34.49501291910807\n","Epoch: 48/10000, Train Loss: 28.9058104428378, Valid Loss: 34.0808474222819\n","Epoch: 49/10000, Train Loss: 29.140532406893644, Valid Loss: 34.62253443400065\n","Epoch: 50/10000, Train Loss: 29.12922495061701, Valid Loss: 34.611403147379555\n","Epoch: 51/10000, Train Loss: 29.24344236200506, Valid Loss: 34.50674692789713\n","Epoch: 52/10000, Train Loss: 29.05657248063521, Valid Loss: 36.27022806803385\n","Epoch: 53/10000, Train Loss: 28.52157904885032, Valid Loss: 35.89816665649414\n","Epoch: 54/10000, Train Loss: 28.440526615489613, Valid Loss: 34.8577880859375\n","Epoch: 55/10000, Train Loss: 28.26516168767756, Valid Loss: 34.292555491129555\n","Epoch: 56/10000, Train Loss: 28.466473145918414, Valid Loss: 34.54168446858724\n","Epoch: 57/10000, Train Loss: 28.037879597056996, Valid Loss: 35.853782653808594\n","Epoch: 58/10000, Train Loss: 28.00288165699352, Valid Loss: 34.74300003051758\n","Epoch: 59/10000, Train Loss: 27.706063530661844, Valid Loss: 34.078198750813804\n","Epoch: 60/10000, Train Loss: 27.67058025706898, Valid Loss: 35.47726821899414\n","Epoch: 61/10000, Train Loss: 28.365168137983844, Valid Loss: 35.45056915283203\n","Epoch: 62/10000, Train Loss: 27.697676398537375, Valid Loss: 34.443311055501304\n","Epoch: 63/10000, Train Loss: 27.166731574318625, Valid Loss: 34.71877670288086\n","Epoch: 64/10000, Train Loss: 27.985957405783914, Valid Loss: 34.546688079833984\n","Epoch: 65/10000, Train Loss: 27.635183681141246, Valid Loss: 34.41562271118164\n","Epoch: 66/10000, Train Loss: 27.583086013793945, Valid Loss: 34.42239888509115\n","Epoch: 67/10000, Train Loss: 27.749700026078656, Valid Loss: 34.02317682902018\n","Epoch: 68/10000, Train Loss: 27.100365205244586, Valid Loss: 33.88120905558268\n","Epoch: 69/10000, Train Loss: 27.166621468283914, Valid Loss: 34.398886362711586\n","Epoch: 70/10000, Train Loss: 27.271837754683062, Valid Loss: 34.93230692545573\n","Epoch: 71/10000, Train Loss: 26.689610741355203, Valid Loss: 34.509934743245445\n","Epoch: 72/10000, Train Loss: 27.537879943847656, Valid Loss: 34.74174372355143\n","Epoch: 73/10000, Train Loss: 26.668639963323418, Valid Loss: 35.808067321777344\n","Epoch: 74/10000, Train Loss: 27.555817517367277, Valid Loss: 35.50359344482422\n","Epoch: 75/10000, Train Loss: 26.764917720447887, Valid Loss: 35.56677373250326\n","Epoch: 76/10000, Train Loss: 26.906491192904387, Valid Loss: 34.42296600341797\n","Epoch: 77/10000, Train Loss: 26.478637868707832, Valid Loss: 34.69452158610026\n","Epoch: 78/10000, Train Loss: 26.949804999611594, Valid Loss: 34.296138763427734\n","Epoch: 79/10000, Train Loss: 26.61798737265847, Valid Loss: 34.07285817464193\n","Epoch: 80/10000, Train Loss: 26.715342781760476, Valid Loss: 34.272298177083336\n","Epoch: 81/10000, Train Loss: 26.254247318614613, Valid Loss: 35.242104848225914\n","Epoch: 82/10000, Train Loss: 26.104449185458098, Valid Loss: 34.94364674886068\n","Epoch: 83/10000, Train Loss: 26.577496788718484, Valid Loss: 35.06927744547526\n","Epoch: 84/10000, Train Loss: 25.84882216020064, Valid Loss: 35.298834482828774\n","Epoch: 85/10000, Train Loss: 25.54116301103072, Valid Loss: 34.9093386332194\n","Epoch: 86/10000, Train Loss: 26.67076336253773, Valid Loss: 36.25987243652344\n","Epoch: 87/10000, Train Loss: 26.13987992026589, Valid Loss: 33.709004720052086\n","Epoch: 88/10000, Train Loss: 25.69102027199485, Valid Loss: 34.10581588745117\n","Epoch: 89/10000, Train Loss: 25.241032860495828, Valid Loss: 34.68078867594401\n","Epoch: 90/10000, Train Loss: 25.976423957131125, Valid Loss: 34.1758664449056\n","Epoch: 91/10000, Train Loss: 25.140380859375, Valid Loss: 35.048047383626304\n","Epoch: 92/10000, Train Loss: 25.668910460038617, Valid Loss: 34.54001235961914\n","Epoch: 93/10000, Train Loss: 25.482583479447797, Valid Loss: 35.497596740722656\n","Epoch: 94/10000, Train Loss: 25.43278000571511, Valid Loss: 35.208168029785156\n","Epoch: 95/10000, Train Loss: 25.833758787675336, Valid Loss: 35.48795954386393\n","Epoch: 96/10000, Train Loss: 25.056569879705254, Valid Loss: 35.06407801310221\n","Epoch: 97/10000, Train Loss: 25.1248045834628, Valid Loss: 35.029650370279946\n","Epoch: 98/10000, Train Loss: 25.138069673018023, Valid Loss: 35.53501637776693\n","Epoch: 99/10000, Train Loss: 24.81630412015048, Valid Loss: 35.91358311971029\n","Epoch: 100/10000, Train Loss: 25.24914620139382, Valid Loss: 35.723211924235024\n","Epoch: 101/10000, Train Loss: 25.203984520652078, Valid Loss: 35.04212315877279\n","Epoch: 102/10000, Train Loss: 25.175934531471945, Valid Loss: 35.135369618733726\n","Epoch: 103/10000, Train Loss: 24.78141247142445, Valid Loss: 35.401936848958336\n","Epoch: 104/10000, Train Loss: 24.75000710920854, Valid Loss: 35.710933685302734\n","Epoch: 105/10000, Train Loss: 24.70370171286843, Valid Loss: 34.681495666503906\n","Epoch: 106/10000, Train Loss: 24.681453184647992, Valid Loss: 35.04348627726237\n","Epoch: 107/10000, Train Loss: 24.84436139193448, Valid Loss: 35.682692209879555\n","Epoch: 108/10000, Train Loss: 24.55027909712358, Valid Loss: 35.10488637288412\n","Epoch: 109/10000, Train Loss: 24.61848293651234, Valid Loss: 34.881211598714195\n","Epoch: 110/10000, Train Loss: 25.055504885586824, Valid Loss: 35.09950256347656\n","Epoch: 111/10000, Train Loss: 24.10245565934615, Valid Loss: 36.03204854329427\n","Epoch: 112/10000, Train Loss: 24.518408862027254, Valid Loss: 35.29631042480469\n","Epoch: 113/10000, Train Loss: 24.041787581010297, Valid Loss: 35.034865061442055\n","얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 113에서 훈련 중단.\n"]}],"id":"B61MVSEnhnUC"},{"cell_type":"code","source":["torch.save(morganModel_MLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyMorganFingerprint_DNN_Model_MLM_3.pth')  # 모델 객체의 state_dict 저장\n","torch.save(morganModel_HLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyMorganFingerprint_DNN_Model_HLM_3.pth')"],"metadata":{"id":"V4F3RpGVhnVU","executionInfo":{"status":"ok","timestamp":1693229923729,"user_tz":-540,"elapsed":2755,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":42,"outputs":[],"id":"V4F3RpGVhnVU"},{"cell_type":"code","source":["morganModel_MLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyMorganFingerprint_DNN_Model_MLM_3.pth'))\n","morganModel_HLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyMorganFingerprint_DNN_Model_HLM_3.pth'))"],"metadata":{"id":"xwgZcOB6hnWm"},"execution_count":null,"outputs":[],"id":"xwgZcOB6hnWm"},{"cell_type":"code","source":["test_MLM = MorganDataset(test, target_col=None, transform=transform, is_test=True)\n","test_HLM = MorganDataset(test, target_col=None, transform=transform, is_test=True)\n","\n","test_MLM_loader = DataLoader(dataset=test_MLM,\n","                             batch_size=CFG['BATCH_SIZE'],\n","                             shuffle=False)\n","\n","test_HLM_loader = DataLoader(dataset=test_HLM,\n","                             batch_size=CFG['BATCH_SIZE'],\n","                             shuffle=False)"],"metadata":{"id":"Pa1VzMsUhnX8","executionInfo":{"status":"ok","timestamp":1693231404905,"user_tz":-540,"elapsed":611,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":43,"outputs":[],"id":"Pa1VzMsUhnX8"},{"cell_type":"code","source":["def inference(test_loader, model):\n","    model.eval()\n","    preds = []\n","\n","    with torch.no_grad():\n","        for inputs in test_loader:\n","            output = model(inputs)\n","            preds.extend(output.cpu().numpy().flatten().tolist())\n","\n","    return preds"],"metadata":{"id":"R5In4LUnhnZQ","executionInfo":{"status":"ok","timestamp":1693231406645,"user_tz":-540,"elapsed":1,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":44,"outputs":[],"id":"R5In4LUnhnZQ"},{"cell_type":"code","source":["morgan_predictions_MLM = inference(test_MLM_loader, morganModel_MLM)\n","morgan_predictions_HLM = inference(test_HLM_loader, morganModel_HLM)"],"metadata":{"id":"zSo5wHfjhnao","executionInfo":{"status":"ok","timestamp":1693231407118,"user_tz":-540,"elapsed":3,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"execution_count":45,"outputs":[],"id":"zSo5wHfjhnao"},{"cell_type":"code","source":["import numpy as np\n","\n","morgan_predict_MLM = np.array(morgan_predictions_MLM)\n","newFeature_predict_MLM = np.array(newFeature_predictions_MLM)\n","\n","morgan_predict_HLM = np.array(morgan_predictions_HLM)\n","newFeature_predict_HLM = np.array(newFeature_predictions_HLM)\n","\n","ensemble_predictions_MLM = (morgan_predict_MLM + newFeature_predict_MLM) / 2\n","ensemble_predictions_HLM = (morgan_predict_HLM + newFeature_predict_HLM) / 2\n","\n","print(ensemble_predictions_HLM)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lz3HbzYT7KYL","executionInfo":{"status":"ok","timestamp":1693231408708,"user_tz":-540,"elapsed":3,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"b844edfe-7071-4b92-dca9-f438bb3be97a"},"id":"lz3HbzYT7KYL","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["[60.46485901 86.49609375 61.66860771 68.56609726 89.90302658 83.05833435\n"," 34.22291851 61.19134903 27.76046944 43.76188087 37.40528107 83.35913849\n"," 76.06226349 70.82374573 28.8131752  82.70818329 41.24373722 59.513237\n"," 84.40455627 76.69549561 59.70306206 25.78975296 28.51766205 26.89700985\n"," 39.10724068 63.7062149  58.40719414 85.40262985 69.63093758 24.79141331\n"," 60.55277061 34.51484489 47.14051247 48.41561317 73.45012283 29.45366383\n"," 50.29488564 81.99282074 36.84933472 27.42454624 36.21807671 88.9228096\n"," 22.88251591 43.12194538 28.44503403 87.02971649 29.41240215 27.10623264\n"," 74.64299774 60.62939644 83.29589081 29.78503895 37.05029488 22.7594223\n"," 46.39592934 64.76865578 64.74313545 39.76068497 31.2953968  36.34324837\n"," 44.07148552 73.46212769 49.8523407  79.96922302 68.48624039 51.85720825\n"," 74.70523453 58.68249321 73.4086895  62.44529533 63.1654911  76.51783752\n"," 63.22107506 83.98417664 36.63149071 78.38834381 68.07758904 59.97873306\n"," 80.65073395 31.84738922 77.74015045 77.7095108  33.41256046 30.7139492\n"," 29.29846001 83.12488174 26.91385841 88.63310242 72.63542175 84.20558548\n"," 34.56208515 36.45487404 56.89003944 39.47088337 74.44121552 40.4779377\n"," 82.98122787 84.67821121 29.04201508 30.09610367 72.58998871 23.22131729\n"," 84.84384918 86.12030029 40.67434502 78.30173492 65.6403141  89.72273254\n"," 54.80699158 59.01169777 66.55761147 79.00321388 49.81889534 71.59780121\n"," 67.91893959 27.88210011 32.44266701 61.1699295  65.6508255  39.37947655\n"," 85.0872345  54.2284584  38.73181343 43.60653591 30.04988289 84.84178543\n"," 62.22458458 81.57638931 82.52929306 36.15610123 51.72983742 45.03666973\n"," 58.44248581 64.97231483 50.55693626 64.58412933 78.88075447 66.12952995\n"," 22.6492548  31.68234158 25.40207672 23.35592079 50.37376022 22.87116528\n"," 81.71398163 27.10751724 89.08230591 77.56839561 20.71870708 80.05456924\n"," 46.13536644 81.73828125 52.41414261 26.91935349 43.00426483 63.39374352\n"," 52.51113319 67.62394142 47.7589016  26.91409016 81.87760544 53.90954971\n"," 83.00505066 69.98539162 63.43994331 30.23697281 40.07745743 76.82723236\n"," 45.68315887 23.17366123 62.15314674 50.1999073  40.98519325 26.52960682\n"," 23.30479527 83.2486496  54.87098122 40.53775215 66.72307014 66.66103554\n"," 36.01346397 24.28330803 51.04339981 25.04615307 48.89141083 32.93330669\n"," 39.9298954  65.04593277 21.76852226 58.3249588  78.92503738 62.07183647\n"," 38.74218559 46.31859875 26.43873119 43.58700752 75.2544632  72.77259636\n"," 63.98724937 60.73161316 70.23216438 32.82089615 83.28216553 61.25887489\n"," 62.50647926 61.96206284 24.80632877 58.69525814 29.37305164 25.65936756\n"," 63.56611633 77.55569267 39.43900108 28.68026638 25.2274828  41.07106304\n"," 39.6092453  61.06335258 83.01367188 30.58803749 32.54459381 62.19452095\n"," 24.88030052 88.07781982 63.90536118 42.45258713 30.28757668 36.88651848\n"," 57.17954063 22.32774448 22.67547512 39.0002327  53.24992752 39.86527061\n"," 32.3346529  67.78036308 28.99720192 42.49821472 37.45392418 32.2694521\n"," 46.79755211 85.69850922 75.76799011 52.43623161 88.78154373 72.11896515\n"," 83.52479553 38.59732246 69.0747242  73.32534599 86.03668213 46.85749435\n"," 84.3085022  40.13910484 89.35087967 59.9675808  68.68457222 86.43841553\n"," 59.87926102 83.39322662 69.74707031 89.35583496 31.83052254 42.16225815\n"," 31.71608734 51.34800339 41.83976364 69.76410294 85.02600098 23.41070271\n"," 83.68991852 77.18571854 61.89612389 68.34269524 29.16184998 30.16025543\n"," 54.17109299 70.26980591 51.70866966 57.94388962 37.98219299 26.57814121\n"," 25.02655125 67.44099808 65.12057304 76.46156693 86.7552948  87.01367188\n"," 77.345047   39.70803642 23.99001694 68.76675224 55.93448067 45.94475555\n"," 77.8609333  32.56354523 41.95622921 47.74533081 73.92313766 71.17179108\n"," 80.09872437 91.47187042 67.82150269 63.68505669 48.3637352  24.6249485\n"," 71.47937012 31.60237789 28.82270718 43.3074398  64.57588577 50.8145504\n"," 41.07132912 36.28846169 42.82869911 73.65754318 40.75718498 60.75591469\n"," 65.25024414 73.94296265 26.9549818  49.56140709 72.23803711 38.88468361\n"," 68.76260185 54.93346786 32.94032669 81.31173706 70.27971649 30.41200066\n"," 78.85106659 58.35260582 74.02876282 32.84481621 88.42021942 24.45404816\n"," 42.87098122 62.0228138  68.69897652 22.57908535 30.26003647 89.66674042\n"," 40.88563824 76.32126236 50.86941338 56.58433247 48.28791332 84.63954163\n"," 33.65654945 63.37362289 27.68743801 39.40283203 27.45596886 57.91280556\n"," 79.63395691 31.90436649 83.28552628 54.10941124 29.68250942 43.93909264\n"," 32.78653717 31.65276432 73.17333984 73.59889603 71.89221382 81.13745499\n"," 27.58194351 70.43544197 80.2026825  77.49964523 63.37068939 87.53681564\n"," 81.04071426 25.48922062 41.76707077 86.93774414 50.41738129 49.29543304\n"," 54.18420982 56.29216576 41.41837311 64.235075   89.36320877 34.03163528\n"," 62.68628311 86.40973663 35.68646526 83.29353333 27.09671307 19.46661186\n"," 75.03603363 65.62362289 61.12734032 49.96092892 28.5577364  84.27320862\n"," 71.37324142 56.60463333 28.89559364 84.42549133 80.06420898 47.90352058\n"," 34.09671783 61.60820198 74.47297096 83.02242661 83.47483063 67.03738213\n"," 60.49741173 36.28500652 84.07894135 39.31023598 82.41787338 42.39912224\n"," 68.67596817 32.92197418 69.57849693 47.52958488 48.11206436 41.93601227\n"," 28.05743504 83.70821762 34.85291862 40.04980659 64.62402916 63.40725136\n"," 52.1837616  36.34402084 85.16146088 53.7539196  87.85740662 54.60074615\n"," 29.20075703 82.48840332 23.16122723 82.93839264 54.59678268 66.68358994\n"," 74.64512634 29.14853573 76.51057434 57.93315506 50.92746353 30.41081905\n"," 77.63547897 76.3759079  27.88046074 74.8228569  28.29460335 24.65928173\n"," 78.17853546 66.43808556 26.89843273 81.40959549 67.51369476 71.88693619\n"," 39.43114376 22.54663467 64.81391144 40.93658638 17.29991007 82.00073242\n"," 28.89105606 41.76825142 82.54458237 32.82950306 26.88725662 85.83348846\n"," 84.35707092 51.12371063 78.18773651 51.45755386 22.14066029 32.47517776\n"," 72.78708649 72.23503685 59.4379406  22.85767937 30.33393002 93.9345932\n"," 77.11973572 74.77776337 83.53997803]\n"]}]},{"cell_type":"code","source":["submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/sample_submission.csv')\n","\n","submission['MLM'] = ensemble_predictions_MLM\n","submission['HLM'] = ensemble_predictions_HLM\n","submission"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"MYTcRzosjtau","executionInfo":{"status":"ok","timestamp":1693231415971,"user_tz":-540,"elapsed":1018,"user":{"displayName":"임송재","userId":"10220915962739075092"}},"outputId":"02b52b2c-f4b8-4aac-da5c-271d9b5087be"},"id":"MYTcRzosjtau","execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           id        MLM        HLM\n","0    TEST_000  14.932632  60.464859\n","1    TEST_001  69.077065  86.496094\n","2    TEST_002  26.201039  61.668608\n","3    TEST_003  48.412098  68.566097\n","4    TEST_004  39.029470  89.903027\n","..        ...        ...        ...\n","478  TEST_478  13.450228  30.333930\n","479  TEST_479  82.493900  93.934593\n","480  TEST_480  30.967842  77.119736\n","481  TEST_481  49.739725  74.777763\n","482  TEST_482  23.241776  83.539978\n","\n","[483 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-95399722-0756-4598-a1f8-fb0c6a32fdfd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>MLM</th>\n","      <th>HLM</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TEST_000</td>\n","      <td>14.932632</td>\n","      <td>60.464859</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TEST_001</td>\n","      <td>69.077065</td>\n","      <td>86.496094</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TEST_002</td>\n","      <td>26.201039</td>\n","      <td>61.668608</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TEST_003</td>\n","      <td>48.412098</td>\n","      <td>68.566097</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TEST_004</td>\n","      <td>39.029470</td>\n","      <td>89.903027</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>478</th>\n","      <td>TEST_478</td>\n","      <td>13.450228</td>\n","      <td>30.333930</td>\n","    </tr>\n","    <tr>\n","      <th>479</th>\n","      <td>TEST_479</td>\n","      <td>82.493900</td>\n","      <td>93.934593</td>\n","    </tr>\n","    <tr>\n","      <th>480</th>\n","      <td>TEST_480</td>\n","      <td>30.967842</td>\n","      <td>77.119736</td>\n","    </tr>\n","    <tr>\n","      <th>481</th>\n","      <td>TEST_481</td>\n","      <td>49.739725</td>\n","      <td>74.777763</td>\n","    </tr>\n","    <tr>\n","      <th>482</th>\n","      <td>TEST_482</td>\n","      <td>23.241776</td>\n","      <td>83.539978</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>483 rows × 3 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95399722-0756-4598-a1f8-fb0c6a32fdfd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-95399722-0756-4598-a1f8-fb0c6a32fdfd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-95399722-0756-4598-a1f8-fb0c6a32fdfd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-fd9ee8fd-8ab5-45a3-abe3-bc4905c74b0a\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd9ee8fd-8ab5-45a3-abe3-bc4905c74b0a')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const charts = await google.colab.kernel.invokeFunction(\n","          'suggestCharts', [key], {});\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-fd9ee8fd-8ab5-45a3-abe3-bc4905c74b0a button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","execution_count":48,"id":"aJpMgK_OdxXR","metadata":{"id":"aJpMgK_OdxXR","executionInfo":{"status":"ok","timestamp":1693231418806,"user_tz":-540,"elapsed":617,"user":{"displayName":"임송재","userId":"10220915962739075092"}}},"outputs":[],"source":["submission.to_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/submissions/NewFeature+Morgan_DNN_EnsembleModel_1_submission.csv', index=False)"]},{"cell_type":"code","source":[],"metadata":{"id":"F6iMwd0e9-78"},"id":"F6iMwd0e9-78","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}