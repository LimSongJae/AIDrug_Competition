{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "86cd3349",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1692638632634,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "86cd3349",
    "outputId": "a242fcd6-3e0b-40a8-dbf4-ffee5869b18f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import missingno\n",
    "\n",
    "# device 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print (device)\n",
    "\n",
    "seed = 42 # seed 값 설정\n",
    "random.seed(seed) # 파이썬 난수 생성기\n",
    "os.environ['PYTHONHASHSEED'] = str(seed) # 해시 시크릿값 고정\n",
    "np.random.seed(seed) # 넘파이 난수 생성기\n",
    "\n",
    "torch.manual_seed(seed) # 파이토치 CPU 난수 생성기\n",
    "torch.backends.cudnn.deterministic = True # 확정적 연산 사용 설정\n",
    "torch.backends.cudnn.benchmark = False   # 벤치마크 기능 사용 해제\n",
    "torch.backends.cudnn.enabled = False        # cudnn 기능 사용 해제\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(seed) # 파이토치 GPU 난수 생성기\n",
    "    torch.cuda.manual_seed_all(seed) # 파이토치 멀티 GPU 난수 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a30f491",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19276,
     "status": "ok",
     "timestamp": 1692623455602,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "5a30f491",
    "outputId": "f49d5c70-8e39-4420-9bce-d43acff2ae9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "a2911bd2",
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1692638636309,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "a2911bd2"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/train.csv')\n",
    "test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "d784f776",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1692638637489,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "d784f776",
    "outputId": "f1121e60-e3a9-4b50-c3e6-5fd8c896d689"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ad2ef661-097e-4a98-84df-e9132741d841\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>Molecular_Weight</th>\n",
       "      <th>Num_H_Acceptors</th>\n",
       "      <th>Num_H_Donors</th>\n",
       "      <th>Num_RotatableBonds</th>\n",
       "      <th>LogD</th>\n",
       "      <th>Molecular_PolarSurfaceArea</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n",
       "      <td>26.010</td>\n",
       "      <td>50.680</td>\n",
       "      <td>3.259</td>\n",
       "      <td>400.495</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3.259</td>\n",
       "      <td>117.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n",
       "      <td>29.270</td>\n",
       "      <td>50.590</td>\n",
       "      <td>2.169</td>\n",
       "      <td>301.407</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.172</td>\n",
       "      <td>73.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n",
       "      <td>5.586</td>\n",
       "      <td>80.892</td>\n",
       "      <td>1.593</td>\n",
       "      <td>297.358</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.585</td>\n",
       "      <td>62.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n",
       "      <td>5.710</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.771</td>\n",
       "      <td>494.652</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.475</td>\n",
       "      <td>92.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n",
       "      <td>93.270</td>\n",
       "      <td>99.990</td>\n",
       "      <td>2.335</td>\n",
       "      <td>268.310</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.337</td>\n",
       "      <td>42.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3493</th>\n",
       "      <td>TRAIN_3493</td>\n",
       "      <td>Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl</td>\n",
       "      <td>1.556</td>\n",
       "      <td>3.079</td>\n",
       "      <td>3.409</td>\n",
       "      <td>396.195</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.409</td>\n",
       "      <td>64.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494</th>\n",
       "      <td>TRAIN_3494</td>\n",
       "      <td>CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...</td>\n",
       "      <td>35.560</td>\n",
       "      <td>47.630</td>\n",
       "      <td>1.912</td>\n",
       "      <td>359.381</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.844</td>\n",
       "      <td>77.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>TRAIN_3495</td>\n",
       "      <td>CCOC(=O)CCCc1nc2cc(N)ccc2n1C</td>\n",
       "      <td>56.150</td>\n",
       "      <td>1.790</td>\n",
       "      <td>1.941</td>\n",
       "      <td>261.320</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.124</td>\n",
       "      <td>70.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>TRAIN_3496</td>\n",
       "      <td>Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2.770</td>\n",
       "      <td>0.989</td>\n",
       "      <td>284.696</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.989</td>\n",
       "      <td>91.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>TRAIN_3497</td>\n",
       "      <td>COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1</td>\n",
       "      <td>0.450</td>\n",
       "      <td>2.650</td>\n",
       "      <td>4.321</td>\n",
       "      <td>295.399</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.321</td>\n",
       "      <td>50.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3498 rows × 11 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ad2ef661-097e-4a98-84df-e9132741d841')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-ad2ef661-097e-4a98-84df-e9132741d841 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-ad2ef661-097e-4a98-84df-e9132741d841');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-8c18c5c6-dd0c-4d16-abc7-dfe5c1117da5\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c18c5c6-dd0c-4d16-abc7-dfe5c1117da5')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const charts = await google.colab.kernel.invokeFunction(\n",
       "          'suggestCharts', [key], {});\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-8c18c5c6-dd0c-4d16-abc7-dfe5c1117da5 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "              id                                             SMILES     MLM  \\\n",
       "0     TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n",
       "1     TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n",
       "2     TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n",
       "3     TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n",
       "4     TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n",
       "...          ...                                                ...     ...   \n",
       "3493  TRAIN_3493     Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl   1.556   \n",
       "3494  TRAIN_3494  CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...  35.560   \n",
       "3495  TRAIN_3495                       CCOC(=O)CCCc1nc2cc(N)ccc2n1C  56.150   \n",
       "3496  TRAIN_3496                     Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl   0.030   \n",
       "3497  TRAIN_3497                   COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1   0.450   \n",
       "\n",
       "         HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
       "0     50.680  3.259           400.495                5             2   \n",
       "1     50.590  2.169           301.407                2             1   \n",
       "2     80.892  1.593           297.358                5             0   \n",
       "3      2.000  4.771           494.652                6             0   \n",
       "4     99.990  2.335           268.310                3             0   \n",
       "...      ...    ...               ...              ...           ...   \n",
       "3493   3.079  3.409           396.195                3             1   \n",
       "3494  47.630  1.912           359.381                4             1   \n",
       "3495   1.790  1.941           261.320                3             1   \n",
       "3496   2.770  0.989           284.696                5             1   \n",
       "3497   2.650  4.321           295.399                2             0   \n",
       "\n",
       "      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea  \n",
       "0                      8  3.259                      117.37  \n",
       "1                      2  2.172                       73.47  \n",
       "2                      3  1.585                       62.45  \n",
       "3                      5  3.475                       92.60  \n",
       "4                      1  2.337                       42.43  \n",
       "...                  ...    ...                         ...  \n",
       "3493                   5  3.409                       64.74  \n",
       "3494                   3  1.844                       77.37  \n",
       "3495                   6  2.124                       70.14  \n",
       "3496                   5  0.989                       91.51  \n",
       "3497                   4  4.321                       50.36  \n",
       "\n",
       "[3498 rows x 11 columns]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "6e6800c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5200,
     "status": "ok",
     "timestamp": 1692638644323,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "6e6800c9",
    "outputId": "53fd4d00-e8f5-4de3-b960-3bbca0c4dcab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdkit-pypi in /usr/local/lib/python3.10/dist-packages (2022.9.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (1.23.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit-pypi) (9.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdkit-pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "3c2cda0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 17620,
     "status": "ok",
     "timestamp": 1692638661936,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "3c2cda0b",
    "outputId": "379caa82-3db9-426f-9553-ddcabc3a49a2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-38a2a0b4-1cab-4621-929b-eee8edbf2d5c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>Molecular_Weight</th>\n",
       "      <th>Num_H_Acceptors</th>\n",
       "      <th>Num_H_Donors</th>\n",
       "      <th>Num_RotatableBonds</th>\n",
       "      <th>LogD</th>\n",
       "      <th>Molecular_PolarSurfaceArea</th>\n",
       "      <th>logP</th>\n",
       "      <th>apka</th>\n",
       "      <th>num_rotatable_bonds</th>\n",
       "      <th>num_heteroatoms</th>\n",
       "      <th>num_hydrogen_acceptors</th>\n",
       "      <th>num_hydrogen_donors</th>\n",
       "      <th>morgan_fingerprint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_0000</td>\n",
       "      <td>CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC</td>\n",
       "      <td>26.010</td>\n",
       "      <td>50.680</td>\n",
       "      <td>3.259</td>\n",
       "      <td>400.495</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3.259</td>\n",
       "      <td>117.37</td>\n",
       "      <td>3.87744</td>\n",
       "      <td>400.504</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_0001</td>\n",
       "      <td>Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1</td>\n",
       "      <td>29.270</td>\n",
       "      <td>50.590</td>\n",
       "      <td>2.169</td>\n",
       "      <td>301.407</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.172</td>\n",
       "      <td>73.47</td>\n",
       "      <td>3.35474</td>\n",
       "      <td>301.415</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_0002</td>\n",
       "      <td>CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1</td>\n",
       "      <td>5.586</td>\n",
       "      <td>80.892</td>\n",
       "      <td>1.593</td>\n",
       "      <td>297.358</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.585</td>\n",
       "      <td>62.45</td>\n",
       "      <td>1.20450</td>\n",
       "      <td>297.366</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_0003</td>\n",
       "      <td>Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...</td>\n",
       "      <td>5.710</td>\n",
       "      <td>2.000</td>\n",
       "      <td>4.771</td>\n",
       "      <td>494.652</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.475</td>\n",
       "      <td>92.60</td>\n",
       "      <td>3.89356</td>\n",
       "      <td>494.665</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_0004</td>\n",
       "      <td>Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2</td>\n",
       "      <td>93.270</td>\n",
       "      <td>99.990</td>\n",
       "      <td>2.335</td>\n",
       "      <td>268.310</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.337</td>\n",
       "      <td>42.43</td>\n",
       "      <td>2.81772</td>\n",
       "      <td>268.316</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3493</th>\n",
       "      <td>TRAIN_3493</td>\n",
       "      <td>Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl</td>\n",
       "      <td>1.556</td>\n",
       "      <td>3.079</td>\n",
       "      <td>3.409</td>\n",
       "      <td>396.195</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.409</td>\n",
       "      <td>64.74</td>\n",
       "      <td>2.74730</td>\n",
       "      <td>396.200</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3494</th>\n",
       "      <td>TRAIN_3494</td>\n",
       "      <td>CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...</td>\n",
       "      <td>35.560</td>\n",
       "      <td>47.630</td>\n",
       "      <td>1.912</td>\n",
       "      <td>359.381</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.844</td>\n",
       "      <td>77.37</td>\n",
       "      <td>2.27630</td>\n",
       "      <td>359.389</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3495</th>\n",
       "      <td>TRAIN_3495</td>\n",
       "      <td>CCOC(=O)CCCc1nc2cc(N)ccc2n1C</td>\n",
       "      <td>56.150</td>\n",
       "      <td>1.790</td>\n",
       "      <td>1.941</td>\n",
       "      <td>261.320</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.124</td>\n",
       "      <td>70.14</td>\n",
       "      <td>2.04130</td>\n",
       "      <td>261.325</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>TRAIN_3496</td>\n",
       "      <td>Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2.770</td>\n",
       "      <td>0.989</td>\n",
       "      <td>284.696</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.989</td>\n",
       "      <td>91.51</td>\n",
       "      <td>1.42720</td>\n",
       "      <td>284.699</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>TRAIN_3497</td>\n",
       "      <td>COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1</td>\n",
       "      <td>0.450</td>\n",
       "      <td>2.650</td>\n",
       "      <td>4.321</td>\n",
       "      <td>295.399</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.321</td>\n",
       "      <td>50.36</td>\n",
       "      <td>4.71792</td>\n",
       "      <td>295.407</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3498 rows × 18 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38a2a0b4-1cab-4621-929b-eee8edbf2d5c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-38a2a0b4-1cab-4621-929b-eee8edbf2d5c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-38a2a0b4-1cab-4621-929b-eee8edbf2d5c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-c4d57e42-1185-4e7b-8ba5-5f0b69a89d60\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c4d57e42-1185-4e7b-8ba5-5f0b69a89d60')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const charts = await google.colab.kernel.invokeFunction(\n",
       "          'suggestCharts', [key], {});\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-c4d57e42-1185-4e7b-8ba5-5f0b69a89d60 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "              id                                             SMILES     MLM  \\\n",
       "0     TRAIN_0000    CCOc1ccc(CNC(=O)c2cc(-c3sc(C)nc3C)n[nH]2)cc1OCC  26.010   \n",
       "1     TRAIN_0001               Cc1nc(C)c(CN2CC(C)C(=O)Nc3ccccc32)s1  29.270   \n",
       "2     TRAIN_0002                   CCCN1CCN(c2nn3nnnc3c3ccccc23)CC1   5.586   \n",
       "3     TRAIN_0003  Cc1ccc(-c2ccc(-n3nc(C)c(S(=O)(=O)N4CCN(C5CCCCC...   5.710   \n",
       "4     TRAIN_0004                Cc1ccc2c(c1)N(C(=O)c1ccncc1)CC(C)O2  93.270   \n",
       "...          ...                                                ...     ...   \n",
       "3493  TRAIN_3493     Cn1nc(CNC(=O)Cn2nc(C(F)(F)F)c3c2CCC3)c(Cl)c1Cl   1.556   \n",
       "3494  TRAIN_3494  CCn1[nH]cc/c1=N\\C(=O)c1nn(-c2ccccc2)c(=O)c2ccc...  35.560   \n",
       "3495  TRAIN_3495                       CCOC(=O)CCCc1nc2cc(N)ccc2n1C  56.150   \n",
       "3496  TRAIN_3496                     Nc1cc(C(=O)OCCC2CCOC2=O)cnc1Cl   0.030   \n",
       "3497  TRAIN_3497                   COc1ccc(-c2nc(Cc3ccccc3)sc2C)cc1   0.450   \n",
       "\n",
       "         HLM  AlogP  Molecular_Weight  Num_H_Acceptors  Num_H_Donors  \\\n",
       "0     50.680  3.259           400.495                5             2   \n",
       "1     50.590  2.169           301.407                2             1   \n",
       "2     80.892  1.593           297.358                5             0   \n",
       "3      2.000  4.771           494.652                6             0   \n",
       "4     99.990  2.335           268.310                3             0   \n",
       "...      ...    ...               ...              ...           ...   \n",
       "3493   3.079  3.409           396.195                3             1   \n",
       "3494  47.630  1.912           359.381                4             1   \n",
       "3495   1.790  1.941           261.320                3             1   \n",
       "3496   2.770  0.989           284.696                5             1   \n",
       "3497   2.650  4.321           295.399                2             0   \n",
       "\n",
       "      Num_RotatableBonds   LogD  Molecular_PolarSurfaceArea     logP     apka  \\\n",
       "0                      8  3.259                      117.37  3.87744  400.504   \n",
       "1                      2  2.172                       73.47  3.35474  301.415   \n",
       "2                      3  1.585                       62.45  1.20450  297.366   \n",
       "3                      5  3.475                       92.60  3.89356  494.665   \n",
       "4                      1  2.337                       42.43  2.81772  268.316   \n",
       "...                  ...    ...                         ...      ...      ...   \n",
       "3493                   5  3.409                       64.74  2.74730  396.200   \n",
       "3494                   3  1.844                       77.37  2.27630  359.389   \n",
       "3495                   6  2.124                       70.14  2.04130  261.325   \n",
       "3496                   5  0.989                       91.51  1.42720  284.699   \n",
       "3497                   4  4.321                       50.36  4.71792  295.407   \n",
       "\n",
       "      num_rotatable_bonds  num_heteroatoms  num_hydrogen_acceptors  \\\n",
       "0                       8                8                       6   \n",
       "1                       2                5                       4   \n",
       "2                       3                7                       7   \n",
       "3                       5                9                       7   \n",
       "4                       1                4                       3   \n",
       "...                   ...              ...                     ...   \n",
       "3493                    4               11                       5   \n",
       "3494                    3                7                       5   \n",
       "3495                    5                5                       5   \n",
       "3496                    4                7                       6   \n",
       "3497                    4                3                       3   \n",
       "\n",
       "      num_hydrogen_donors                                 morgan_fingerprint  \n",
       "0                       2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1                       1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3                       0  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4                       0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                   ...                                                ...  \n",
       "3493                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3494                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3495                    1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3496                    1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3497                    0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[3498 rows x 18 columns]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import AllChem, Descriptors\n",
    "\n",
    "def calculate_metabolic_stability_descriptors(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    logP = Descriptors.MolLogP(mol)\n",
    "    # 화합물의 친유성을 측정한 것으로 지질 또는 비극성 환경에서의 용해도를 나타냅니다. 생물학적 막을 통과하는 화합물의 능력을 반영합니다.\n",
    "    apka = Descriptors.MolWt(mol)\n",
    "    # 화합물의 산 해리 상수의 추정치로 다양한 pH 조건에서 이온화 거동에 대한 정보를 제공합니다.\n",
    "    num_rotatable_bonds = Descriptors.NumRotatableBonds(mol)\n",
    "    # 화합물에서 회전 가능한 결합의 수입니다. 이것은 화합물의 유연성과 효소 또는 다른 분자와의 잠재적인 상호 작용에 대한 통찰력을 제공할 수 있습니다.\n",
    "    num_heteroatoms = Descriptors.NumHeteroatoms(mol)\n",
    "    # 분자 내 헤테로원자(탄소 및 수소 이외의 원자) 수. 이는 화합물의 반응성과 대사 안정성에 영향을 줄 수 있습니다.\n",
    "    num_hydrogen_acceptors = Descriptors.NumHAcceptors(mol)\n",
    "    # 분자 내 수소 결합 수용체의 수. 이들은 결합 및 반응성에 영향을 미치는 다른 분자의 수소 결합 기증자와 상호 작용할 수 있는 사이트입니다.\n",
    "    num_hydrogen_donors = Descriptors.NumHDonors(mol)\n",
    "    # 분자 내 수소 결합 기증자의 수입니다. 이들은 수소 결합 상호작용에서 수소 원자를 제공할 수 있는 사이트입니다.\n",
    "    # morgan_fingerprint = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048)\n",
    "    morgan_fingerprint = AllChem.GetHashedMorganFingerprint(mol, 6, nBits=4096)\n",
    "    # 분자 하위 구조의 이진 벡터 표현입니다. 이 열에는 화합물과 효소의 상호 작용 및 대사 안정성에 영향을 줄 수 있는 구조적 특징을 포착하는 이진 지문이 포함되어 있습니다.\n",
    "    morgan_array = np.zeros((1,), dtype=np.int8)\n",
    "    DataStructs.ConvertToNumpyArray(morgan_fingerprint, morgan_array)\n",
    "\n",
    "    return logP, apka, num_rotatable_bonds, num_heteroatoms, num_hydrogen_acceptors, num_hydrogen_donors, morgan_array\n",
    "\n",
    "train[[\n",
    "    'logP', 'apka', 'num_rotatable_bonds', 'num_heteroatoms',\n",
    "    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint'\n",
    "]] = train['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n",
    "\n",
    "test[[\n",
    "    'logP', 'apka', 'num_rotatable_bonds', 'num_heteroatoms',\n",
    "    'num_hydrogen_acceptors', 'num_hydrogen_donors', 'morgan_fingerprint'\n",
    "]] = test['SMILES'].apply(calculate_metabolic_stability_descriptors).apply(pd.Series)\n",
    "\n",
    "train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "M84FfbMS-Ttt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1692638145141,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "M84FfbMS-Ttt",
    "outputId": "fc5c40c6-3de5-4d9e-b858-4de4d11896eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2075c062-aab4-4dda-b3a6-39b0d62dc89d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2075c062-aab4-4dda-b3a6-39b0d62dc89d\")) {                    Plotly.newPlot(                        \"2075c062-aab4-4dda-b3a6-39b0d62dc89d\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"MLM\",\"HLM\",\"AlogP\",\"Molecular_Weight\",\"Num_H_Acceptors\",\"Num_H_Donors\",\"Num_RotatableBonds\",\"LogD\",\"Molecular_PolarSurfaceArea\",\"logP\",\"apka\",\"num_rotatable_bonds\",\"num_heteroatoms\",\"num_hydrogen_acceptors\",\"num_hydrogen_donors\"],\"y\":[\"MLM\",\"HLM\",\"AlogP\",\"Molecular_Weight\",\"Num_H_Acceptors\",\"Num_H_Donors\",\"Num_RotatableBonds\",\"LogD\",\"Molecular_PolarSurfaceArea\",\"logP\",\"apka\",\"num_rotatable_bonds\",\"num_heteroatoms\",\"num_hydrogen_acceptors\",\"num_hydrogen_donors\"],\"z\":[[1.0,0.7067251024878,-0.3300826499827315,-0.08123913966297934,0.16450975744691224,0.21083588810096493,-0.09256266192711304,-0.35014591323933564,0.184849785645308,-0.30734844288027663,-0.08327303905972998,-0.091376903443668,0.16492651941443726,0.09172953254568401,0.21699615393852822],[0.7067251024878,1.0,-0.346022143896057,-0.1751168230755293,0.09231267579874262,0.176549393873596,-0.13226256314849838,-0.3574559486898405,0.09432262585644133,-0.32336264361777783,-0.1771216851920563,-0.12488780668276511,0.07547112050553692,0.047136218752318174,0.18451167849848712],[-0.3300826499827315,-0.346022143896057,1.0,0.38975964987189593,-0.2844153878193624,-0.17222215929788845,0.11184434754920093,0.9576111642662698,-0.29819398553681126,0.887321738681376,0.4187236445194466,0.11781285831289552,-0.18659173454852074,-0.1745781382871405,-0.134278934773394],[-0.08123913966297934,-0.1751168230755293,0.38975964987189593,1.0,0.4718137354590274,0.11618610965521532,0.5837107521276598,0.3694619827451061,0.43911413238619246,0.4720392677274182,0.9824542186412591,0.5568283393004163,0.6073252705999541,0.4198237003386296,0.07694871788140818],[0.16450975744691224,0.09231267579874262,-0.2844153878193624,0.4718137354590274,1.0,0.20843323791440715,0.47401156367726943,-0.30550596770408145,0.7143154321060738,-0.25194266971571516,0.4345879145175044,0.4252297922299244,0.6927861820128884,0.7672830328823523,0.1437639792156416],[0.21083588810096493,0.176549393873596,-0.17222215929788845,0.11618610965521532,0.20843323791440715,1.0,0.17687113618402622,-0.2120824238069809,0.4746141029761259,-0.12459914602412905,0.07732531123866317,0.1583734948530644,0.2077131492322306,0.028964026386888563,0.970630453907293],[-0.09256266192711304,-0.13226256314849838,0.11184434754920093,0.5837107521276598,0.47401156367726943,0.17687113618402622,1.0,0.07165891227719459,0.37157351825140145,0.15201701989735578,0.5563962152972314,0.9643034440905066,0.4031651047866803,0.32139602201763856,0.1350720686291281],[-0.35014591323933564,-0.3574559486898405,0.9576111642662698,0.3694619827451061,-0.30550596770408145,-0.2120824238069809,0.07165891227719459,1.0,-0.29467045023768507,0.861420986900028,0.3999216074184136,0.07331173280230192,-0.16958030574710356,-0.15808933540525358,-0.17256720832818515],[0.184849785645308,0.09432262585644133,-0.29819398553681126,0.43911413238619246,0.7143154321060738,0.4746141029761259,0.37157351825140145,-0.29467045023768507,1.0,-0.24883685207476436,0.4091708792398705,0.33954477203408295,0.7668731242309601,0.650884590114377,0.4339986806749017],[-0.30734844288027663,-0.32336264361777783,0.887321738681376,0.4720392677274182,-0.25194266971571516,-0.12459914602412905,0.15201701989735578,0.861420986900028,-0.24883685207476436,1.0,0.4775393071091598,0.1404972188868642,-0.20127172722392092,-0.2184982441286418,-0.1343242783562854],[-0.08327303905972998,-0.1771216851920563,0.4187236445194466,0.9824542186412591,0.4345879145175044,0.07732531123866317,0.5563962152972314,0.3999216074184136,0.4091708792398705,0.4775393071091598,1.0,0.5531673084105448,0.626279682559663,0.43172407847154165,0.07734936419973609],[-0.091376903443668,-0.12488780668276511,0.11781285831289552,0.5568283393004163,0.4252297922299244,0.1583734948530644,0.9643034440905066,0.07331173280230192,0.33954477203408295,0.1404972188868642,0.5531673084105448,1.0,0.3957234965282207,0.31681357775173913,0.14435990413835367],[0.16492651941443726,0.07547112050553692,-0.18659173454852074,0.6073252705999541,0.6927861820128884,0.2077131492322306,0.4031651047866803,-0.16958030574710356,0.7668731242309601,-0.20127172722392092,0.626279682559663,0.3957234965282207,1.0,0.7420191961494077,0.22367547933123424],[0.09172953254568401,0.047136218752318174,-0.1745781382871405,0.4198237003386296,0.7672830328823523,0.028964026386888563,0.32139602201763856,-0.15808933540525358,0.650884590114377,-0.2184982441286418,0.43172407847154165,0.31681357775173913,0.7420191961494077,1.0,0.034616786492217576],[0.21699615393852822,0.18451167849848712,-0.134278934773394,0.07694871788140818,0.1437639792156416,0.970630453907293,0.1350720686291281,-0.17256720832818515,0.4339986806749017,-0.1343242783562854,0.07734936419973609,0.14435990413835367,0.22367547933123424,0.034616786492217576,1.0]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}\\u003cbr\\u003ey: %{y}\\u003cbr\\u003ecolor: %{z}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\"},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(255,255,204)\"],[0.125,\"rgb(255,237,160)\"],[0.25,\"rgb(254,217,118)\"],[0.375,\"rgb(254,178,76)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(252,78,42)\"],[0.75,\"rgb(227,26,28)\"],[0.875,\"rgb(189,0,38)\"],[1.0,\"rgb(128,0,38)\"]]},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('2075c062-aab4-4dda-b3a6-39b0d62dc89d');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "correlation_matrix = train.corr(numeric_only=True)\n",
    "\n",
    "fig = px.imshow(correlation_matrix, color_continuous_scale='YlOrRd')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "oi2XGXTX1kV_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1692638661937,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "oi2XGXTX1kV_",
    "outputId": "4572adad-45f9-4f10-b6db-d255c8574556"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3498 entries, 0 to 3497\n",
      "Data columns (total 18 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   id                          3498 non-null   object \n",
      " 1   SMILES                      3498 non-null   object \n",
      " 2   MLM                         3498 non-null   float64\n",
      " 3   HLM                         3498 non-null   float64\n",
      " 4   AlogP                       3496 non-null   float64\n",
      " 5   Molecular_Weight            3498 non-null   float64\n",
      " 6   Num_H_Acceptors             3498 non-null   int64  \n",
      " 7   Num_H_Donors                3498 non-null   int64  \n",
      " 8   Num_RotatableBonds          3498 non-null   int64  \n",
      " 9   LogD                        3498 non-null   float64\n",
      " 10  Molecular_PolarSurfaceArea  3498 non-null   float64\n",
      " 11  logP                        3498 non-null   float64\n",
      " 12  apka                        3498 non-null   float64\n",
      " 13  num_rotatable_bonds         3498 non-null   int64  \n",
      " 14  num_heteroatoms             3498 non-null   int64  \n",
      " 15  num_hydrogen_acceptors      3498 non-null   int64  \n",
      " 16  num_hydrogen_donors         3498 non-null   int64  \n",
      " 17  morgan_fingerprint          3498 non-null   object \n",
      "dtypes: float64(8), int64(7), object(3)\n",
      "memory usage: 492.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b8592a22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1692638667881,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "b8592a22",
    "outputId": "baeb6c0a-7efb-4499-8380-ab09661bfc0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3498 entries, 0 to 3497\n",
      "Data columns (total 15 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   MLM                         3498 non-null   float64\n",
      " 1   HLM                         3498 non-null   float64\n",
      " 2   AlogP                       3496 non-null   float64\n",
      " 3   Molecular_Weight            3498 non-null   float64\n",
      " 4   Num_H_Acceptors             3498 non-null   int64  \n",
      " 5   Num_H_Donors                3498 non-null   int64  \n",
      " 6   Num_RotatableBonds          3498 non-null   int64  \n",
      " 7   LogD                        3498 non-null   float64\n",
      " 8   Molecular_PolarSurfaceArea  3498 non-null   float64\n",
      " 9   logP                        3498 non-null   float64\n",
      " 10  apka                        3498 non-null   float64\n",
      " 11  num_rotatable_bonds         3498 non-null   int64  \n",
      " 12  num_heteroatoms             3498 non-null   int64  \n",
      " 13  num_hydrogen_acceptors      3498 non-null   int64  \n",
      " 14  num_hydrogen_donors         3498 non-null   int64  \n",
      "dtypes: float64(8), int64(7)\n",
      "memory usage: 410.0 KB\n"
     ]
    }
   ],
   "source": [
    "train.drop(['id','SMILES','morgan_fingerprint'], axis=1).info() # (3496, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "WNrp5UFLnVnQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1692638669460,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "WNrp5UFLnVnQ",
    "outputId": "c76a7ae9-15e8-4ce6-d87e-c9da15972863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 483 entries, 0 to 482\n",
      "Data columns (total 16 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   id                          483 non-null    object \n",
      " 1   SMILES                      483 non-null    object \n",
      " 2   AlogP                       482 non-null    float64\n",
      " 3   Molecular_Weight            483 non-null    float64\n",
      " 4   Num_H_Acceptors             483 non-null    int64  \n",
      " 5   Num_H_Donors                483 non-null    int64  \n",
      " 6   Num_RotatableBonds          483 non-null    int64  \n",
      " 7   LogD                        483 non-null    float64\n",
      " 8   Molecular_PolarSurfaceArea  483 non-null    float64\n",
      " 9   logP                        483 non-null    float64\n",
      " 10  apka                        483 non-null    float64\n",
      " 11  num_rotatable_bonds         483 non-null    int64  \n",
      " 12  num_heteroatoms             483 non-null    int64  \n",
      " 13  num_hydrogen_acceptors      483 non-null    int64  \n",
      " 14  num_hydrogen_donors         483 non-null    int64  \n",
      " 15  morgan_fingerprint          483 non-null    object \n",
      "dtypes: float64(6), int64(7), object(3)\n",
      "memory usage: 60.5+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "c72ec77f",
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1692638676676,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "c72ec77f"
   },
   "outputs": [],
   "source": [
    "train['AlogP'].fillna(train['AlogP'].median(), inplace=True)\n",
    "test['AlogP'].fillna(test['AlogP'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "CyrKl5mXTYFG",
   "metadata": {
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1692638703756,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "CyrKl5mXTYFG"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, target_col=None, transform=None, is_test=False):\n",
    "        self.is_test = is_test\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.data = data.drop(['id', 'SMILES', 'morgan_fingerprint','MLM', 'HLM'], axis=1)\n",
    "        else: # test\n",
    "            self.data = data.drop(['id', 'SMILES', 'morgan_fingerprint'], axis=1)\n",
    "\n",
    "\n",
    "        if self.transform is not None and not self.is_test:  # 훈련 데이터에만 fit_transform 적용\n",
    "            self.data = self.transform.fit_transform(self.data)\n",
    "        elif self.transform is not None and self.is_test:  # 테스트 데이터에는 transform만 적용\n",
    "            self.data = self.transform.transform(self.data)\n",
    "\n",
    "        if target_col is not None and not self.is_test:\n",
    "            self.target = data[target_col]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.data[index]\n",
    "\n",
    "        if hasattr(self, 'target'):\n",
    "            target = self.target[index]\n",
    "            return torch.tensor(features).to(device).float(), torch.tensor(target).to(device).float().unsqueeze(dim=-1)\n",
    "        else:\n",
    "            return torch.tensor(features).to(device).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "a5dfdf8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1692638704107,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "a5dfdf8b",
    "outputId": "3511a59d-1944-4a79-abde-fb8a76adf4e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = StandardScaler()\n",
    "transform.fit(train.drop(['id','SMILES','morgan_fingerprint', 'MLM', 'HLM'], axis=1))\n",
    "\n",
    "train_MLM = CustomDataset(train, target_col='MLM', transform=transform, is_test=False)\n",
    "train_HLM = CustomDataset(train, target_col='HLM', transform=transform, is_test=False)\n",
    "\n",
    "input_size = train_MLM.data.shape[1]\n",
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e46c90d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1692638707162,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "e46c90d2",
    "outputId": "0a163ca8-8ece-472c-c196-67cfcb5485a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3498, 13)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_HLM.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "78a83026",
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1692638709337,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "78a83026"
   },
   "outputs": [],
   "source": [
    "# train,valid split\n",
    "train_MLM_dataset, valid_MLM_dataset = train_test_split(train_MLM, test_size=0.2, random_state=42)\n",
    "train_HLM_dataset, valid_HLM_dataset = train_test_split(train_HLM, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "b15bd4f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 635,
     "status": "ok",
     "timestamp": 1692638712102,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "b15bd4f4",
    "outputId": "2da4a10d-1e8f-4f7f-8c4a-bb7ab0b416cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([13]), torch.Size([1]))"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_MLM.data[1]).shape, torch.tensor(train['MLM'][1]).float().unsqueeze(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d73f8af6",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1692639095448,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "d73f8af6"
   },
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "CFG = {'BATCH_SIZE': 256,\n",
    "       'EPOCHS': 5000,\n",
    "       'INPUT_SIZE': input_size,\n",
    "       'HIDDEN_SIZE': 256,\n",
    "       'OUTPUT_SIZE': 1,\n",
    "       'DROPOUT_RATE': 0.5,\n",
    "       'LEARNING_RATE': 0.0001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f328f1d7",
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1692639095880,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "f328f1d7"
   },
   "outputs": [],
   "source": [
    "train_MLM_loader = DataLoader(dataset=train_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_MLM_loader = DataLoader(dataset=valid_MLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)\n",
    "\n",
    "\n",
    "train_HLM_loader = DataLoader(dataset=train_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=True)\n",
    "\n",
    "valid_HLM_loader = DataLoader(dataset=valid_HLM_dataset,\n",
    "                              batch_size=CFG['BATCH_SIZE'],\n",
    "                              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "7f29fcb9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1692639096202,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "7f29fcb9",
    "outputId": "918b633e-ee73-4f14-e08d-ad858ffd4d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 13]) torch.Size([256, 1])\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = next(iter(train_MLM_loader))\n",
    "print (X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "bfd4adf6",
   "metadata": {
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1692639097020,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "bfd4adf6"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_rate, out_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "\n",
    "        self.fc_out = nn.Linear(64, out_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc_layers(x)\n",
    "        out = self.fc_out(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "ab4fd7ce",
   "metadata": {
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1692639097850,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "ab4fd7ce"
   },
   "outputs": [],
   "source": [
    "model_MLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])\n",
    "model_HLM = Net(CFG['INPUT_SIZE'],CFG['HIDDEN_SIZE'],CFG['DROPOUT_RATE'],CFG['OUTPUT_SIZE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "941a230e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1692639098587,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "941a230e"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        self.mse = nn.MSELoss()  # 기존의 MSELoss 함수 사용\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        mse_loss = self.mse(output, target)  # 기존의 MSELoss를 계산\n",
    "        rmse_loss = torch.sqrt(mse_loss)  # MSE에 제곱근 씌워 RMSE 계산\n",
    "        return rmse_loss\n",
    "\n",
    "criterion = RMSELoss()\n",
    "optimizer_MLM = torch.optim.Adam(model_MLM.parameters(), lr=CFG['LEARNING_RATE'])\n",
    "optimizer_HLM = torch.optim.Adam(model_HLM.parameters(), lr=CFG['LEARNING_RATE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c8c50632",
   "metadata": {
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1692639099488,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "c8c50632"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, model, criterion, optimizer, epochs, patience=100):\n",
    "    best_valid_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # 모델을 훈련 모드로 설정\n",
    "        running_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()  # 모델을 검증 모드로 설정\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "          for inputs, targets in valid_loader:\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, targets)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        print(f'Epoch: {epoch}/{epochs}, Train Loss: {avg_train_loss}, Valid Loss: {avg_valid_loss}')\n",
    "\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "          best_valid_loss = avg_valid_loss\n",
    "          no_improvement_count = 0\n",
    "          best_model_state = model.state_dict()\n",
    "        else:\n",
    "          no_improvement_count += 1\n",
    "          if no_improvement_count >= patience:\n",
    "            print(f'얼리 스토핑: {patience} 에포크 동안 검증 손실이 향상되지 않음. 에포크 {epoch}에서 훈련 중단.')\n",
    "            break\n",
    "\n",
    "    # 최적의 모델 상태 불러오기\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "7bf20b4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 160168,
     "status": "ok",
     "timestamp": 1692639259970,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "7bf20b4b",
    "outputId": "7737cb93-f853-4402-99ed-d05040f91b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start: MLM\n",
      "Epoch: 0/5000, Train Loss: 51.5286303433505, Valid Loss: 52.453792572021484\n",
      "Epoch: 1/5000, Train Loss: 51.486658963290125, Valid Loss: 52.44477335611979\n",
      "Epoch: 2/5000, Train Loss: 51.41654586791992, Valid Loss: 52.42379887898763\n",
      "Epoch: 3/5000, Train Loss: 51.38082573630593, Valid Loss: 52.397701263427734\n",
      "Epoch: 4/5000, Train Loss: 51.310728246515446, Valid Loss: 52.36537170410156\n",
      "Epoch: 5/5000, Train Loss: 51.31650751287287, Valid Loss: 52.333168029785156\n",
      "Epoch: 6/5000, Train Loss: 51.21786325628107, Valid Loss: 52.29958979288737\n",
      "Epoch: 7/5000, Train Loss: 51.183260137384586, Valid Loss: 52.26835632324219\n",
      "Epoch: 8/5000, Train Loss: 51.19061279296875, Valid Loss: 52.230621337890625\n",
      "Epoch: 9/5000, Train Loss: 51.114615353670985, Valid Loss: 52.19485219319662\n",
      "Epoch: 10/5000, Train Loss: 51.06747540560636, Valid Loss: 52.16429901123047\n",
      "Epoch: 11/5000, Train Loss: 51.04846780950373, Valid Loss: 52.13271458943685\n",
      "Epoch: 12/5000, Train Loss: 50.94061626087535, Valid Loss: 52.08517710367838\n",
      "Epoch: 13/5000, Train Loss: 50.957951285622336, Valid Loss: 52.045693715413414\n",
      "Epoch: 14/5000, Train Loss: 50.86723327636719, Valid Loss: 52.01960754394531\n",
      "Epoch: 15/5000, Train Loss: 50.87872383811257, Valid Loss: 51.983079274495445\n",
      "Epoch: 16/5000, Train Loss: 50.84145944768732, Valid Loss: 51.94109598795573\n",
      "Epoch: 17/5000, Train Loss: 50.79286575317383, Valid Loss: 51.890342712402344\n",
      "Epoch: 18/5000, Train Loss: 50.74552119861949, Valid Loss: 51.86382039388021\n",
      "Epoch: 19/5000, Train Loss: 50.69699478149414, Valid Loss: 51.82683817545573\n",
      "Epoch: 20/5000, Train Loss: 50.675197254527696, Valid Loss: 51.7798105875651\n",
      "Epoch: 21/5000, Train Loss: 50.636716322465375, Valid Loss: 51.739410400390625\n",
      "Epoch: 22/5000, Train Loss: 50.59457501498136, Valid Loss: 51.70694478352865\n",
      "Epoch: 23/5000, Train Loss: 50.580196380615234, Valid Loss: 51.669602711995445\n",
      "Epoch: 24/5000, Train Loss: 50.494140278209336, Valid Loss: 51.63742319742838\n",
      "Epoch: 25/5000, Train Loss: 50.47625524347479, Valid Loss: 51.58587773640951\n",
      "Epoch: 26/5000, Train Loss: 50.461539875377305, Valid Loss: 51.536685943603516\n",
      "Epoch: 27/5000, Train Loss: 50.39570444280451, Valid Loss: 51.51455307006836\n",
      "Epoch: 28/5000, Train Loss: 50.36228353326971, Valid Loss: 51.48453140258789\n",
      "Epoch: 29/5000, Train Loss: 50.29437602650035, Valid Loss: 51.43926493326823\n",
      "Epoch: 30/5000, Train Loss: 50.28192832253196, Valid Loss: 51.41358311971029\n",
      "Epoch: 31/5000, Train Loss: 50.286351290616125, Valid Loss: 51.38443374633789\n",
      "Epoch: 32/5000, Train Loss: 50.20995330810547, Valid Loss: 51.31702677408854\n",
      "Epoch: 33/5000, Train Loss: 50.19852967695756, Valid Loss: 51.31267801920573\n",
      "Epoch: 34/5000, Train Loss: 50.15204446965998, Valid Loss: 51.28045145670573\n",
      "Epoch: 35/5000, Train Loss: 50.14554942737926, Valid Loss: 51.21135965983073\n",
      "Epoch: 36/5000, Train Loss: 50.04472385753285, Valid Loss: 51.22088114420573\n",
      "Epoch: 37/5000, Train Loss: 50.06416112726385, Valid Loss: 51.187103271484375\n",
      "Epoch: 38/5000, Train Loss: 50.030465906316586, Valid Loss: 51.13550313313802\n",
      "Epoch: 39/5000, Train Loss: 49.96346386996183, Valid Loss: 51.09782028198242\n",
      "Epoch: 40/5000, Train Loss: 49.91417347301137, Valid Loss: 51.06441752115885\n",
      "Epoch: 41/5000, Train Loss: 49.8967944058505, Valid Loss: 51.04782358805338\n",
      "Epoch: 42/5000, Train Loss: 49.865923101251774, Valid Loss: 50.99101765950521\n",
      "Epoch: 43/5000, Train Loss: 49.80035261674361, Valid Loss: 50.948682149251304\n",
      "Epoch: 44/5000, Train Loss: 49.870709505948156, Valid Loss: 50.93862660725912\n",
      "Epoch: 45/5000, Train Loss: 49.78522630171342, Valid Loss: 50.86487833658854\n",
      "Epoch: 46/5000, Train Loss: 49.71300784024325, Valid Loss: 50.84496943155924\n",
      "Epoch: 47/5000, Train Loss: 49.69894721291282, Valid Loss: 50.84928639729818\n",
      "Epoch: 48/5000, Train Loss: 49.64892508766868, Valid Loss: 50.78256607055664\n",
      "Epoch: 49/5000, Train Loss: 49.53798710216176, Valid Loss: 50.74252446492513\n",
      "Epoch: 50/5000, Train Loss: 49.59406661987305, Valid Loss: 50.71509679158529\n",
      "Epoch: 51/5000, Train Loss: 49.52597392689098, Valid Loss: 50.69672520955404\n",
      "Epoch: 52/5000, Train Loss: 49.44933180375533, Valid Loss: 50.62563451131185\n",
      "Epoch: 53/5000, Train Loss: 49.44813710992987, Valid Loss: 50.60786437988281\n",
      "Epoch: 54/5000, Train Loss: 49.402527895840734, Valid Loss: 50.53547286987305\n",
      "Epoch: 55/5000, Train Loss: 49.37181091308594, Valid Loss: 50.50432332356771\n",
      "Epoch: 56/5000, Train Loss: 49.276368227871984, Valid Loss: 50.49976094563802\n",
      "Epoch: 57/5000, Train Loss: 49.271364038640804, Valid Loss: 50.4363161722819\n",
      "Epoch: 58/5000, Train Loss: 49.26025182550604, Valid Loss: 50.41281509399414\n",
      "Epoch: 59/5000, Train Loss: 49.15743637084961, Valid Loss: 50.34404500325521\n",
      "Epoch: 60/5000, Train Loss: 49.13579351251776, Valid Loss: 50.324938456217446\n",
      "Epoch: 61/5000, Train Loss: 49.13909010453658, Valid Loss: 50.28287251790365\n",
      "Epoch: 62/5000, Train Loss: 49.0591593655673, Valid Loss: 50.25980885823568\n",
      "Epoch: 63/5000, Train Loss: 49.02682772549716, Valid Loss: 50.20354207356771\n",
      "Epoch: 64/5000, Train Loss: 49.018448222767226, Valid Loss: 50.15980021158854\n",
      "Epoch: 65/5000, Train Loss: 49.00908106023615, Valid Loss: 50.12657928466797\n",
      "Epoch: 66/5000, Train Loss: 48.89650622281161, Valid Loss: 50.06426493326823\n",
      "Epoch: 67/5000, Train Loss: 48.890967629172586, Valid Loss: 50.00558090209961\n",
      "Epoch: 68/5000, Train Loss: 48.83551614934748, Valid Loss: 50.00103251139323\n",
      "Epoch: 69/5000, Train Loss: 48.727940646084875, Valid Loss: 49.937416076660156\n",
      "Epoch: 70/5000, Train Loss: 48.71278242631392, Valid Loss: 49.9011599222819\n",
      "Epoch: 71/5000, Train Loss: 48.67409758134322, Valid Loss: 49.86797078450521\n",
      "Epoch: 72/5000, Train Loss: 48.70194764570756, Valid Loss: 49.84141286214193\n",
      "Epoch: 73/5000, Train Loss: 48.562171936035156, Valid Loss: 49.792744954427086\n",
      "Epoch: 74/5000, Train Loss: 48.564766970547765, Valid Loss: 49.75868352254232\n",
      "Epoch: 75/5000, Train Loss: 48.53381139581854, Valid Loss: 49.67593892415365\n",
      "Epoch: 76/5000, Train Loss: 48.50252082131126, Valid Loss: 49.67031478881836\n",
      "Epoch: 77/5000, Train Loss: 48.508052132346414, Valid Loss: 49.61662801106771\n",
      "Epoch: 78/5000, Train Loss: 48.381439208984375, Valid Loss: 49.58116785685221\n",
      "Epoch: 79/5000, Train Loss: 48.32818326083097, Valid Loss: 49.50787480672201\n",
      "Epoch: 80/5000, Train Loss: 48.30773787064986, Valid Loss: 49.49277877807617\n",
      "Epoch: 81/5000, Train Loss: 48.26735617897727, Valid Loss: 49.439884185791016\n",
      "Epoch: 82/5000, Train Loss: 48.2554106278853, Valid Loss: 49.40830993652344\n",
      "Epoch: 83/5000, Train Loss: 48.2077785838734, Valid Loss: 49.396324157714844\n",
      "Epoch: 84/5000, Train Loss: 48.14951705932617, Valid Loss: 49.38726043701172\n",
      "Epoch: 85/5000, Train Loss: 48.12671592018821, Valid Loss: 49.31322352091471\n",
      "Epoch: 86/5000, Train Loss: 48.09377947720614, Valid Loss: 49.2866096496582\n",
      "Epoch: 87/5000, Train Loss: 47.933167544278234, Valid Loss: 49.25220235188802\n",
      "Epoch: 88/5000, Train Loss: 47.9594841003418, Valid Loss: 49.18415069580078\n",
      "Epoch: 89/5000, Train Loss: 47.82248583706942, Valid Loss: 49.0908203125\n",
      "Epoch: 90/5000, Train Loss: 47.83238844438033, Valid Loss: 49.05908075968424\n",
      "Epoch: 91/5000, Train Loss: 47.86978947032582, Valid Loss: 49.01655069986979\n",
      "Epoch: 92/5000, Train Loss: 47.77990930730646, Valid Loss: 48.99804178873698\n",
      "Epoch: 93/5000, Train Loss: 47.66284907947887, Valid Loss: 48.94499969482422\n",
      "Epoch: 94/5000, Train Loss: 47.68914656205611, Valid Loss: 48.931705474853516\n",
      "Epoch: 95/5000, Train Loss: 47.531681060791016, Valid Loss: 48.84483083089193\n",
      "Epoch: 96/5000, Train Loss: 47.53959135575728, Valid Loss: 48.806287129720054\n",
      "Epoch: 97/5000, Train Loss: 47.543244795365766, Valid Loss: 48.809000651041664\n",
      "Epoch: 98/5000, Train Loss: 47.48288484053178, Valid Loss: 48.78102111816406\n",
      "Epoch: 99/5000, Train Loss: 47.40569652210582, Valid Loss: 48.700705210367836\n",
      "Epoch: 100/5000, Train Loss: 47.36245068636808, Valid Loss: 48.61846796671549\n",
      "Epoch: 101/5000, Train Loss: 47.34909439086914, Valid Loss: 48.60097122192383\n",
      "Epoch: 102/5000, Train Loss: 47.26351200450551, Valid Loss: 48.594008127848305\n",
      "Epoch: 103/5000, Train Loss: 47.26627800681374, Valid Loss: 48.50058237711588\n",
      "Epoch: 104/5000, Train Loss: 47.16593759710138, Valid Loss: 48.47545496622721\n",
      "Epoch: 105/5000, Train Loss: 47.10131974653764, Valid Loss: 48.440179189046226\n",
      "Epoch: 106/5000, Train Loss: 47.084569757634945, Valid Loss: 48.354461669921875\n",
      "Epoch: 107/5000, Train Loss: 47.020503997802734, Valid Loss: 48.29663213094076\n",
      "Epoch: 108/5000, Train Loss: 47.066180142489344, Valid Loss: 48.29606374104818\n",
      "Epoch: 109/5000, Train Loss: 46.93571610884233, Valid Loss: 48.25909296671549\n",
      "Epoch: 110/5000, Train Loss: 46.844063845547765, Valid Loss: 48.21368535359701\n",
      "Epoch: 111/5000, Train Loss: 46.86853235418146, Valid Loss: 48.177740732828774\n",
      "Epoch: 112/5000, Train Loss: 46.71086189963601, Valid Loss: 48.079837799072266\n",
      "Epoch: 113/5000, Train Loss: 46.7719331221147, Valid Loss: 48.00458653767904\n",
      "Epoch: 114/5000, Train Loss: 46.70215398615057, Valid Loss: 47.98109690348307\n",
      "Epoch: 115/5000, Train Loss: 46.60219192504883, Valid Loss: 47.92770004272461\n",
      "Epoch: 116/5000, Train Loss: 46.58438457142223, Valid Loss: 47.90990320841471\n",
      "Epoch: 117/5000, Train Loss: 46.50627691095526, Valid Loss: 47.84413528442383\n",
      "Epoch: 118/5000, Train Loss: 46.421456423672765, Valid Loss: 47.7355842590332\n",
      "Epoch: 119/5000, Train Loss: 46.36869257146662, Valid Loss: 47.73977788289388\n",
      "Epoch: 120/5000, Train Loss: 46.308439774946734, Valid Loss: 47.70169576009115\n",
      "Epoch: 121/5000, Train Loss: 46.38965329256925, Valid Loss: 47.63091023763021\n",
      "Epoch: 122/5000, Train Loss: 46.28319826993075, Valid Loss: 47.62317403157552\n",
      "Epoch: 123/5000, Train Loss: 46.15396291559393, Valid Loss: 47.54731877644857\n",
      "Epoch: 124/5000, Train Loss: 46.149846510453656, Valid Loss: 47.53003438313802\n",
      "Epoch: 125/5000, Train Loss: 46.13542487404563, Valid Loss: 47.472582499186196\n",
      "Epoch: 126/5000, Train Loss: 46.08289822665128, Valid Loss: 47.444052378336586\n",
      "Epoch: 127/5000, Train Loss: 45.99684004350142, Valid Loss: 47.368340810139976\n",
      "Epoch: 128/5000, Train Loss: 45.97617790915749, Valid Loss: 47.31689325968424\n",
      "Epoch: 129/5000, Train Loss: 45.92703143033114, Valid Loss: 47.28231302897135\n",
      "Epoch: 130/5000, Train Loss: 45.839346798983485, Valid Loss: 47.24807484944662\n",
      "Epoch: 131/5000, Train Loss: 45.848084536465734, Valid Loss: 47.238101959228516\n",
      "Epoch: 132/5000, Train Loss: 45.78283275257457, Valid Loss: 47.225781758626304\n",
      "Epoch: 133/5000, Train Loss: 45.79998189752752, Valid Loss: 47.06639099121094\n",
      "Epoch: 134/5000, Train Loss: 45.60068650679155, Valid Loss: 47.05282465616862\n",
      "Epoch: 135/5000, Train Loss: 45.575679085471414, Valid Loss: 46.97848002115885\n",
      "Epoch: 136/5000, Train Loss: 45.54203068126332, Valid Loss: 46.923728942871094\n",
      "Epoch: 137/5000, Train Loss: 45.45531671697443, Valid Loss: 46.87807846069336\n",
      "Epoch: 138/5000, Train Loss: 45.43882127241655, Valid Loss: 46.85925038655599\n",
      "Epoch: 139/5000, Train Loss: 45.34935448386452, Valid Loss: 46.749743143717446\n",
      "Epoch: 140/5000, Train Loss: 45.277190468528055, Valid Loss: 46.68502934773763\n",
      "Epoch: 141/5000, Train Loss: 45.24860347401012, Valid Loss: 46.66188430786133\n",
      "Epoch: 142/5000, Train Loss: 45.22836026278409, Valid Loss: 46.63091532389323\n",
      "Epoch: 143/5000, Train Loss: 45.04487679221413, Valid Loss: 46.58433405558268\n",
      "Epoch: 144/5000, Train Loss: 44.996276508678086, Valid Loss: 46.488904317220054\n",
      "Epoch: 145/5000, Train Loss: 45.06171070445668, Valid Loss: 46.46166102091471\n",
      "Epoch: 146/5000, Train Loss: 45.03548535433683, Valid Loss: 46.39810053507487\n",
      "Epoch: 147/5000, Train Loss: 44.96293952248313, Valid Loss: 46.354681650797524\n",
      "Epoch: 148/5000, Train Loss: 44.94906373457475, Valid Loss: 46.20753860473633\n",
      "Epoch: 149/5000, Train Loss: 44.818845922296696, Valid Loss: 46.21558507283529\n",
      "Epoch: 150/5000, Train Loss: 44.76203987815163, Valid Loss: 46.14546330769857\n",
      "Epoch: 151/5000, Train Loss: 44.69432067871094, Valid Loss: 46.137349446614586\n",
      "Epoch: 152/5000, Train Loss: 44.70239049738104, Valid Loss: 46.03657786051432\n",
      "Epoch: 153/5000, Train Loss: 44.51328242908824, Valid Loss: 46.025390625\n",
      "Epoch: 154/5000, Train Loss: 44.62107537009499, Valid Loss: 46.00286102294922\n",
      "Epoch: 155/5000, Train Loss: 44.58409118652344, Valid Loss: 45.94265492757162\n",
      "Epoch: 156/5000, Train Loss: 44.50723613392223, Valid Loss: 45.88693364461263\n",
      "Epoch: 157/5000, Train Loss: 44.38107403841886, Valid Loss: 45.8744150797526\n",
      "Epoch: 158/5000, Train Loss: 44.43806249445135, Valid Loss: 45.784610748291016\n",
      "Epoch: 159/5000, Train Loss: 44.284276095303625, Valid Loss: 45.7900390625\n",
      "Epoch: 160/5000, Train Loss: 44.23918325250799, Valid Loss: 45.67704645792643\n",
      "Epoch: 161/5000, Train Loss: 44.196776303378016, Valid Loss: 45.58720779418945\n",
      "Epoch: 162/5000, Train Loss: 44.182982358065516, Valid Loss: 45.61371103922526\n",
      "Epoch: 163/5000, Train Loss: 44.0955397865989, Valid Loss: 45.53255716959635\n",
      "Epoch: 164/5000, Train Loss: 44.00594711303711, Valid Loss: 45.499603271484375\n",
      "Epoch: 165/5000, Train Loss: 43.92149630459872, Valid Loss: 45.398033142089844\n",
      "Epoch: 166/5000, Train Loss: 43.94048344005238, Valid Loss: 45.451822916666664\n",
      "Epoch: 167/5000, Train Loss: 43.85852813720703, Valid Loss: 45.40908686319987\n",
      "Epoch: 168/5000, Train Loss: 43.7172157981179, Valid Loss: 45.31163024902344\n",
      "Epoch: 169/5000, Train Loss: 43.76645417646928, Valid Loss: 45.20805867513021\n",
      "Epoch: 170/5000, Train Loss: 43.66979009454901, Valid Loss: 45.18074417114258\n",
      "Epoch: 171/5000, Train Loss: 43.50515434958718, Valid Loss: 45.05602773030599\n",
      "Epoch: 172/5000, Train Loss: 43.580895857377485, Valid Loss: 45.03823598225912\n",
      "Epoch: 173/5000, Train Loss: 43.511357047341086, Valid Loss: 44.97991434733073\n",
      "Epoch: 174/5000, Train Loss: 43.48515562577681, Valid Loss: 44.90434773763021\n",
      "Epoch: 175/5000, Train Loss: 43.331478812477805, Valid Loss: 44.91165542602539\n",
      "Epoch: 176/5000, Train Loss: 43.36192599209872, Valid Loss: 44.860755920410156\n",
      "Epoch: 177/5000, Train Loss: 43.39926494251598, Valid Loss: 44.84388987223307\n",
      "Epoch: 178/5000, Train Loss: 43.199317238547586, Valid Loss: 44.72895304361979\n",
      "Epoch: 179/5000, Train Loss: 43.17804995450106, Valid Loss: 44.703470865885414\n",
      "Epoch: 180/5000, Train Loss: 43.122312719171696, Valid Loss: 44.6335817972819\n",
      "Epoch: 181/5000, Train Loss: 43.02399791370738, Valid Loss: 44.59135437011719\n",
      "Epoch: 182/5000, Train Loss: 43.02234510941939, Valid Loss: 44.481215159098305\n",
      "Epoch: 183/5000, Train Loss: 42.929729808460586, Valid Loss: 44.51856486002604\n",
      "Epoch: 184/5000, Train Loss: 42.856117248535156, Valid Loss: 44.431583404541016\n",
      "Epoch: 185/5000, Train Loss: 42.83596697720614, Valid Loss: 44.37794876098633\n",
      "Epoch: 186/5000, Train Loss: 42.82593605735085, Valid Loss: 44.38901392618815\n",
      "Epoch: 187/5000, Train Loss: 42.77106059681285, Valid Loss: 44.33476765950521\n",
      "Epoch: 188/5000, Train Loss: 42.64172987504439, Valid Loss: 44.30759048461914\n",
      "Epoch: 189/5000, Train Loss: 42.73972216519442, Valid Loss: 44.183878580729164\n",
      "Epoch: 190/5000, Train Loss: 42.75410114635121, Valid Loss: 44.149061838785805\n",
      "Epoch: 191/5000, Train Loss: 42.40624167702415, Valid Loss: 44.06020736694336\n",
      "Epoch: 192/5000, Train Loss: 42.4166672446511, Valid Loss: 44.031105041503906\n",
      "Epoch: 193/5000, Train Loss: 42.44142601706765, Valid Loss: 43.94157155354818\n",
      "Epoch: 194/5000, Train Loss: 42.38758364590731, Valid Loss: 43.85367965698242\n",
      "Epoch: 195/5000, Train Loss: 42.37360763549805, Valid Loss: 43.7688242594401\n",
      "Epoch: 196/5000, Train Loss: 42.27440192482688, Valid Loss: 43.80232365926107\n",
      "Epoch: 197/5000, Train Loss: 42.163129633123226, Valid Loss: 43.798327128092446\n",
      "Epoch: 198/5000, Train Loss: 41.94556322964755, Valid Loss: 43.71424992879232\n",
      "Epoch: 199/5000, Train Loss: 42.13283226706765, Valid Loss: 43.716328938802086\n",
      "Epoch: 200/5000, Train Loss: 42.09745788574219, Valid Loss: 43.626593271891274\n",
      "Epoch: 201/5000, Train Loss: 41.99704014171254, Valid Loss: 43.63328297932943\n",
      "Epoch: 202/5000, Train Loss: 41.917814081365414, Valid Loss: 43.56362787882487\n",
      "Epoch: 203/5000, Train Loss: 41.92360305786133, Valid Loss: 43.49592081705729\n",
      "Epoch: 204/5000, Train Loss: 41.89251015403054, Valid Loss: 43.35863240559896\n",
      "Epoch: 205/5000, Train Loss: 41.7357212413441, Valid Loss: 43.31500371297201\n",
      "Epoch: 206/5000, Train Loss: 41.72189088301225, Valid Loss: 43.28929138183594\n",
      "Epoch: 207/5000, Train Loss: 41.63027607310902, Valid Loss: 43.24646886189779\n",
      "Epoch: 208/5000, Train Loss: 41.49428315596147, Valid Loss: 43.215624491373696\n",
      "Epoch: 209/5000, Train Loss: 41.50584827769887, Valid Loss: 43.1587282816569\n",
      "Epoch: 210/5000, Train Loss: 41.44786973433061, Valid Loss: 43.07522455851237\n",
      "Epoch: 211/5000, Train Loss: 41.351578105579726, Valid Loss: 43.0464235941569\n",
      "Epoch: 212/5000, Train Loss: 41.181642358953304, Valid Loss: 42.980753580729164\n",
      "Epoch: 213/5000, Train Loss: 41.30122965032404, Valid Loss: 42.901896158854164\n",
      "Epoch: 214/5000, Train Loss: 41.14390876076438, Valid Loss: 42.85960006713867\n",
      "Epoch: 215/5000, Train Loss: 41.06312630393288, Valid Loss: 42.8210703531901\n",
      "Epoch: 216/5000, Train Loss: 41.23981787941673, Valid Loss: 42.814568837483726\n",
      "Epoch: 217/5000, Train Loss: 41.06782219626687, Valid Loss: 42.75723648071289\n",
      "Epoch: 218/5000, Train Loss: 41.02333554354581, Valid Loss: 42.69373067220052\n",
      "Epoch: 219/5000, Train Loss: 41.03088586980646, Valid Loss: 42.64118448893229\n",
      "Epoch: 220/5000, Train Loss: 40.80164198441939, Valid Loss: 42.60167566935221\n",
      "Epoch: 221/5000, Train Loss: 40.74692812832919, Valid Loss: 42.554604848225914\n",
      "Epoch: 222/5000, Train Loss: 40.810522599653765, Valid Loss: 42.462477366129555\n",
      "Epoch: 223/5000, Train Loss: 40.824337699196555, Valid Loss: 42.33275858561198\n",
      "Epoch: 224/5000, Train Loss: 40.73683235862038, Valid Loss: 42.274600982666016\n",
      "Epoch: 225/5000, Train Loss: 40.469388441606, Valid Loss: 42.30550765991211\n",
      "Epoch: 226/5000, Train Loss: 40.481111006303266, Valid Loss: 42.250528971354164\n",
      "Epoch: 227/5000, Train Loss: 40.43331666426225, Valid Loss: 42.1982536315918\n",
      "Epoch: 228/5000, Train Loss: 40.39697334983132, Valid Loss: 42.163883209228516\n",
      "Epoch: 229/5000, Train Loss: 40.43002978238192, Valid Loss: 42.07314936319987\n",
      "Epoch: 230/5000, Train Loss: 40.2505541714755, Valid Loss: 42.02955627441406\n",
      "Epoch: 231/5000, Train Loss: 40.191369490189985, Valid Loss: 41.974063873291016\n",
      "Epoch: 232/5000, Train Loss: 40.19768212058327, Valid Loss: 41.960960388183594\n",
      "Epoch: 233/5000, Train Loss: 40.21692484075373, Valid Loss: 41.90975697835287\n",
      "Epoch: 234/5000, Train Loss: 40.16959485140714, Valid Loss: 41.851147969563804\n",
      "Epoch: 235/5000, Train Loss: 40.156974445689805, Valid Loss: 41.755297342936196\n",
      "Epoch: 236/5000, Train Loss: 39.89977299083363, Valid Loss: 41.7263552347819\n",
      "Epoch: 237/5000, Train Loss: 39.865870389071375, Valid Loss: 41.744712829589844\n",
      "Epoch: 238/5000, Train Loss: 39.91579680009322, Valid Loss: 41.60479863484701\n",
      "Epoch: 239/5000, Train Loss: 39.75046851418235, Valid Loss: 41.58092371622721\n",
      "Epoch: 240/5000, Train Loss: 39.818666284734554, Valid Loss: 41.54828770955404\n",
      "Epoch: 241/5000, Train Loss: 39.77528069236062, Valid Loss: 41.548333485921226\n",
      "Epoch: 242/5000, Train Loss: 39.690345070578836, Valid Loss: 41.48342641194662\n",
      "Epoch: 243/5000, Train Loss: 39.646493044766515, Valid Loss: 41.44823455810547\n",
      "Epoch: 244/5000, Train Loss: 39.62401719526811, Valid Loss: 41.38984807332357\n",
      "Epoch: 245/5000, Train Loss: 39.58907248757102, Valid Loss: 41.294934590657554\n",
      "Epoch: 246/5000, Train Loss: 39.60428237915039, Valid Loss: 41.30067571004232\n",
      "Epoch: 247/5000, Train Loss: 39.46066041426225, Valid Loss: 41.14554977416992\n",
      "Epoch: 248/5000, Train Loss: 39.28301863236861, Valid Loss: 41.078409830729164\n",
      "Epoch: 249/5000, Train Loss: 39.25028679587624, Valid Loss: 41.117993672688804\n",
      "Epoch: 250/5000, Train Loss: 39.18905396894975, Valid Loss: 41.13290659586588\n",
      "Epoch: 251/5000, Train Loss: 39.19472191550515, Valid Loss: 41.07524617513021\n",
      "Epoch: 252/5000, Train Loss: 39.16706674749201, Valid Loss: 40.96133931477865\n",
      "Epoch: 253/5000, Train Loss: 39.01553726196289, Valid Loss: 40.96553421020508\n",
      "Epoch: 254/5000, Train Loss: 39.071663943204015, Valid Loss: 40.826090494791664\n",
      "Epoch: 255/5000, Train Loss: 38.99404248324308, Valid Loss: 40.72380447387695\n",
      "Epoch: 256/5000, Train Loss: 38.857782884077594, Valid Loss: 40.6691042582194\n",
      "Epoch: 257/5000, Train Loss: 38.833069888028234, Valid Loss: 40.689955393473305\n",
      "Epoch: 258/5000, Train Loss: 38.80251693725586, Valid Loss: 40.67410405476888\n",
      "Epoch: 259/5000, Train Loss: 38.739748521284625, Valid Loss: 40.53512954711914\n",
      "Epoch: 260/5000, Train Loss: 38.74580244584517, Valid Loss: 40.535118103027344\n",
      "Epoch: 261/5000, Train Loss: 38.620207353071734, Valid Loss: 40.431708017985024\n",
      "Epoch: 262/5000, Train Loss: 38.559831792658024, Valid Loss: 40.39829889933268\n",
      "Epoch: 263/5000, Train Loss: 38.63973132046786, Valid Loss: 40.343588511149086\n",
      "Epoch: 264/5000, Train Loss: 38.52422367442738, Valid Loss: 40.41972986857096\n",
      "Epoch: 265/5000, Train Loss: 38.552195115522906, Valid Loss: 40.3665885925293\n",
      "Epoch: 266/5000, Train Loss: 38.41098299893466, Valid Loss: 40.17060089111328\n",
      "Epoch: 267/5000, Train Loss: 38.45541693947532, Valid Loss: 40.0713857014974\n",
      "Epoch: 268/5000, Train Loss: 38.2394492409446, Valid Loss: 40.12040710449219\n",
      "Epoch: 269/5000, Train Loss: 38.14372530850497, Valid Loss: 40.15650304158529\n",
      "Epoch: 270/5000, Train Loss: 38.267254916104406, Valid Loss: 39.9328244527181\n",
      "Epoch: 271/5000, Train Loss: 38.064115350896664, Valid Loss: 39.94535573323568\n",
      "Epoch: 272/5000, Train Loss: 38.19217959317294, Valid Loss: 39.922054290771484\n",
      "Epoch: 273/5000, Train Loss: 37.962167566472836, Valid Loss: 39.94727452596029\n",
      "Epoch: 274/5000, Train Loss: 37.846201463179156, Valid Loss: 39.893114725748696\n",
      "Epoch: 275/5000, Train Loss: 37.950683940540664, Valid Loss: 39.773810068766274\n",
      "Epoch: 276/5000, Train Loss: 37.80171168934215, Valid Loss: 39.72505569458008\n",
      "Epoch: 277/5000, Train Loss: 37.929899735884234, Valid Loss: 39.696667989095054\n",
      "Epoch: 278/5000, Train Loss: 37.852138172496446, Valid Loss: 39.699843088785805\n",
      "Epoch: 279/5000, Train Loss: 37.796843095259234, Valid Loss: 39.57771301269531\n",
      "Epoch: 280/5000, Train Loss: 37.730460080233485, Valid Loss: 39.478023529052734\n",
      "Epoch: 281/5000, Train Loss: 37.58241930874911, Valid Loss: 39.498488108317055\n",
      "Epoch: 282/5000, Train Loss: 37.60539869828658, Valid Loss: 39.52993138631185\n",
      "Epoch: 283/5000, Train Loss: 37.58106197010387, Valid Loss: 39.411651611328125\n",
      "Epoch: 284/5000, Train Loss: 37.6273640719327, Valid Loss: 39.37048085530599\n",
      "Epoch: 285/5000, Train Loss: 37.537348313765094, Valid Loss: 39.243064880371094\n",
      "Epoch: 286/5000, Train Loss: 37.31121201948686, Valid Loss: 39.24463653564453\n",
      "Epoch: 287/5000, Train Loss: 37.37735852328214, Valid Loss: 39.16053009033203\n",
      "Epoch: 288/5000, Train Loss: 37.22226229580966, Valid Loss: 39.18075180053711\n",
      "Epoch: 289/5000, Train Loss: 37.14085076071999, Valid Loss: 39.06331888834635\n",
      "Epoch: 290/5000, Train Loss: 37.31235400113192, Valid Loss: 39.06639734903971\n",
      "Epoch: 291/5000, Train Loss: 37.154206709428266, Valid Loss: 38.9860471089681\n",
      "Epoch: 292/5000, Train Loss: 37.08261004361239, Valid Loss: 39.06267547607422\n",
      "Epoch: 293/5000, Train Loss: 36.98686287619851, Valid Loss: 39.036216735839844\n",
      "Epoch: 294/5000, Train Loss: 36.89318431507457, Valid Loss: 38.954461415608726\n",
      "Epoch: 295/5000, Train Loss: 37.0444370616566, Valid Loss: 38.848734537760414\n",
      "Epoch: 296/5000, Train Loss: 37.020536249334164, Valid Loss: 38.84823481241862\n",
      "Epoch: 297/5000, Train Loss: 36.67448564009233, Valid Loss: 38.746212005615234\n",
      "Epoch: 298/5000, Train Loss: 36.79279535466974, Valid Loss: 38.797247568766274\n",
      "Epoch: 299/5000, Train Loss: 36.77233401211825, Valid Loss: 38.75149663289388\n",
      "Epoch: 300/5000, Train Loss: 36.635123166170985, Valid Loss: 38.625152587890625\n",
      "Epoch: 301/5000, Train Loss: 36.59341604059393, Valid Loss: 38.56514358520508\n",
      "Epoch: 302/5000, Train Loss: 36.82754898071289, Valid Loss: 38.59778340657552\n",
      "Epoch: 303/5000, Train Loss: 36.56049797751687, Valid Loss: 38.52039591471354\n",
      "Epoch: 304/5000, Train Loss: 36.44457279552113, Valid Loss: 38.41819636027018\n",
      "Epoch: 305/5000, Train Loss: 36.395816802978516, Valid Loss: 38.37889734903971\n",
      "Epoch: 306/5000, Train Loss: 36.367855765602805, Valid Loss: 38.4137814839681\n",
      "Epoch: 307/5000, Train Loss: 36.461333361538976, Valid Loss: 38.41093317667643\n",
      "Epoch: 308/5000, Train Loss: 36.35525131225586, Valid Loss: 38.36475626627604\n",
      "Epoch: 309/5000, Train Loss: 36.255738345059484, Valid Loss: 38.23438517252604\n",
      "Epoch: 310/5000, Train Loss: 36.35250264948065, Valid Loss: 38.27863438924154\n",
      "Epoch: 311/5000, Train Loss: 36.14888520674272, Valid Loss: 38.18350474039713\n",
      "Epoch: 312/5000, Train Loss: 36.03029563210227, Valid Loss: 38.15557734171549\n",
      "Epoch: 313/5000, Train Loss: 36.1643759987571, Valid Loss: 38.06094741821289\n",
      "Epoch: 314/5000, Train Loss: 36.08620002053001, Valid Loss: 37.978319803873696\n",
      "Epoch: 315/5000, Train Loss: 36.12839265303178, Valid Loss: 38.001565297444664\n",
      "Epoch: 316/5000, Train Loss: 36.01103834672408, Valid Loss: 37.91026178995768\n",
      "Epoch: 317/5000, Train Loss: 35.92916349931197, Valid Loss: 37.88147226969401\n",
      "Epoch: 318/5000, Train Loss: 35.859154441139914, Valid Loss: 37.80523173014323\n",
      "Epoch: 319/5000, Train Loss: 35.95605746182528, Valid Loss: 37.74796803792318\n",
      "Epoch: 320/5000, Train Loss: 35.7609187039462, Valid Loss: 37.692813873291016\n",
      "Epoch: 321/5000, Train Loss: 35.80515809492631, Valid Loss: 37.73801040649414\n",
      "Epoch: 322/5000, Train Loss: 35.68975899436257, Valid Loss: 37.67102559407552\n",
      "Epoch: 323/5000, Train Loss: 35.842464447021484, Valid Loss: 37.68624623616537\n",
      "Epoch: 324/5000, Train Loss: 35.54659375277433, Valid Loss: 37.61371103922526\n",
      "Epoch: 325/5000, Train Loss: 35.60922622680664, Valid Loss: 37.546374003092446\n",
      "Epoch: 326/5000, Train Loss: 35.58038399436257, Valid Loss: 37.53334426879883\n",
      "Epoch: 327/5000, Train Loss: 35.483773318204015, Valid Loss: 37.47189585367838\n",
      "Epoch: 328/5000, Train Loss: 35.4396147294478, Valid Loss: 37.46149571736654\n",
      "Epoch: 329/5000, Train Loss: 35.51060971346769, Valid Loss: 37.40255228678385\n",
      "Epoch: 330/5000, Train Loss: 35.5472973910245, Valid Loss: 37.4399668375651\n",
      "Epoch: 331/5000, Train Loss: 35.416389812122695, Valid Loss: 37.35334904988607\n",
      "Epoch: 332/5000, Train Loss: 35.14497965032404, Valid Loss: 37.25121815999349\n",
      "Epoch: 333/5000, Train Loss: 35.098364396528765, Valid Loss: 37.214822133382164\n",
      "Epoch: 334/5000, Train Loss: 35.12206198952415, Valid Loss: 37.1929562886556\n",
      "Epoch: 335/5000, Train Loss: 34.962452455000445, Valid Loss: 37.22280629475912\n",
      "Epoch: 336/5000, Train Loss: 35.04349223050204, Valid Loss: 36.99952189127604\n",
      "Epoch: 337/5000, Train Loss: 35.282010858709164, Valid Loss: 37.08143997192383\n",
      "Epoch: 338/5000, Train Loss: 34.99477178400213, Valid Loss: 37.07775370279948\n",
      "Epoch: 339/5000, Train Loss: 35.11773959073153, Valid Loss: 37.07176081339518\n",
      "Epoch: 340/5000, Train Loss: 34.67673804543235, Valid Loss: 37.031532287597656\n",
      "Epoch: 341/5000, Train Loss: 34.68317898837003, Valid Loss: 36.94772593180338\n",
      "Epoch: 342/5000, Train Loss: 34.868681474165484, Valid Loss: 36.90947596232096\n",
      "Epoch: 343/5000, Train Loss: 34.82583722201261, Valid Loss: 36.94404729207357\n",
      "Epoch: 344/5000, Train Loss: 34.83583450317383, Valid Loss: 36.80386606852213\n",
      "Epoch: 345/5000, Train Loss: 34.91249084472656, Valid Loss: 36.82377497355143\n",
      "Epoch: 346/5000, Train Loss: 34.7164644761519, Valid Loss: 36.71011098225912\n",
      "Epoch: 347/5000, Train Loss: 34.59904115850275, Valid Loss: 36.7627067565918\n",
      "Epoch: 348/5000, Train Loss: 34.72487050836737, Valid Loss: 36.67495854695638\n",
      "Epoch: 349/5000, Train Loss: 34.70805983109908, Valid Loss: 36.71343231201172\n",
      "Epoch: 350/5000, Train Loss: 34.44766824895685, Valid Loss: 36.589429219563804\n",
      "Epoch: 351/5000, Train Loss: 34.54446879300204, Valid Loss: 36.56114705403646\n",
      "Epoch: 352/5000, Train Loss: 34.4750695662065, Valid Loss: 36.517739613850914\n",
      "Epoch: 353/5000, Train Loss: 34.55390600724654, Valid Loss: 36.533888498942055\n",
      "Epoch: 354/5000, Train Loss: 34.412072441794656, Valid Loss: 36.497352600097656\n",
      "Epoch: 355/5000, Train Loss: 34.38179189508612, Valid Loss: 36.46795527140299\n",
      "Epoch: 356/5000, Train Loss: 34.4962966225364, Valid Loss: 36.44916661580404\n",
      "Epoch: 357/5000, Train Loss: 34.20800226384943, Valid Loss: 36.41530990600586\n",
      "Epoch: 358/5000, Train Loss: 34.27097095142711, Valid Loss: 36.34008280436198\n",
      "Epoch: 359/5000, Train Loss: 34.49366500160911, Valid Loss: 36.41201527913412\n",
      "Epoch: 360/5000, Train Loss: 34.181290019642226, Valid Loss: 36.30673472086588\n",
      "Epoch: 361/5000, Train Loss: 34.18554201993075, Valid Loss: 36.244458516438804\n",
      "Epoch: 362/5000, Train Loss: 34.23913782293146, Valid Loss: 36.24355697631836\n",
      "Epoch: 363/5000, Train Loss: 34.17939342151988, Valid Loss: 36.22005971272787\n",
      "Epoch: 364/5000, Train Loss: 34.021330920132726, Valid Loss: 36.1844596862793\n",
      "Epoch: 365/5000, Train Loss: 34.067630941217594, Valid Loss: 36.11824162801107\n",
      "Epoch: 366/5000, Train Loss: 33.8107795715332, Valid Loss: 36.12033716837565\n",
      "Epoch: 367/5000, Train Loss: 33.796645944768734, Valid Loss: 36.116224924723305\n",
      "Epoch: 368/5000, Train Loss: 33.91667383367365, Valid Loss: 36.04403813680013\n",
      "Epoch: 369/5000, Train Loss: 34.05217812278054, Valid Loss: 36.001583099365234\n",
      "Epoch: 370/5000, Train Loss: 33.71996636824174, Valid Loss: 35.97186787923177\n",
      "Epoch: 371/5000, Train Loss: 33.90444686196067, Valid Loss: 35.95927302042643\n",
      "Epoch: 372/5000, Train Loss: 33.90763612227006, Valid Loss: 35.900864919026695\n",
      "Epoch: 373/5000, Train Loss: 33.72007734125311, Valid Loss: 35.80578740437826\n",
      "Epoch: 374/5000, Train Loss: 33.706668333573774, Valid Loss: 35.78536860148112\n",
      "Epoch: 375/5000, Train Loss: 33.66849119013006, Valid Loss: 35.75547663370768\n",
      "Epoch: 376/5000, Train Loss: 33.78319549560547, Valid Loss: 35.72488911946615\n",
      "Epoch: 377/5000, Train Loss: 33.667553121393375, Valid Loss: 35.71487045288086\n",
      "Epoch: 378/5000, Train Loss: 33.582462310791016, Valid Loss: 35.68134562174479\n",
      "Epoch: 379/5000, Train Loss: 33.58509133078835, Valid Loss: 35.67701085408529\n",
      "Epoch: 380/5000, Train Loss: 33.44920661232688, Valid Loss: 35.61575190226237\n",
      "Epoch: 381/5000, Train Loss: 33.53854699568315, Valid Loss: 35.66033681233724\n",
      "Epoch: 382/5000, Train Loss: 33.49473016912287, Valid Loss: 35.65179443359375\n",
      "Epoch: 383/5000, Train Loss: 33.3296122117476, Valid Loss: 35.609639485677086\n",
      "Epoch: 384/5000, Train Loss: 33.36492625149813, Valid Loss: 35.58576965332031\n",
      "Epoch: 385/5000, Train Loss: 33.41372073780406, Valid Loss: 35.548177083333336\n",
      "Epoch: 386/5000, Train Loss: 33.402298320423476, Valid Loss: 35.49313990275065\n",
      "Epoch: 387/5000, Train Loss: 33.342078642411664, Valid Loss: 35.508477528889976\n",
      "Epoch: 388/5000, Train Loss: 33.44545156305487, Valid Loss: 35.50025431315104\n",
      "Epoch: 389/5000, Train Loss: 33.30914323980158, Valid Loss: 35.37914276123047\n",
      "Epoch: 390/5000, Train Loss: 33.31597397544167, Valid Loss: 35.36332702636719\n",
      "Epoch: 391/5000, Train Loss: 33.12870251048695, Valid Loss: 35.285107930501304\n",
      "Epoch: 392/5000, Train Loss: 33.0800105008212, Valid Loss: 35.23858133951823\n",
      "Epoch: 393/5000, Train Loss: 33.172829541293055, Valid Loss: 35.27678934733073\n",
      "Epoch: 394/5000, Train Loss: 33.02877928993919, Valid Loss: 35.321250915527344\n",
      "Epoch: 395/5000, Train Loss: 33.11559469049627, Valid Loss: 35.23493194580078\n",
      "Epoch: 396/5000, Train Loss: 33.10464373501864, Valid Loss: 35.114131927490234\n",
      "Epoch: 397/5000, Train Loss: 32.78963574496183, Valid Loss: 35.150516510009766\n",
      "Epoch: 398/5000, Train Loss: 33.20873815363104, Valid Loss: 35.071818033854164\n",
      "Epoch: 399/5000, Train Loss: 32.973505366932265, Valid Loss: 35.08360036214193\n",
      "Epoch: 400/5000, Train Loss: 33.108857588334516, Valid Loss: 35.04639180501302\n",
      "Epoch: 401/5000, Train Loss: 33.09138627485795, Valid Loss: 35.08146540323893\n",
      "Epoch: 402/5000, Train Loss: 32.86121108315208, Valid Loss: 35.03895696004232\n",
      "Epoch: 403/5000, Train Loss: 32.86942187222567, Valid Loss: 35.02220153808594\n",
      "Epoch: 404/5000, Train Loss: 32.88505796952681, Valid Loss: 34.931138356526695\n",
      "Epoch: 405/5000, Train Loss: 32.846522244540125, Valid Loss: 34.86907068888346\n",
      "Epoch: 406/5000, Train Loss: 32.81223817305131, Valid Loss: 34.77185185750326\n",
      "Epoch: 407/5000, Train Loss: 32.54925034262917, Valid Loss: 34.773293813069664\n",
      "Epoch: 408/5000, Train Loss: 32.631569082086735, Valid Loss: 34.836594899495445\n",
      "Epoch: 409/5000, Train Loss: 32.81568440524015, Valid Loss: 34.811171213785805\n",
      "Epoch: 410/5000, Train Loss: 32.64367814497514, Valid Loss: 34.77450688680013\n",
      "Epoch: 411/5000, Train Loss: 32.85249918157404, Valid Loss: 34.73967742919922\n",
      "Epoch: 412/5000, Train Loss: 32.47443060441451, Valid Loss: 34.73986689249674\n",
      "Epoch: 413/5000, Train Loss: 32.26679091020064, Valid Loss: 34.694479624430336\n",
      "Epoch: 414/5000, Train Loss: 32.99830523404208, Valid Loss: 34.63582229614258\n",
      "Epoch: 415/5000, Train Loss: 32.502824956720524, Valid Loss: 34.661877950032554\n",
      "Epoch: 416/5000, Train Loss: 32.697243603793055, Valid Loss: 34.636619567871094\n",
      "Epoch: 417/5000, Train Loss: 32.56839284029874, Valid Loss: 34.573899586995445\n",
      "Epoch: 418/5000, Train Loss: 32.19638356295499, Valid Loss: 34.55024337768555\n",
      "Epoch: 419/5000, Train Loss: 32.60567717118697, Valid Loss: 34.498087565104164\n",
      "Epoch: 420/5000, Train Loss: 32.30537345192649, Valid Loss: 34.569933573404946\n",
      "Epoch: 421/5000, Train Loss: 32.440371773459695, Valid Loss: 34.500179290771484\n",
      "Epoch: 422/5000, Train Loss: 32.1970873746005, Valid Loss: 34.531533559163414\n",
      "Epoch: 423/5000, Train Loss: 32.478247209028765, Valid Loss: 34.44142278035482\n",
      "Epoch: 424/5000, Train Loss: 32.289254448630594, Valid Loss: 34.39288075764974\n",
      "Epoch: 425/5000, Train Loss: 32.24280149286444, Valid Loss: 34.409463246663414\n",
      "Epoch: 426/5000, Train Loss: 32.16788447986949, Valid Loss: 34.416969299316406\n",
      "Epoch: 427/5000, Train Loss: 32.23879692771218, Valid Loss: 34.339490254720054\n",
      "Epoch: 428/5000, Train Loss: 32.21340144764293, Valid Loss: 34.32496579488119\n",
      "Epoch: 429/5000, Train Loss: 32.348964344371446, Valid Loss: 34.33900578816732\n",
      "Epoch: 430/5000, Train Loss: 32.172917105934836, Valid Loss: 34.29999287923177\n",
      "Epoch: 431/5000, Train Loss: 32.133840560913086, Valid Loss: 34.2574208577474\n",
      "Epoch: 432/5000, Train Loss: 32.367344942959875, Valid Loss: 34.2651252746582\n",
      "Epoch: 433/5000, Train Loss: 32.22182343222878, Valid Loss: 34.227809270222984\n",
      "Epoch: 434/5000, Train Loss: 32.25489755110307, Valid Loss: 34.19956143697103\n",
      "Epoch: 435/5000, Train Loss: 32.16876775568182, Valid Loss: 34.2188663482666\n",
      "Epoch: 436/5000, Train Loss: 32.14748573303223, Valid Loss: 34.253173192342125\n",
      "Epoch: 437/5000, Train Loss: 32.22746883739125, Valid Loss: 34.1584358215332\n",
      "Epoch: 438/5000, Train Loss: 31.96612514149059, Valid Loss: 34.098977406819664\n",
      "Epoch: 439/5000, Train Loss: 31.96298737959428, Valid Loss: 34.130889892578125\n",
      "Epoch: 440/5000, Train Loss: 31.703990762883965, Valid Loss: 34.08424504597982\n",
      "Epoch: 441/5000, Train Loss: 32.21750675548207, Valid Loss: 34.09199078877767\n",
      "Epoch: 442/5000, Train Loss: 32.14191454107111, Valid Loss: 34.03943761189779\n",
      "Epoch: 443/5000, Train Loss: 32.316842859441586, Valid Loss: 34.06923230489095\n",
      "Epoch: 444/5000, Train Loss: 32.11684313687411, Valid Loss: 34.05617459615072\n",
      "Epoch: 445/5000, Train Loss: 32.134891683405094, Valid Loss: 34.04473686218262\n",
      "Epoch: 446/5000, Train Loss: 31.95453505082564, Valid Loss: 33.93674659729004\n",
      "Epoch: 447/5000, Train Loss: 31.899179111827504, Valid Loss: 34.056722005208336\n",
      "Epoch: 448/5000, Train Loss: 31.801635915582832, Valid Loss: 34.007982889811196\n",
      "Epoch: 449/5000, Train Loss: 32.117944197221235, Valid Loss: 33.940760930379234\n",
      "Epoch: 450/5000, Train Loss: 31.724476033991035, Valid Loss: 33.93969472249349\n",
      "Epoch: 451/5000, Train Loss: 31.8930194161155, Valid Loss: 33.93560663859049\n",
      "Epoch: 452/5000, Train Loss: 32.03630048578436, Valid Loss: 33.84747187296549\n",
      "Epoch: 453/5000, Train Loss: 31.616410862315785, Valid Loss: 33.836995442708336\n",
      "Epoch: 454/5000, Train Loss: 31.566371744329278, Valid Loss: 33.835899353027344\n",
      "Epoch: 455/5000, Train Loss: 31.868173946033824, Valid Loss: 33.8464298248291\n",
      "Epoch: 456/5000, Train Loss: 31.60617620294744, Valid Loss: 33.8668212890625\n",
      "Epoch: 457/5000, Train Loss: 31.79825869473544, Valid Loss: 33.875588734944664\n",
      "Epoch: 458/5000, Train Loss: 31.70833830399947, Valid Loss: 33.837896982828774\n",
      "Epoch: 459/5000, Train Loss: 31.928860924460672, Valid Loss: 33.796765645345054\n",
      "Epoch: 460/5000, Train Loss: 31.632040197199043, Valid Loss: 33.80373954772949\n",
      "Epoch: 461/5000, Train Loss: 31.561953457919035, Valid Loss: 33.72294616699219\n",
      "Epoch: 462/5000, Train Loss: 31.825315475463867, Valid Loss: 33.705827713012695\n",
      "Epoch: 463/5000, Train Loss: 31.872779152610086, Valid Loss: 33.6822706858317\n",
      "Epoch: 464/5000, Train Loss: 31.471796902743254, Valid Loss: 33.669105529785156\n",
      "Epoch: 465/5000, Train Loss: 31.768023924394086, Valid Loss: 33.72375233968099\n",
      "Epoch: 466/5000, Train Loss: 31.445782574740324, Valid Loss: 33.69881629943848\n",
      "Epoch: 467/5000, Train Loss: 31.661236329512164, Valid Loss: 33.7001158396403\n",
      "Epoch: 468/5000, Train Loss: 31.60567387667569, Valid Loss: 33.63235092163086\n",
      "Epoch: 469/5000, Train Loss: 31.629893216219816, Valid Loss: 33.621297200520836\n",
      "Epoch: 470/5000, Train Loss: 31.339637236161664, Valid Loss: 33.60067494710287\n",
      "Epoch: 471/5000, Train Loss: 31.351219177246094, Valid Loss: 33.61629549662272\n",
      "Epoch: 472/5000, Train Loss: 31.57858831232244, Valid Loss: 33.66964975992838\n",
      "Epoch: 473/5000, Train Loss: 31.71497969193892, Valid Loss: 33.57832590738932\n",
      "Epoch: 474/5000, Train Loss: 31.580523924394086, Valid Loss: 33.56096585591634\n",
      "Epoch: 475/5000, Train Loss: 31.51091367548162, Valid Loss: 33.50995635986328\n",
      "Epoch: 476/5000, Train Loss: 31.353777798739348, Valid Loss: 33.50105857849121\n",
      "Epoch: 477/5000, Train Loss: 31.663247195157137, Valid Loss: 33.50320180257162\n",
      "Epoch: 478/5000, Train Loss: 31.42931435324929, Valid Loss: 33.5561326344808\n",
      "Epoch: 479/5000, Train Loss: 31.706817106767133, Valid Loss: 33.48683865865072\n",
      "Epoch: 480/5000, Train Loss: 31.312041195956144, Valid Loss: 33.48486200968424\n",
      "Epoch: 481/5000, Train Loss: 31.457585074684836, Valid Loss: 33.47096252441406\n",
      "Epoch: 482/5000, Train Loss: 31.481903943148527, Valid Loss: 33.4530029296875\n",
      "Epoch: 483/5000, Train Loss: 31.42690190401944, Valid Loss: 33.47396723429362\n",
      "Epoch: 484/5000, Train Loss: 31.120046095414594, Valid Loss: 33.462354024251304\n",
      "Epoch: 485/5000, Train Loss: 31.306262276389383, Valid Loss: 33.475189208984375\n",
      "Epoch: 486/5000, Train Loss: 31.295829252763227, Valid Loss: 33.43900680541992\n",
      "Epoch: 487/5000, Train Loss: 31.141734383322976, Valid Loss: 33.391048431396484\n",
      "Epoch: 488/5000, Train Loss: 31.36443953080611, Valid Loss: 33.45384979248047\n",
      "Epoch: 489/5000, Train Loss: 31.30165516246449, Valid Loss: 33.42414665222168\n",
      "Epoch: 490/5000, Train Loss: 31.503591710870918, Valid Loss: 33.41703351338705\n",
      "Epoch: 491/5000, Train Loss: 31.325768037275836, Valid Loss: 33.36713473002116\n",
      "Epoch: 492/5000, Train Loss: 31.33354377746582, Valid Loss: 33.344308853149414\n",
      "Epoch: 493/5000, Train Loss: 31.091947035356, Valid Loss: 33.32703844706217\n",
      "Epoch: 494/5000, Train Loss: 31.236486955122515, Valid Loss: 33.32018152872721\n",
      "Epoch: 495/5000, Train Loss: 31.066002932461824, Valid Loss: 33.29231516520182\n",
      "Epoch: 496/5000, Train Loss: 31.34299052845348, Valid Loss: 33.28032684326172\n",
      "Epoch: 497/5000, Train Loss: 31.14861956509677, Valid Loss: 33.35814094543457\n",
      "Epoch: 498/5000, Train Loss: 31.31814193725586, Valid Loss: 33.32269159952799\n",
      "Epoch: 499/5000, Train Loss: 31.178711804476652, Valid Loss: 33.30519676208496\n",
      "Epoch: 500/5000, Train Loss: 31.106219725175336, Valid Loss: 33.283145904541016\n",
      "Epoch: 501/5000, Train Loss: 30.92496074329723, Valid Loss: 33.2169386545817\n",
      "Epoch: 502/5000, Train Loss: 31.347943045876242, Valid Loss: 33.22857157389323\n",
      "Epoch: 503/5000, Train Loss: 31.020191366022285, Valid Loss: 33.22744878133138\n",
      "Epoch: 504/5000, Train Loss: 31.184702613136984, Valid Loss: 33.217397689819336\n",
      "Epoch: 505/5000, Train Loss: 31.005981271917168, Valid Loss: 33.19510968526205\n",
      "Epoch: 506/5000, Train Loss: 30.93947237188166, Valid Loss: 33.186265309651695\n",
      "Epoch: 507/5000, Train Loss: 31.300872282548383, Valid Loss: 33.19000244140625\n",
      "Epoch: 508/5000, Train Loss: 31.335502451116387, Valid Loss: 33.19321632385254\n",
      "Epoch: 509/5000, Train Loss: 31.100161639126863, Valid Loss: 33.148128509521484\n",
      "Epoch: 510/5000, Train Loss: 30.8043138330633, Valid Loss: 33.16783587137858\n",
      "Epoch: 511/5000, Train Loss: 31.210174560546875, Valid Loss: 33.15246136983236\n",
      "Epoch: 512/5000, Train Loss: 31.063406684181906, Valid Loss: 33.14906883239746\n",
      "Epoch: 513/5000, Train Loss: 31.018047679554332, Valid Loss: 33.15066782633463\n",
      "Epoch: 514/5000, Train Loss: 30.954280679876153, Valid Loss: 33.15085029602051\n",
      "Epoch: 515/5000, Train Loss: 30.796870318326082, Valid Loss: 33.14081891377767\n",
      "Epoch: 516/5000, Train Loss: 31.214615041559394, Valid Loss: 33.114047368367515\n",
      "Epoch: 517/5000, Train Loss: 31.053103186867453, Valid Loss: 33.07122548421224\n",
      "Epoch: 518/5000, Train Loss: 31.005302255803887, Valid Loss: 33.099343617757164\n",
      "Epoch: 519/5000, Train Loss: 30.846418207341973, Valid Loss: 33.08399963378906\n",
      "Epoch: 520/5000, Train Loss: 30.99485362659801, Valid Loss: 33.084062576293945\n",
      "Epoch: 521/5000, Train Loss: 31.02352541143244, Valid Loss: 33.061591466267906\n",
      "Epoch: 522/5000, Train Loss: 31.000140103426848, Valid Loss: 33.07279968261719\n",
      "Epoch: 523/5000, Train Loss: 31.134146777066317, Valid Loss: 33.08982022603353\n",
      "Epoch: 524/5000, Train Loss: 31.193415208296344, Valid Loss: 33.05299441019694\n",
      "Epoch: 525/5000, Train Loss: 31.23875791376287, Valid Loss: 33.04213205973307\n",
      "Epoch: 526/5000, Train Loss: 30.907351407137785, Valid Loss: 33.0474541982015\n",
      "Epoch: 527/5000, Train Loss: 31.126609455455434, Valid Loss: 33.04171498616537\n",
      "Epoch: 528/5000, Train Loss: 30.77644261446866, Valid Loss: 33.0098819732666\n",
      "Epoch: 529/5000, Train Loss: 30.84872193769975, Valid Loss: 33.007555643717446\n",
      "Epoch: 530/5000, Train Loss: 30.94581517306241, Valid Loss: 32.99847539265951\n",
      "Epoch: 531/5000, Train Loss: 30.734797911210492, Valid Loss: 33.06105359395345\n",
      "Epoch: 532/5000, Train Loss: 30.964614174582742, Valid Loss: 33.008941650390625\n",
      "Epoch: 533/5000, Train Loss: 31.006291129372336, Valid Loss: 33.00972366333008\n",
      "Epoch: 534/5000, Train Loss: 30.844116037542168, Valid Loss: 32.95190175374349\n",
      "Epoch: 535/5000, Train Loss: 31.160736083984375, Valid Loss: 32.98546155293783\n",
      "Epoch: 536/5000, Train Loss: 30.99451411854137, Valid Loss: 32.979601542154946\n",
      "Epoch: 537/5000, Train Loss: 30.972468983043324, Valid Loss: 32.978267669677734\n",
      "Epoch: 538/5000, Train Loss: 30.915450703014027, Valid Loss: 32.97444725036621\n",
      "Epoch: 539/5000, Train Loss: 30.791977969082918, Valid Loss: 32.94419479370117\n",
      "Epoch: 540/5000, Train Loss: 30.817190170288086, Valid Loss: 32.9595750172933\n",
      "Epoch: 541/5000, Train Loss: 30.865211140025746, Valid Loss: 32.94778251647949\n",
      "Epoch: 542/5000, Train Loss: 30.912279302423652, Valid Loss: 32.97736612955729\n",
      "Epoch: 543/5000, Train Loss: 30.992091265591707, Valid Loss: 32.93731180826823\n",
      "Epoch: 544/5000, Train Loss: 30.90569270740856, Valid Loss: 32.93583869934082\n",
      "Epoch: 545/5000, Train Loss: 30.980094736272637, Valid Loss: 32.91632397969564\n",
      "Epoch: 546/5000, Train Loss: 30.79865871776234, Valid Loss: 32.93192481994629\n",
      "Epoch: 547/5000, Train Loss: 30.81576052579013, Valid Loss: 32.92989921569824\n",
      "Epoch: 548/5000, Train Loss: 30.811388189142402, Valid Loss: 32.92688179016113\n",
      "Epoch: 549/5000, Train Loss: 30.77773128856312, Valid Loss: 32.95031929016113\n",
      "Epoch: 550/5000, Train Loss: 30.99867196516557, Valid Loss: 32.93506685892741\n",
      "Epoch: 551/5000, Train Loss: 31.066641374067828, Valid Loss: 32.91835276285807\n",
      "Epoch: 552/5000, Train Loss: 30.955751765858043, Valid Loss: 32.899138768514\n",
      "Epoch: 553/5000, Train Loss: 30.74044383655895, Valid Loss: 32.87858200073242\n",
      "Epoch: 554/5000, Train Loss: 30.90310339494185, Valid Loss: 32.904099782307945\n",
      "Epoch: 555/5000, Train Loss: 30.918617075139824, Valid Loss: 32.888875325520836\n",
      "Epoch: 556/5000, Train Loss: 31.032188068736684, Valid Loss: 32.888299306233726\n",
      "Epoch: 557/5000, Train Loss: 30.812693162397906, Valid Loss: 32.889892578125\n",
      "Epoch: 558/5000, Train Loss: 30.435088591142133, Valid Loss: 32.89931042989095\n",
      "Epoch: 559/5000, Train Loss: 30.736413435502485, Valid Loss: 32.90647951761881\n",
      "Epoch: 560/5000, Train Loss: 30.908133593472567, Valid Loss: 32.91951370239258\n",
      "Epoch: 561/5000, Train Loss: 30.692373969338156, Valid Loss: 32.898831049601235\n",
      "Epoch: 562/5000, Train Loss: 30.502877495505594, Valid Loss: 32.90612983703613\n",
      "Epoch: 563/5000, Train Loss: 30.720166639848188, Valid Loss: 32.89263725280762\n",
      "Epoch: 564/5000, Train Loss: 30.72864480452104, Valid Loss: 32.88433710734049\n",
      "Epoch: 565/5000, Train Loss: 30.608495365489613, Valid Loss: 32.82076517740885\n",
      "Epoch: 566/5000, Train Loss: 30.785537719726562, Valid Loss: 32.8184445699056\n",
      "Epoch: 567/5000, Train Loss: 30.60772809115323, Valid Loss: 32.831278483072914\n",
      "Epoch: 568/5000, Train Loss: 30.634464610706676, Valid Loss: 32.8360341389974\n",
      "Epoch: 569/5000, Train Loss: 30.81978485801003, Valid Loss: 32.80367088317871\n",
      "Epoch: 570/5000, Train Loss: 30.68667117032138, Valid Loss: 32.819396336873375\n",
      "Epoch: 571/5000, Train Loss: 30.79139015891335, Valid Loss: 32.84532864888509\n",
      "Epoch: 572/5000, Train Loss: 30.690493670376863, Valid Loss: 32.832560221354164\n",
      "Epoch: 573/5000, Train Loss: 30.55852473865856, Valid Loss: 32.824958165486656\n",
      "Epoch: 574/5000, Train Loss: 30.57503908330744, Valid Loss: 32.81920178731283\n",
      "Epoch: 575/5000, Train Loss: 30.467553919011895, Valid Loss: 32.79680252075195\n",
      "Epoch: 576/5000, Train Loss: 30.88380276073109, Valid Loss: 32.83529154459635\n",
      "Epoch: 577/5000, Train Loss: 30.52644174749201, Valid Loss: 32.82977485656738\n",
      "Epoch: 578/5000, Train Loss: 30.647239858453926, Valid Loss: 32.7983283996582\n",
      "Epoch: 579/5000, Train Loss: 30.82873431119052, Valid Loss: 32.805975596110024\n",
      "Epoch: 580/5000, Train Loss: 30.593841032548383, Valid Loss: 32.77489153544108\n",
      "Epoch: 581/5000, Train Loss: 30.583304145119406, Valid Loss: 32.76151911417643\n",
      "Epoch: 582/5000, Train Loss: 30.599748611450195, Valid Loss: 32.792547861735024\n",
      "Epoch: 583/5000, Train Loss: 30.61842745000666, Valid Loss: 32.82001177469889\n",
      "Epoch: 584/5000, Train Loss: 30.71822738647461, Valid Loss: 32.797916412353516\n",
      "Epoch: 585/5000, Train Loss: 30.669246153397992, Valid Loss: 32.79800669352213\n",
      "Epoch: 586/5000, Train Loss: 30.811152891679242, Valid Loss: 32.81687609354655\n",
      "Epoch: 587/5000, Train Loss: 30.690350099043414, Valid Loss: 32.78552436828613\n",
      "Epoch: 588/5000, Train Loss: 30.43721493807706, Valid Loss: 32.785637537638344\n",
      "Epoch: 589/5000, Train Loss: 30.478778145530008, Valid Loss: 32.79659207661947\n",
      "Epoch: 590/5000, Train Loss: 30.736956162886187, Valid Loss: 32.78815587361654\n",
      "Epoch: 591/5000, Train Loss: 30.670841217041016, Valid Loss: 32.793317794799805\n",
      "Epoch: 592/5000, Train Loss: 30.505471316250887, Valid Loss: 32.781192779541016\n",
      "Epoch: 593/5000, Train Loss: 30.662242889404297, Valid Loss: 32.79277229309082\n",
      "Epoch: 594/5000, Train Loss: 30.83959336714311, Valid Loss: 32.78439076741537\n",
      "Epoch: 595/5000, Train Loss: 30.522485559636895, Valid Loss: 32.76079877217611\n",
      "Epoch: 596/5000, Train Loss: 30.59433937072754, Valid Loss: 32.74893124898275\n",
      "Epoch: 597/5000, Train Loss: 30.71538734436035, Valid Loss: 32.72505633036295\n",
      "Epoch: 598/5000, Train Loss: 30.443084543401543, Valid Loss: 32.73556900024414\n",
      "Epoch: 599/5000, Train Loss: 30.88594471324574, Valid Loss: 32.736375172932945\n",
      "Epoch: 600/5000, Train Loss: 30.555193987759676, Valid Loss: 32.7438055674235\n",
      "Epoch: 601/5000, Train Loss: 30.667209798639472, Valid Loss: 32.72676912943522\n",
      "Epoch: 602/5000, Train Loss: 30.794030796397816, Valid Loss: 32.74829928080241\n",
      "Epoch: 603/5000, Train Loss: 30.67561149597168, Valid Loss: 32.7454039255778\n",
      "Epoch: 604/5000, Train Loss: 30.752642024647106, Valid Loss: 32.74818738301595\n",
      "Epoch: 605/5000, Train Loss: 30.703337062488902, Valid Loss: 32.74510129292806\n",
      "Epoch: 606/5000, Train Loss: 30.599965008822355, Valid Loss: 32.758419036865234\n",
      "Epoch: 607/5000, Train Loss: 30.729637666182086, Valid Loss: 32.72318013509115\n",
      "Epoch: 608/5000, Train Loss: 30.760288758711383, Valid Loss: 32.71621004740397\n",
      "Epoch: 609/5000, Train Loss: 30.480358123779297, Valid Loss: 32.710240046183266\n",
      "Epoch: 610/5000, Train Loss: 30.468150919133965, Valid Loss: 32.70952033996582\n",
      "Epoch: 611/5000, Train Loss: 30.555568174882367, Valid Loss: 32.700113932291664\n",
      "Epoch: 612/5000, Train Loss: 30.457707144997336, Valid Loss: 32.679409662882485\n",
      "Epoch: 613/5000, Train Loss: 30.450442747636274, Valid Loss: 32.68628184000651\n",
      "Epoch: 614/5000, Train Loss: 30.657142985950816, Valid Loss: 32.699097315470375\n",
      "Epoch: 615/5000, Train Loss: 30.85018504749645, Valid Loss: 32.70040512084961\n",
      "Epoch: 616/5000, Train Loss: 30.602442481301047, Valid Loss: 32.70966084798177\n",
      "Epoch: 617/5000, Train Loss: 30.52938565340909, Valid Loss: 32.71811739603678\n",
      "Epoch: 618/5000, Train Loss: 30.420731457796965, Valid Loss: 32.681352615356445\n",
      "Epoch: 619/5000, Train Loss: 30.628407044844195, Valid Loss: 32.68024571736654\n",
      "Epoch: 620/5000, Train Loss: 30.46439569646662, Valid Loss: 32.67246691385905\n",
      "Epoch: 621/5000, Train Loss: 30.308941407637164, Valid Loss: 32.67243957519531\n",
      "Epoch: 622/5000, Train Loss: 30.475341623479668, Valid Loss: 32.66080856323242\n",
      "Epoch: 623/5000, Train Loss: 30.611619082364168, Valid Loss: 32.647853215535484\n",
      "Epoch: 624/5000, Train Loss: 30.696806474165484, Valid Loss: 32.665070215861\n",
      "Epoch: 625/5000, Train Loss: 30.435754429210316, Valid Loss: 32.666313807169594\n",
      "Epoch: 626/5000, Train Loss: 30.412304964932527, Valid Loss: 32.65812683105469\n",
      "Epoch: 627/5000, Train Loss: 30.47200983220881, Valid Loss: 32.66501553853353\n",
      "Epoch: 628/5000, Train Loss: 30.641387072476473, Valid Loss: 32.661702473958336\n",
      "Epoch: 629/5000, Train Loss: 30.607090516523883, Valid Loss: 32.66665458679199\n",
      "Epoch: 630/5000, Train Loss: 30.67574691772461, Valid Loss: 32.69399833679199\n",
      "Epoch: 631/5000, Train Loss: 30.522578672929242, Valid Loss: 32.67845026652018\n",
      "Epoch: 632/5000, Train Loss: 30.48632881858132, Valid Loss: 32.68945503234863\n",
      "Epoch: 633/5000, Train Loss: 30.582922328602184, Valid Loss: 32.66779454549154\n",
      "Epoch: 634/5000, Train Loss: 30.559763995083895, Valid Loss: 32.6640739440918\n",
      "Epoch: 635/5000, Train Loss: 30.443696975708008, Valid Loss: 32.69423484802246\n",
      "Epoch: 636/5000, Train Loss: 30.407331466674805, Valid Loss: 32.68937110900879\n",
      "Epoch: 637/5000, Train Loss: 30.500079068270598, Valid Loss: 32.683305740356445\n",
      "Epoch: 638/5000, Train Loss: 30.948747808283027, Valid Loss: 32.68910535176595\n",
      "Epoch: 639/5000, Train Loss: 30.27870733087713, Valid Loss: 32.687187830607094\n",
      "Epoch: 640/5000, Train Loss: 30.83350285616788, Valid Loss: 32.663317362467446\n",
      "Epoch: 641/5000, Train Loss: 30.760128021240234, Valid Loss: 32.66212590535482\n",
      "Epoch: 642/5000, Train Loss: 30.33534015308727, Valid Loss: 32.662543614705406\n",
      "Epoch: 643/5000, Train Loss: 30.473846088756215, Valid Loss: 32.65142567952474\n",
      "Epoch: 644/5000, Train Loss: 30.41214405406605, Valid Loss: 32.65155919392904\n",
      "Epoch: 645/5000, Train Loss: 30.579564701427113, Valid Loss: 32.681365331014\n",
      "Epoch: 646/5000, Train Loss: 30.383853912353516, Valid Loss: 32.64366022745768\n",
      "Epoch: 647/5000, Train Loss: 30.40915940024636, Valid Loss: 32.66347567240397\n",
      "Epoch: 648/5000, Train Loss: 30.547043540261008, Valid Loss: 32.667091369628906\n",
      "Epoch: 649/5000, Train Loss: 30.225336941805754, Valid Loss: 32.631845474243164\n",
      "Epoch: 650/5000, Train Loss: 30.319293629039418, Valid Loss: 32.655555725097656\n",
      "Epoch: 651/5000, Train Loss: 30.42045645280318, Valid Loss: 32.66040229797363\n",
      "Epoch: 652/5000, Train Loss: 30.551497892899945, Valid Loss: 32.659510930379234\n",
      "Epoch: 653/5000, Train Loss: 30.27634724703702, Valid Loss: 32.67406908671061\n",
      "Epoch: 654/5000, Train Loss: 30.194935885342684, Valid Loss: 32.67718696594238\n",
      "Epoch: 655/5000, Train Loss: 30.651622252030805, Valid Loss: 32.68446667989095\n",
      "Epoch: 656/5000, Train Loss: 30.48773193359375, Valid Loss: 32.65687561035156\n",
      "Epoch: 657/5000, Train Loss: 30.476273449984465, Valid Loss: 32.66786575317383\n",
      "Epoch: 658/5000, Train Loss: 30.460309288718484, Valid Loss: 32.65578079223633\n",
      "Epoch: 659/5000, Train Loss: 30.515128048983488, Valid Loss: 32.635979334513344\n",
      "Epoch: 660/5000, Train Loss: 30.39072539589622, Valid Loss: 32.65626335144043\n",
      "Epoch: 661/5000, Train Loss: 30.480160279707476, Valid Loss: 32.66592534383138\n",
      "Epoch: 662/5000, Train Loss: 30.44019230929288, Valid Loss: 32.66291109720866\n",
      "Epoch: 663/5000, Train Loss: 30.18670550259677, Valid Loss: 32.646233240763344\n",
      "Epoch: 664/5000, Train Loss: 30.689315102317117, Valid Loss: 32.65456453959147\n",
      "Epoch: 665/5000, Train Loss: 30.65653419494629, Valid Loss: 32.64891242980957\n",
      "Epoch: 666/5000, Train Loss: 30.32301764054732, Valid Loss: 32.65773073832194\n",
      "Epoch: 667/5000, Train Loss: 30.70262839577415, Valid Loss: 32.64411226908366\n",
      "Epoch: 668/5000, Train Loss: 30.581817280162465, Valid Loss: 32.63753573099772\n",
      "Epoch: 669/5000, Train Loss: 30.639395627108488, Valid Loss: 32.63230578104655\n",
      "Epoch: 670/5000, Train Loss: 30.47862399708141, Valid Loss: 32.64703114827474\n",
      "Epoch: 671/5000, Train Loss: 30.542564738880504, Valid Loss: 32.639357248942055\n",
      "Epoch: 672/5000, Train Loss: 30.812742059881035, Valid Loss: 32.65343030293783\n",
      "Epoch: 673/5000, Train Loss: 30.359094099564985, Valid Loss: 32.625502268473305\n",
      "Epoch: 674/5000, Train Loss: 30.268529198386453, Valid Loss: 32.6256415049235\n",
      "Epoch: 675/5000, Train Loss: 30.645136052911933, Valid Loss: 32.626718521118164\n",
      "Epoch: 676/5000, Train Loss: 30.484620701182973, Valid Loss: 32.63997332255045\n",
      "Epoch: 677/5000, Train Loss: 30.5562067898837, Valid Loss: 32.62897872924805\n",
      "Epoch: 678/5000, Train Loss: 30.283605922352184, Valid Loss: 32.634769439697266\n",
      "Epoch: 679/5000, Train Loss: 30.702363621104848, Valid Loss: 32.64143943786621\n",
      "Epoch: 680/5000, Train Loss: 30.54907833446156, Valid Loss: 32.64584096272787\n",
      "Epoch: 681/5000, Train Loss: 30.15993413058194, Valid Loss: 32.64545122782389\n",
      "Epoch: 682/5000, Train Loss: 30.469201348044656, Valid Loss: 32.63442039489746\n",
      "Epoch: 683/5000, Train Loss: 30.461484215476297, Valid Loss: 32.63342157999674\n",
      "Epoch: 684/5000, Train Loss: 30.475281975486062, Valid Loss: 32.64549573262533\n",
      "Epoch: 685/5000, Train Loss: 30.31022834777832, Valid Loss: 32.6053663889567\n",
      "Epoch: 686/5000, Train Loss: 30.31975017894398, Valid Loss: 32.6070499420166\n",
      "Epoch: 687/5000, Train Loss: 30.331377723000266, Valid Loss: 32.62902069091797\n",
      "Epoch: 688/5000, Train Loss: 30.25498320839622, Valid Loss: 32.60570844014486\n",
      "Epoch: 689/5000, Train Loss: 30.125692540949043, Valid Loss: 32.610146840413414\n",
      "Epoch: 690/5000, Train Loss: 30.396364905617453, Valid Loss: 32.62647310892741\n",
      "Epoch: 691/5000, Train Loss: 30.124026558615945, Valid Loss: 32.626687367757164\n",
      "Epoch: 692/5000, Train Loss: 30.406497608531605, Valid Loss: 32.64044761657715\n",
      "Epoch: 693/5000, Train Loss: 30.490572669289328, Valid Loss: 32.64535268147787\n",
      "Epoch: 694/5000, Train Loss: 30.431675824252043, Valid Loss: 32.64376576741537\n",
      "Epoch: 695/5000, Train Loss: 30.330792687155984, Valid Loss: 32.658331553141274\n",
      "Epoch: 696/5000, Train Loss: 30.41541168906472, Valid Loss: 32.649282455444336\n",
      "Epoch: 697/5000, Train Loss: 30.532655022361062, Valid Loss: 32.652404149373375\n",
      "Epoch: 698/5000, Train Loss: 30.521567257967863, Valid Loss: 32.65678024291992\n",
      "Epoch: 699/5000, Train Loss: 30.16173119978471, Valid Loss: 32.65386199951172\n",
      "Epoch: 700/5000, Train Loss: 30.448956576260652, Valid Loss: 32.65959040323893\n",
      "Epoch: 701/5000, Train Loss: 30.368830767544832, Valid Loss: 32.62820688883463\n",
      "Epoch: 702/5000, Train Loss: 30.314931002530184, Valid Loss: 32.6353505452474\n",
      "Epoch: 703/5000, Train Loss: 30.172673138705168, Valid Loss: 32.65038808186849\n",
      "Epoch: 704/5000, Train Loss: 30.569005272605203, Valid Loss: 32.627236684163414\n",
      "Epoch: 705/5000, Train Loss: 30.192719372836027, Valid Loss: 32.61556434631348\n",
      "Epoch: 706/5000, Train Loss: 30.25433973832564, Valid Loss: 32.608744303385414\n",
      "Epoch: 707/5000, Train Loss: 30.4158992767334, Valid Loss: 32.606005350748696\n",
      "Epoch: 708/5000, Train Loss: 30.537113883278586, Valid Loss: 32.61743036905924\n",
      "Epoch: 709/5000, Train Loss: 30.653391231190074, Valid Loss: 32.597838719685875\n",
      "Epoch: 710/5000, Train Loss: 30.327335877852008, Valid Loss: 32.575260162353516\n",
      "Epoch: 711/5000, Train Loss: 30.20183701948686, Valid Loss: 32.58540916442871\n",
      "Epoch: 712/5000, Train Loss: 30.435360301624645, Valid Loss: 32.582459131876625\n",
      "Epoch: 713/5000, Train Loss: 30.24295356056907, Valid Loss: 32.621045430501304\n",
      "Epoch: 714/5000, Train Loss: 30.577192653309215, Valid Loss: 32.648460388183594\n",
      "Epoch: 715/5000, Train Loss: 30.493793314153496, Valid Loss: 32.64340845743815\n",
      "Epoch: 716/5000, Train Loss: 30.523210178722035, Valid Loss: 32.63456026713053\n",
      "Epoch: 717/5000, Train Loss: 30.608749389648438, Valid Loss: 32.638060887654625\n",
      "Epoch: 718/5000, Train Loss: 29.975807363336738, Valid Loss: 32.61220105489095\n",
      "Epoch: 719/5000, Train Loss: 30.266870498657227, Valid Loss: 32.60957781473795\n",
      "Epoch: 720/5000, Train Loss: 30.53206374428489, Valid Loss: 32.60266431172689\n",
      "Epoch: 721/5000, Train Loss: 30.34874049100009, Valid Loss: 32.617122650146484\n",
      "Epoch: 722/5000, Train Loss: 30.739821173928, Valid Loss: 32.63511721293131\n",
      "Epoch: 723/5000, Train Loss: 30.18482711098411, Valid Loss: 32.645921071370445\n",
      "Epoch: 724/5000, Train Loss: 30.42176645452326, Valid Loss: 32.621066411336265\n",
      "Epoch: 725/5000, Train Loss: 30.35991807417436, Valid Loss: 32.61588478088379\n",
      "Epoch: 726/5000, Train Loss: 30.609309976751153, Valid Loss: 32.59678077697754\n",
      "Epoch: 727/5000, Train Loss: 30.329299233176492, Valid Loss: 32.59979565938314\n",
      "Epoch: 728/5000, Train Loss: 30.31832521611994, Valid Loss: 32.612622578938804\n",
      "Epoch: 729/5000, Train Loss: 30.4983133836226, Valid Loss: 32.592641830444336\n",
      "Epoch: 730/5000, Train Loss: 30.73067318309437, Valid Loss: 32.619410832722984\n",
      "Epoch: 731/5000, Train Loss: 30.33753880587491, Valid Loss: 32.628235499064125\n",
      "Epoch: 732/5000, Train Loss: 30.298904072154652, Valid Loss: 32.60680262247721\n",
      "Epoch: 733/5000, Train Loss: 30.44016456604004, Valid Loss: 32.62552134195963\n",
      "Epoch: 734/5000, Train Loss: 30.282400131225586, Valid Loss: 32.62302017211914\n",
      "Epoch: 735/5000, Train Loss: 30.445737145163797, Valid Loss: 32.62059656778971\n",
      "Epoch: 736/5000, Train Loss: 30.36674031344327, Valid Loss: 32.6069081624349\n",
      "Epoch: 737/5000, Train Loss: 30.427744258533824, Valid Loss: 32.58958371480306\n",
      "Epoch: 738/5000, Train Loss: 30.56634486805309, Valid Loss: 32.58682060241699\n",
      "Epoch: 739/5000, Train Loss: 30.45524770563299, Valid Loss: 32.59665107727051\n",
      "Epoch: 740/5000, Train Loss: 30.36598188226873, Valid Loss: 32.59016990661621\n",
      "Epoch: 741/5000, Train Loss: 30.164916818792168, Valid Loss: 32.59104919433594\n",
      "Epoch: 742/5000, Train Loss: 30.20415722240101, Valid Loss: 32.586920420328774\n",
      "Epoch: 743/5000, Train Loss: 30.782990368929777, Valid Loss: 32.59936714172363\n",
      "Epoch: 744/5000, Train Loss: 30.515980460427024, Valid Loss: 32.602149963378906\n",
      "Epoch: 745/5000, Train Loss: 30.63910016146573, Valid Loss: 32.61665026346842\n",
      "Epoch: 746/5000, Train Loss: 30.53849254954945, Valid Loss: 32.63529586791992\n",
      "Epoch: 747/5000, Train Loss: 30.23279415477406, Valid Loss: 32.621289571126304\n",
      "Epoch: 748/5000, Train Loss: 29.59117282520641, Valid Loss: 32.61616961161295\n",
      "Epoch: 749/5000, Train Loss: 30.563012036410246, Valid Loss: 32.60492070515951\n",
      "Epoch: 750/5000, Train Loss: 30.253543853759766, Valid Loss: 32.584826151529946\n",
      "Epoch: 751/5000, Train Loss: 30.40633236278187, Valid Loss: 32.59199333190918\n",
      "Epoch: 752/5000, Train Loss: 30.351008848710492, Valid Loss: 32.59833335876465\n",
      "Epoch: 753/5000, Train Loss: 30.372130480679598, Valid Loss: 32.59964243570963\n",
      "Epoch: 754/5000, Train Loss: 30.16002793745561, Valid Loss: 32.59395408630371\n",
      "Epoch: 755/5000, Train Loss: 30.234807274558328, Valid Loss: 32.57918167114258\n",
      "Epoch: 756/5000, Train Loss: 30.53249237754128, Valid Loss: 32.5719248453776\n",
      "Epoch: 757/5000, Train Loss: 30.703687494451348, Valid Loss: 32.581956227620445\n",
      "Epoch: 758/5000, Train Loss: 30.678212425925516, Valid Loss: 32.59314155578613\n",
      "Epoch: 759/5000, Train Loss: 30.279733137650922, Valid Loss: 32.59333801269531\n",
      "Epoch: 760/5000, Train Loss: 30.257123253562234, Valid Loss: 32.59051767985026\n",
      "Epoch: 761/5000, Train Loss: 30.650129838423297, Valid Loss: 32.596888860066734\n",
      "Epoch: 762/5000, Train Loss: 30.394881508567117, Valid Loss: 32.58751360575358\n",
      "Epoch: 763/5000, Train Loss: 30.271433223377574, Valid Loss: 32.59572792053223\n",
      "Epoch: 764/5000, Train Loss: 30.30642492120916, Valid Loss: 32.603437423706055\n",
      "Epoch: 765/5000, Train Loss: 30.4450740814209, Valid Loss: 32.60199737548828\n",
      "Epoch: 766/5000, Train Loss: 30.129296736283735, Valid Loss: 32.61382929484049\n",
      "Epoch: 767/5000, Train Loss: 30.558422782204367, Valid Loss: 32.60078048706055\n",
      "Epoch: 768/5000, Train Loss: 30.357431931929156, Valid Loss: 32.586249669392906\n",
      "Epoch: 769/5000, Train Loss: 30.003923242742363, Valid Loss: 32.59050687154134\n",
      "Epoch: 770/5000, Train Loss: 30.112290469082918, Valid Loss: 32.59319814046224\n",
      "Epoch: 771/5000, Train Loss: 30.163496537642047, Valid Loss: 32.62395350138346\n",
      "Epoch: 772/5000, Train Loss: 30.249114123257723, Valid Loss: 32.607077280680336\n",
      "Epoch: 773/5000, Train Loss: 30.4785643490878, Valid Loss: 32.59259859720866\n",
      "Epoch: 774/5000, Train Loss: 30.0547402121804, Valid Loss: 32.597630182902016\n",
      "Epoch: 775/5000, Train Loss: 30.243885387073863, Valid Loss: 32.583685557047524\n",
      "Epoch: 776/5000, Train Loss: 30.115700461647727, Valid Loss: 32.585838317871094\n",
      "Epoch: 777/5000, Train Loss: 30.555734634399414, Valid Loss: 32.59366226196289\n",
      "Epoch: 778/5000, Train Loss: 30.13585090637207, Valid Loss: 32.58450508117676\n",
      "Epoch: 779/5000, Train Loss: 30.37824110551314, Valid Loss: 32.5767822265625\n",
      "Epoch: 780/5000, Train Loss: 30.48497095975009, Valid Loss: 32.58328946431478\n",
      "Epoch: 781/5000, Train Loss: 30.543093421242453, Valid Loss: 32.600406646728516\n",
      "Epoch: 782/5000, Train Loss: 30.413542140613902, Valid Loss: 32.61377271016439\n",
      "Epoch: 783/5000, Train Loss: 30.019310344349254, Valid Loss: 32.59350458780924\n",
      "Epoch: 784/5000, Train Loss: 30.149910493330523, Valid Loss: 32.56082089742025\n",
      "Epoch: 785/5000, Train Loss: 30.17262840270996, Valid Loss: 32.55734443664551\n",
      "Epoch: 786/5000, Train Loss: 30.369387713345613, Valid Loss: 32.567434310913086\n",
      "Epoch: 787/5000, Train Loss: 30.39355572787198, Valid Loss: 32.563170750935875\n",
      "Epoch: 788/5000, Train Loss: 30.44984384016557, Valid Loss: 32.576700846354164\n",
      "Epoch: 789/5000, Train Loss: 30.428980220447887, Valid Loss: 32.60117721557617\n",
      "Epoch: 790/5000, Train Loss: 30.25435413013805, Valid Loss: 32.595001856486\n",
      "Epoch: 791/5000, Train Loss: 30.09637624567205, Valid Loss: 32.58588727315267\n",
      "Epoch: 792/5000, Train Loss: 30.42684728449041, Valid Loss: 32.60401153564453\n",
      "Epoch: 793/5000, Train Loss: 30.06233735518022, Valid Loss: 32.60205332438151\n",
      "Epoch: 794/5000, Train Loss: 30.255905498157848, Valid Loss: 32.6101926167806\n",
      "Epoch: 795/5000, Train Loss: 30.414824052290484, Valid Loss: 32.606288274129234\n",
      "Epoch: 796/5000, Train Loss: 29.9589035727761, Valid Loss: 32.59010378519694\n",
      "Epoch: 797/5000, Train Loss: 30.456115202470258, Valid Loss: 32.575523376464844\n",
      "Epoch: 798/5000, Train Loss: 30.184641577980734, Valid Loss: 32.5751584370931\n",
      "Epoch: 799/5000, Train Loss: 30.201947992498223, Valid Loss: 32.60983657836914\n",
      "Epoch: 800/5000, Train Loss: 30.010832179676402, Valid Loss: 32.602684020996094\n",
      "Epoch: 801/5000, Train Loss: 30.352712111039594, Valid Loss: 32.62868690490723\n",
      "Epoch: 802/5000, Train Loss: 30.001542004671965, Valid Loss: 32.617262522379555\n",
      "Epoch: 803/5000, Train Loss: 30.01654018055309, Valid Loss: 32.602681477864586\n",
      "Epoch: 804/5000, Train Loss: 30.23505228215998, Valid Loss: 32.59535916646322\n",
      "Epoch: 805/5000, Train Loss: 30.042315049604937, Valid Loss: 32.60749053955078\n",
      "Epoch: 806/5000, Train Loss: 30.32079939408736, Valid Loss: 32.58481216430664\n",
      "Epoch: 807/5000, Train Loss: 30.622644077647816, Valid Loss: 32.60600662231445\n",
      "Epoch: 808/5000, Train Loss: 30.111710461703215, Valid Loss: 32.60691006978353\n",
      "Epoch: 809/5000, Train Loss: 30.151871074329723, Valid Loss: 32.58255259195963\n",
      "Epoch: 810/5000, Train Loss: 30.208675904707476, Valid Loss: 32.59642664591471\n",
      "Epoch: 811/5000, Train Loss: 30.138011412187055, Valid Loss: 32.593719482421875\n",
      "Epoch: 812/5000, Train Loss: 29.966201782226562, Valid Loss: 32.58682441711426\n",
      "Epoch: 813/5000, Train Loss: 30.232037977738813, Valid Loss: 32.59040323893229\n",
      "Epoch: 814/5000, Train Loss: 30.445402145385742, Valid Loss: 32.59978739420573\n",
      "Epoch: 815/5000, Train Loss: 30.386038520119406, Valid Loss: 32.61639086405436\n",
      "Epoch: 816/5000, Train Loss: 29.994483254172586, Valid Loss: 32.59230105082194\n",
      "Epoch: 817/5000, Train Loss: 30.24076167019931, Valid Loss: 32.589128494262695\n",
      "Epoch: 818/5000, Train Loss: 30.17918656089089, Valid Loss: 32.5888474782308\n",
      "Epoch: 819/5000, Train Loss: 29.915341117165305, Valid Loss: 32.60191853841146\n",
      "Epoch: 820/5000, Train Loss: 30.178308833729137, Valid Loss: 32.58303642272949\n",
      "Epoch: 821/5000, Train Loss: 30.092059048739348, Valid Loss: 32.57732836405436\n",
      "Epoch: 822/5000, Train Loss: 30.177704204212535, Valid Loss: 32.57857577006022\n",
      "Epoch: 823/5000, Train Loss: 29.82532275806774, Valid Loss: 32.56310526529948\n",
      "Epoch: 824/5000, Train Loss: 30.345449447631836, Valid Loss: 32.56510225931803\n",
      "Epoch: 825/5000, Train Loss: 30.16691086509011, Valid Loss: 32.56730524698893\n",
      "Epoch: 826/5000, Train Loss: 30.328021309592508, Valid Loss: 32.57886250813802\n",
      "Epoch: 827/5000, Train Loss: 29.998713580044832, Valid Loss: 32.570828119913735\n",
      "Epoch: 828/5000, Train Loss: 30.38382252779874, Valid Loss: 32.55304781595866\n",
      "Epoch: 829/5000, Train Loss: 30.15939712524414, Valid Loss: 32.550375620524086\n",
      "Epoch: 830/5000, Train Loss: 30.173952449451793, Valid Loss: 32.53112665812174\n",
      "Epoch: 831/5000, Train Loss: 30.183382727883078, Valid Loss: 32.57248115539551\n",
      "Epoch: 832/5000, Train Loss: 30.40260020169345, Valid Loss: 32.56294059753418\n",
      "Epoch: 833/5000, Train Loss: 30.17700732838024, Valid Loss: 32.5896422068278\n",
      "Epoch: 834/5000, Train Loss: 30.053998773748223, Valid Loss: 32.59374173482259\n",
      "Epoch: 835/5000, Train Loss: 29.845590071244672, Valid Loss: 32.598855336507164\n",
      "Epoch: 836/5000, Train Loss: 30.183996200561523, Valid Loss: 32.593017578125\n",
      "Epoch: 837/5000, Train Loss: 29.931802576238457, Valid Loss: 32.62434069315592\n",
      "Epoch: 838/5000, Train Loss: 30.263269251043145, Valid Loss: 32.616726557413735\n",
      "Epoch: 839/5000, Train Loss: 30.148039731112394, Valid Loss: 32.62416394551595\n",
      "Epoch: 840/5000, Train Loss: 30.352390462701972, Valid Loss: 32.622870127360024\n",
      "Epoch: 841/5000, Train Loss: 30.413454749367453, Valid Loss: 32.62200927734375\n",
      "Epoch: 842/5000, Train Loss: 30.456599322232332, Valid Loss: 32.63205909729004\n",
      "Epoch: 843/5000, Train Loss: 30.31285285949707, Valid Loss: 32.63554445902506\n",
      "Epoch: 844/5000, Train Loss: 30.24052689292214, Valid Loss: 32.62740262349447\n",
      "Epoch: 845/5000, Train Loss: 29.819222536954012, Valid Loss: 32.637444814046226\n",
      "Epoch: 846/5000, Train Loss: 30.119097622958098, Valid Loss: 32.599830627441406\n",
      "Epoch: 847/5000, Train Loss: 30.232520190152254, Valid Loss: 32.629639307657875\n",
      "Epoch: 848/5000, Train Loss: 29.999798688021574, Valid Loss: 32.62530008951823\n",
      "Epoch: 849/5000, Train Loss: 30.034020337191496, Valid Loss: 32.65026664733887\n",
      "Epoch: 850/5000, Train Loss: 29.9287034815008, Valid Loss: 32.644003550211586\n",
      "Epoch: 851/5000, Train Loss: 30.256026354703035, Valid Loss: 32.63445917765299\n",
      "Epoch: 852/5000, Train Loss: 30.008834665471856, Valid Loss: 32.62118021647135\n",
      "Epoch: 853/5000, Train Loss: 30.183134425770152, Valid Loss: 32.622559229532875\n",
      "Epoch: 854/5000, Train Loss: 30.196931838989258, Valid Loss: 32.61426862080892\n",
      "Epoch: 855/5000, Train Loss: 30.185870950872246, Valid Loss: 32.628077824910484\n",
      "Epoch: 856/5000, Train Loss: 30.382398085160688, Valid Loss: 32.61601765950521\n",
      "Epoch: 857/5000, Train Loss: 30.472776759754527, Valid Loss: 32.618829091389976\n",
      "Epoch: 858/5000, Train Loss: 30.443869504061613, Valid Loss: 32.606048583984375\n",
      "Epoch: 859/5000, Train Loss: 30.109351938421074, Valid Loss: 32.61718622843424\n",
      "Epoch: 860/5000, Train Loss: 30.417617104270242, Valid Loss: 32.602565129597984\n",
      "Epoch: 861/5000, Train Loss: 29.98789405822754, Valid Loss: 32.603644688924156\n",
      "Epoch: 862/5000, Train Loss: 30.2595232183283, Valid Loss: 32.60803667704264\n",
      "Epoch: 863/5000, Train Loss: 30.101634979248047, Valid Loss: 32.61359405517578\n",
      "Epoch: 864/5000, Train Loss: 29.976198543201793, Valid Loss: 32.606297175089516\n",
      "Epoch: 865/5000, Train Loss: 30.163496017456055, Valid Loss: 32.59020233154297\n",
      "Epoch: 866/5000, Train Loss: 30.1606388092041, Valid Loss: 32.621145248413086\n",
      "Epoch: 867/5000, Train Loss: 30.3713791587136, Valid Loss: 32.63413619995117\n",
      "Epoch: 868/5000, Train Loss: 30.15610764243386, Valid Loss: 32.61369959513346\n",
      "Epoch: 869/5000, Train Loss: 29.874507210471414, Valid Loss: 32.578694661458336\n",
      "Epoch: 870/5000, Train Loss: 30.100812565196644, Valid Loss: 32.58013661702474\n",
      "Epoch: 871/5000, Train Loss: 30.09784074263139, Valid Loss: 32.56210390726725\n",
      "Epoch: 872/5000, Train Loss: 30.43590146845037, Valid Loss: 32.58918062845866\n",
      "Epoch: 873/5000, Train Loss: 30.171145179054953, Valid Loss: 32.582920710245766\n",
      "Epoch: 874/5000, Train Loss: 29.96794145757502, Valid Loss: 32.5942268371582\n",
      "Epoch: 875/5000, Train Loss: 29.865862759676848, Valid Loss: 32.596818923950195\n",
      "Epoch: 876/5000, Train Loss: 30.087893399325285, Valid Loss: 32.57292683919271\n",
      "Epoch: 877/5000, Train Loss: 30.001279657537285, Valid Loss: 32.58196703592936\n",
      "Epoch: 878/5000, Train Loss: 30.32920369234952, Valid Loss: 32.58323415120443\n",
      "Epoch: 879/5000, Train Loss: 29.83437832919034, Valid Loss: 32.573071797688804\n",
      "Epoch: 880/5000, Train Loss: 29.946019085970793, Valid Loss: 32.581494649251304\n",
      "Epoch: 881/5000, Train Loss: 30.417102120139383, Valid Loss: 32.591718673706055\n",
      "Epoch: 882/5000, Train Loss: 30.228967319835316, Valid Loss: 32.58823013305664\n",
      "Epoch: 883/5000, Train Loss: 30.12258390946822, Valid Loss: 32.58650906880697\n",
      "Epoch: 884/5000, Train Loss: 29.58170041170987, Valid Loss: 32.58651606241862\n",
      "Epoch: 885/5000, Train Loss: 29.935650565407492, Valid Loss: 32.576337814331055\n",
      "Epoch: 886/5000, Train Loss: 30.29448925365101, Valid Loss: 32.56000645955404\n",
      "Epoch: 887/5000, Train Loss: 30.124588706276633, Valid Loss: 32.5797430674235\n",
      "Epoch: 888/5000, Train Loss: 30.0617452101274, Valid Loss: 32.58562215169271\n",
      "Epoch: 889/5000, Train Loss: 29.76470340381969, Valid Loss: 32.57507514953613\n",
      "Epoch: 890/5000, Train Loss: 30.126751119440254, Valid Loss: 32.59254519144694\n",
      "Epoch: 891/5000, Train Loss: 30.152511423284356, Valid Loss: 32.579168955485024\n",
      "Epoch: 892/5000, Train Loss: 30.104569175026633, Valid Loss: 32.576171239217125\n",
      "Epoch: 893/5000, Train Loss: 30.34490429271351, Valid Loss: 32.561924616495766\n",
      "Epoch: 894/5000, Train Loss: 30.19312927939675, Valid Loss: 32.56370290120443\n",
      "Epoch: 895/5000, Train Loss: 30.036513935435902, Valid Loss: 32.542981465657554\n",
      "Epoch: 896/5000, Train Loss: 30.254322745583274, Valid Loss: 32.56734275817871\n",
      "Epoch: 897/5000, Train Loss: 30.285051345825195, Valid Loss: 32.5601921081543\n",
      "Epoch: 898/5000, Train Loss: 29.974428870461203, Valid Loss: 32.55408922831217\n",
      "Epoch: 899/5000, Train Loss: 30.298033454201438, Valid Loss: 32.57227007548014\n",
      "Epoch: 900/5000, Train Loss: 30.26263497092507, Valid Loss: 32.587258656819664\n",
      "Epoch: 901/5000, Train Loss: 30.13574912331321, Valid Loss: 32.58679072062174\n",
      "Epoch: 902/5000, Train Loss: 30.2028602253307, Valid Loss: 32.566521962483726\n",
      "Epoch: 903/5000, Train Loss: 30.165379784323953, Valid Loss: 32.58532842000326\n",
      "Epoch: 904/5000, Train Loss: 29.802229794588957, Valid Loss: 32.57183519999186\n",
      "Epoch: 905/5000, Train Loss: 29.99909609014338, Valid Loss: 32.57098388671875\n",
      "Epoch: 906/5000, Train Loss: 29.746200041337445, Valid Loss: 32.57360649108887\n",
      "Epoch: 907/5000, Train Loss: 29.901761661876332, Valid Loss: 32.589817682902016\n",
      "Epoch: 908/5000, Train Loss: 30.128659681840375, Valid Loss: 32.56696001688639\n",
      "Epoch: 909/5000, Train Loss: 30.167748017744586, Valid Loss: 32.589966456095375\n",
      "Epoch: 910/5000, Train Loss: 29.99968043240634, Valid Loss: 32.60116386413574\n",
      "Epoch: 911/5000, Train Loss: 30.294658660888672, Valid Loss: 32.60690434773763\n",
      "Epoch: 912/5000, Train Loss: 29.851954546841707, Valid Loss: 32.599827448527016\n",
      "Epoch: 913/5000, Train Loss: 29.940630826083098, Valid Loss: 32.583047231038414\n",
      "Epoch: 914/5000, Train Loss: 29.897756749933418, Valid Loss: 32.58245595296224\n",
      "Epoch: 915/5000, Train Loss: 30.165942798961293, Valid Loss: 32.59158134460449\n",
      "Epoch: 916/5000, Train Loss: 29.82711271806197, Valid Loss: 32.59026590983073\n",
      "Epoch: 917/5000, Train Loss: 29.827618859030984, Valid Loss: 32.58608436584473\n",
      "Epoch: 918/5000, Train Loss: 29.97164570201527, Valid Loss: 32.602362950642906\n",
      "Epoch: 919/5000, Train Loss: 30.00282062183727, Valid Loss: 32.58360481262207\n",
      "Epoch: 920/5000, Train Loss: 30.143335169011895, Valid Loss: 32.5793399810791\n",
      "Epoch: 921/5000, Train Loss: 30.328765869140625, Valid Loss: 32.56616020202637\n",
      "Epoch: 922/5000, Train Loss: 30.242666591297496, Valid Loss: 32.55745887756348\n",
      "Epoch: 923/5000, Train Loss: 29.981259432705965, Valid Loss: 32.57013511657715\n",
      "Epoch: 924/5000, Train Loss: 30.068445205688477, Valid Loss: 32.58341534932455\n",
      "Epoch: 925/5000, Train Loss: 30.12613764676181, Valid Loss: 32.603563944498696\n",
      "Epoch: 926/5000, Train Loss: 30.03237030722878, Valid Loss: 32.585883458455406\n",
      "Epoch: 927/5000, Train Loss: 30.078089107166637, Valid Loss: 32.58416620890299\n",
      "Epoch: 928/5000, Train Loss: 29.976105083118785, Valid Loss: 32.61178398132324\n",
      "Epoch: 929/5000, Train Loss: 30.097440892999824, Valid Loss: 32.59447352091471\n",
      "Epoch: 930/5000, Train Loss: 30.04137021845037, Valid Loss: 32.58495267232259\n",
      "얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 930에서 훈련 중단.\n",
      "Training Start: HLM\n",
      "Epoch: 0/5000, Train Loss: 64.31791166825728, Valid Loss: 63.11164220174154\n",
      "Epoch: 1/5000, Train Loss: 64.24976140802556, Valid Loss: 63.05907694498698\n",
      "Epoch: 2/5000, Train Loss: 64.22775754061612, Valid Loss: 63.021071116129555\n",
      "Epoch: 3/5000, Train Loss: 64.19413098421964, Valid Loss: 62.993874867757164\n",
      "Epoch: 4/5000, Train Loss: 64.15004626187411, Valid Loss: 62.95254643758138\n",
      "Epoch: 5/5000, Train Loss: 64.05823759599166, Valid Loss: 62.92653020222982\n",
      "Epoch: 6/5000, Train Loss: 64.01504343206233, Valid Loss: 62.88994598388672\n",
      "Epoch: 7/5000, Train Loss: 64.03009484030984, Valid Loss: 62.863356272379555\n",
      "Epoch: 8/5000, Train Loss: 63.93069076538086, Valid Loss: 62.82266108194987\n",
      "Epoch: 9/5000, Train Loss: 63.95398920232599, Valid Loss: 62.793139139811196\n",
      "Epoch: 10/5000, Train Loss: 63.86758804321289, Valid Loss: 62.75650151570638\n",
      "Epoch: 11/5000, Train Loss: 63.82179329612038, Valid Loss: 62.71983846028646\n",
      "Epoch: 12/5000, Train Loss: 63.82796408913352, Valid Loss: 62.68119557698568\n",
      "Epoch: 13/5000, Train Loss: 63.7588285966353, Valid Loss: 62.634751637776695\n",
      "Epoch: 14/5000, Train Loss: 63.704103296453304, Valid Loss: 62.60029729207357\n",
      "Epoch: 15/5000, Train Loss: 63.64922887628729, Valid Loss: 62.564291636149086\n",
      "Epoch: 16/5000, Train Loss: 63.67175015536222, Valid Loss: 62.513806660970054\n",
      "Epoch: 17/5000, Train Loss: 63.60146643898704, Valid Loss: 62.47312672932943\n",
      "Epoch: 18/5000, Train Loss: 63.56524103338068, Valid Loss: 62.43412399291992\n",
      "Epoch: 19/5000, Train Loss: 63.53355581110174, Valid Loss: 62.3996836344401\n",
      "Epoch: 20/5000, Train Loss: 63.46668867631392, Valid Loss: 62.371402740478516\n",
      "Epoch: 21/5000, Train Loss: 63.44446390325373, Valid Loss: 62.316691080729164\n",
      "Epoch: 22/5000, Train Loss: 63.390080885453656, Valid Loss: 62.26925913492838\n",
      "Epoch: 23/5000, Train Loss: 63.351592323996805, Valid Loss: 62.24008051554362\n",
      "Epoch: 24/5000, Train Loss: 63.33337957208807, Valid Loss: 62.184261322021484\n",
      "Epoch: 25/5000, Train Loss: 63.27338062633168, Valid Loss: 62.14134852091471\n",
      "Epoch: 26/5000, Train Loss: 63.18764391812411, Valid Loss: 62.10113525390625\n",
      "Epoch: 27/5000, Train Loss: 63.165746515447445, Valid Loss: 62.053541819254555\n",
      "Epoch: 28/5000, Train Loss: 63.1301550431685, Valid Loss: 62.02108383178711\n",
      "Epoch: 29/5000, Train Loss: 63.14084209095348, Valid Loss: 61.98272450764974\n",
      "Epoch: 30/5000, Train Loss: 63.05096158114347, Valid Loss: 61.94292449951172\n",
      "Epoch: 31/5000, Train Loss: 63.009240583939985, Valid Loss: 61.90651830037435\n",
      "Epoch: 32/5000, Train Loss: 62.972415230490945, Valid Loss: 61.84620030721029\n",
      "Epoch: 33/5000, Train Loss: 62.952903747558594, Valid Loss: 61.820447285970054\n",
      "Epoch: 34/5000, Train Loss: 62.912381952459164, Valid Loss: 61.74855295817057\n",
      "Epoch: 35/5000, Train Loss: 62.86499682339755, Valid Loss: 61.725701649983726\n",
      "Epoch: 36/5000, Train Loss: 62.82002674449574, Valid Loss: 61.67333475748698\n",
      "Epoch: 37/5000, Train Loss: 62.76089720292525, Valid Loss: 61.59477233886719\n",
      "Epoch: 38/5000, Train Loss: 62.6891808943315, Valid Loss: 61.5662841796875\n",
      "Epoch: 39/5000, Train Loss: 62.70953993363814, Valid Loss: 61.56059137980143\n",
      "Epoch: 40/5000, Train Loss: 62.61798823963512, Valid Loss: 61.47993469238281\n",
      "Epoch: 41/5000, Train Loss: 62.571785319935195, Valid Loss: 61.41627629597982\n",
      "Epoch: 42/5000, Train Loss: 62.553970683704726, Valid Loss: 61.40512466430664\n",
      "Epoch: 43/5000, Train Loss: 62.50573661110618, Valid Loss: 61.321661631266274\n",
      "Epoch: 44/5000, Train Loss: 62.48715383356268, Valid Loss: 61.266422271728516\n",
      "Epoch: 45/5000, Train Loss: 62.36198772083629, Valid Loss: 61.20418167114258\n",
      "Epoch: 46/5000, Train Loss: 62.31661675193093, Valid Loss: 61.21601994832357\n",
      "Epoch: 47/5000, Train Loss: 62.32186369462447, Valid Loss: 61.15380350748698\n",
      "Epoch: 48/5000, Train Loss: 62.27115700461648, Valid Loss: 61.131518046061196\n",
      "Epoch: 49/5000, Train Loss: 62.206045324152164, Valid Loss: 61.05534998575846\n",
      "Epoch: 50/5000, Train Loss: 62.173558321866125, Valid Loss: 61.03159713745117\n",
      "Epoch: 51/5000, Train Loss: 62.12674192948775, Valid Loss: 60.981590270996094\n",
      "Epoch: 52/5000, Train Loss: 62.13927182284269, Valid Loss: 60.95417022705078\n",
      "Epoch: 53/5000, Train Loss: 62.03127219460227, Valid Loss: 60.8796501159668\n",
      "Epoch: 54/5000, Train Loss: 62.00993589921431, Valid Loss: 60.80253982543945\n",
      "Epoch: 55/5000, Train Loss: 61.938050356778234, Valid Loss: 60.76498667399088\n",
      "Epoch: 56/5000, Train Loss: 61.91896507956765, Valid Loss: 60.72384770711263\n",
      "Epoch: 57/5000, Train Loss: 61.85293752496893, Valid Loss: 60.6775016784668\n",
      "Epoch: 58/5000, Train Loss: 61.76164661754262, Valid Loss: 60.62878545125326\n",
      "Epoch: 59/5000, Train Loss: 61.737618533047765, Valid Loss: 60.55248133341471\n",
      "Epoch: 60/5000, Train Loss: 61.66433993252841, Valid Loss: 60.51592000325521\n",
      "Epoch: 61/5000, Train Loss: 61.62012204256925, Valid Loss: 60.47249094645182\n",
      "Epoch: 62/5000, Train Loss: 61.55153066461737, Valid Loss: 60.44176355997721\n",
      "Epoch: 63/5000, Train Loss: 61.54015350341797, Valid Loss: 60.40387725830078\n",
      "Epoch: 64/5000, Train Loss: 61.47188394719904, Valid Loss: 60.3056894938151\n",
      "Epoch: 65/5000, Train Loss: 61.42787968028676, Valid Loss: 60.2661984761556\n",
      "Epoch: 66/5000, Train Loss: 61.406091863458805, Valid Loss: 60.22168731689453\n",
      "Epoch: 67/5000, Train Loss: 61.33905618840998, Valid Loss: 60.23041025797526\n",
      "Epoch: 68/5000, Train Loss: 61.2805834683505, Valid Loss: 60.12795384724935\n",
      "Epoch: 69/5000, Train Loss: 61.279578469016336, Valid Loss: 60.08829243977865\n",
      "Epoch: 70/5000, Train Loss: 61.16893421519887, Valid Loss: 60.0227419535319\n",
      "Epoch: 71/5000, Train Loss: 61.16025890003551, Valid Loss: 59.994197845458984\n",
      "Epoch: 72/5000, Train Loss: 61.067322817715734, Valid Loss: 59.92091623942057\n",
      "Epoch: 73/5000, Train Loss: 61.03969227183949, Valid Loss: 59.86987050374349\n",
      "Epoch: 74/5000, Train Loss: 60.976754968816586, Valid Loss: 59.825066884358726\n",
      "Epoch: 75/5000, Train Loss: 60.936790466308594, Valid Loss: 59.79131825764974\n",
      "Epoch: 76/5000, Train Loss: 60.861638502641156, Valid Loss: 59.783390045166016\n",
      "Epoch: 77/5000, Train Loss: 60.82963562011719, Valid Loss: 59.68404897054037\n",
      "Epoch: 78/5000, Train Loss: 60.78344449129972, Valid Loss: 59.63369496663412\n",
      "Epoch: 79/5000, Train Loss: 60.68745006214488, Valid Loss: 59.59480412801107\n",
      "Epoch: 80/5000, Train Loss: 60.64026086980646, Valid Loss: 59.51853942871094\n",
      "Epoch: 81/5000, Train Loss: 60.59492874145508, Valid Loss: 59.4986941019694\n",
      "Epoch: 82/5000, Train Loss: 60.58098775690252, Valid Loss: 59.45036188761393\n",
      "Epoch: 83/5000, Train Loss: 60.46540520407937, Valid Loss: 59.35232798258463\n",
      "Epoch: 84/5000, Train Loss: 60.45296304876154, Valid Loss: 59.31658045450846\n",
      "Epoch: 85/5000, Train Loss: 60.36642490733754, Valid Loss: 59.30930964152018\n",
      "Epoch: 86/5000, Train Loss: 60.32150545987216, Valid Loss: 59.25072224934896\n",
      "Epoch: 87/5000, Train Loss: 60.30646306818182, Valid Loss: 59.171732584635414\n",
      "Epoch: 88/5000, Train Loss: 60.20819993452592, Valid Loss: 59.119574228922524\n",
      "Epoch: 89/5000, Train Loss: 60.17350942438299, Valid Loss: 59.076314290364586\n",
      "Epoch: 90/5000, Train Loss: 60.12251663208008, Valid Loss: 59.0268809000651\n",
      "Epoch: 91/5000, Train Loss: 60.04577151211825, Valid Loss: 58.9866689046224\n",
      "Epoch: 92/5000, Train Loss: 60.00698054920543, Valid Loss: 58.90316390991211\n",
      "Epoch: 93/5000, Train Loss: 59.91911662708629, Valid Loss: 58.747918446858726\n",
      "Epoch: 94/5000, Train Loss: 59.87639097733931, Valid Loss: 58.76231257120768\n",
      "Epoch: 95/5000, Train Loss: 59.821523146195844, Valid Loss: 58.687302907307945\n",
      "Epoch: 96/5000, Train Loss: 59.81207275390625, Valid Loss: 58.631168365478516\n",
      "Epoch: 97/5000, Train Loss: 59.69234015724876, Valid Loss: 58.572120666503906\n",
      "Epoch: 98/5000, Train Loss: 59.695276433771305, Valid Loss: 58.58257039388021\n",
      "Epoch: 99/5000, Train Loss: 59.58171220259233, Valid Loss: 58.503281911214195\n",
      "Epoch: 100/5000, Train Loss: 59.497921683571555, Valid Loss: 58.45655059814453\n",
      "Epoch: 101/5000, Train Loss: 59.48341543024237, Valid Loss: 58.3629264831543\n",
      "Epoch: 102/5000, Train Loss: 59.41820109974254, Valid Loss: 58.36091740926107\n",
      "Epoch: 103/5000, Train Loss: 59.33880060369318, Valid Loss: 58.315966288248696\n",
      "Epoch: 104/5000, Train Loss: 59.364416642622515, Valid Loss: 58.25004959106445\n",
      "Epoch: 105/5000, Train Loss: 59.30568625710227, Valid Loss: 58.19703674316406\n",
      "Epoch: 106/5000, Train Loss: 59.15572079745206, Valid Loss: 58.1213493347168\n",
      "Epoch: 107/5000, Train Loss: 59.15259829434481, Valid Loss: 58.14245859781901\n",
      "Epoch: 108/5000, Train Loss: 59.09504248879173, Valid Loss: 58.03129323323568\n",
      "Epoch: 109/5000, Train Loss: 59.053281957452946, Valid Loss: 58.00643412272135\n",
      "Epoch: 110/5000, Train Loss: 58.92171096801758, Valid Loss: 57.905633290608726\n",
      "Epoch: 111/5000, Train Loss: 58.83652531016957, Valid Loss: 57.85345458984375\n",
      "Epoch: 112/5000, Train Loss: 58.8406007940119, Valid Loss: 57.78301747639974\n",
      "Epoch: 113/5000, Train Loss: 58.8366945440119, Valid Loss: 57.714680989583336\n",
      "Epoch: 114/5000, Train Loss: 58.82701180197976, Valid Loss: 57.71481196085612\n",
      "Epoch: 115/5000, Train Loss: 58.64390737360174, Valid Loss: 57.629259745279946\n",
      "Epoch: 116/5000, Train Loss: 58.65664013949308, Valid Loss: 57.61498006184896\n",
      "Epoch: 117/5000, Train Loss: 58.52842088179155, Valid Loss: 57.551493326822914\n",
      "Epoch: 118/5000, Train Loss: 58.50195104425604, Valid Loss: 57.4786262512207\n",
      "Epoch: 119/5000, Train Loss: 58.41819451071999, Valid Loss: 57.42271423339844\n",
      "Epoch: 120/5000, Train Loss: 58.34428856589577, Valid Loss: 57.40676625569662\n",
      "Epoch: 121/5000, Train Loss: 58.274871826171875, Valid Loss: 57.341241200764976\n",
      "Epoch: 122/5000, Train Loss: 58.26747443459251, Valid Loss: 57.23479334513346\n",
      "Epoch: 123/5000, Train Loss: 58.176382238214664, Valid Loss: 57.14971033732096\n",
      "Epoch: 124/5000, Train Loss: 58.091282931241125, Valid Loss: 57.111626942952476\n",
      "Epoch: 125/5000, Train Loss: 58.07877557927912, Valid Loss: 57.08692169189453\n",
      "Epoch: 126/5000, Train Loss: 57.90145839344371, Valid Loss: 57.07814025878906\n",
      "Epoch: 127/5000, Train Loss: 57.9236252524636, Valid Loss: 56.96252314249674\n",
      "Epoch: 128/5000, Train Loss: 57.84264547174627, Valid Loss: 56.86704889933268\n",
      "Epoch: 129/5000, Train Loss: 57.75300771539862, Valid Loss: 56.889853159586586\n",
      "Epoch: 130/5000, Train Loss: 57.779049613259055, Valid Loss: 56.82362620035807\n",
      "Epoch: 131/5000, Train Loss: 57.70282884077592, Valid Loss: 56.7215207417806\n",
      "Epoch: 132/5000, Train Loss: 57.65821526267312, Valid Loss: 56.6526845296224\n",
      "Epoch: 133/5000, Train Loss: 57.52968493374911, Valid Loss: 56.64322026570638\n",
      "Epoch: 134/5000, Train Loss: 57.52627459439364, Valid Loss: 56.59778086344401\n",
      "Epoch: 135/5000, Train Loss: 57.49869988181374, Valid Loss: 56.519554138183594\n",
      "Epoch: 136/5000, Train Loss: 57.40495577725497, Valid Loss: 56.37104288736979\n",
      "Epoch: 137/5000, Train Loss: 57.41553809426048, Valid Loss: 56.36404164632162\n",
      "Epoch: 138/5000, Train Loss: 57.30808292735707, Valid Loss: 56.39163080851237\n",
      "Epoch: 139/5000, Train Loss: 57.219421733509414, Valid Loss: 56.2525749206543\n",
      "Epoch: 140/5000, Train Loss: 57.08974283391779, Valid Loss: 56.14513397216797\n",
      "Epoch: 141/5000, Train Loss: 57.02728132768111, Valid Loss: 56.10736846923828\n",
      "Epoch: 142/5000, Train Loss: 57.00225205854936, Valid Loss: 56.067186991373696\n",
      "Epoch: 143/5000, Train Loss: 56.97827564586293, Valid Loss: 56.032063802083336\n",
      "Epoch: 144/5000, Train Loss: 56.83717901056463, Valid Loss: 55.962320963541664\n",
      "Epoch: 145/5000, Train Loss: 56.83540205522017, Valid Loss: 55.911275227864586\n",
      "Epoch: 146/5000, Train Loss: 56.70317077636719, Valid Loss: 55.79012934366862\n",
      "Epoch: 147/5000, Train Loss: 56.6058516068892, Valid Loss: 55.74130630493164\n",
      "Epoch: 148/5000, Train Loss: 56.54044099287553, Valid Loss: 55.618675231933594\n",
      "Epoch: 149/5000, Train Loss: 56.52725497159091, Valid Loss: 55.602728525797524\n",
      "Epoch: 150/5000, Train Loss: 56.48794798417525, Valid Loss: 55.52221934000651\n",
      "Epoch: 151/5000, Train Loss: 56.35626220703125, Valid Loss: 55.51940155029297\n",
      "Epoch: 152/5000, Train Loss: 56.34546453302557, Valid Loss: 55.47964859008789\n",
      "Epoch: 153/5000, Train Loss: 56.239763433283024, Valid Loss: 55.43673324584961\n",
      "Epoch: 154/5000, Train Loss: 56.26715434681285, Valid Loss: 55.474709828694664\n",
      "Epoch: 155/5000, Train Loss: 56.12893850153143, Valid Loss: 55.303609212239586\n",
      "Epoch: 156/5000, Train Loss: 56.08784484863281, Valid Loss: 55.21487045288086\n",
      "Epoch: 157/5000, Train Loss: 56.08981045809659, Valid Loss: 55.17335001627604\n",
      "Epoch: 158/5000, Train Loss: 56.04342547329989, Valid Loss: 55.160667419433594\n",
      "Epoch: 159/5000, Train Loss: 55.924714521928266, Valid Loss: 55.06595357259115\n",
      "Epoch: 160/5000, Train Loss: 55.873788660222836, Valid Loss: 54.97935358683268\n",
      "Epoch: 161/5000, Train Loss: 55.80586728182706, Valid Loss: 54.94548416137695\n",
      "Epoch: 162/5000, Train Loss: 55.73777666958895, Valid Loss: 54.90677388509115\n",
      "Epoch: 163/5000, Train Loss: 55.670270746404476, Valid Loss: 54.882100423177086\n",
      "Epoch: 164/5000, Train Loss: 55.582074252041906, Valid Loss: 54.76737721761068\n",
      "Epoch: 165/5000, Train Loss: 55.490652257745914, Valid Loss: 54.73912811279297\n",
      "Epoch: 166/5000, Train Loss: 55.47760599309748, Valid Loss: 54.60742060343424\n",
      "Epoch: 167/5000, Train Loss: 55.37433450872248, Valid Loss: 54.514539082845054\n",
      "Epoch: 168/5000, Train Loss: 55.32520953091708, Valid Loss: 54.45661163330078\n",
      "Epoch: 169/5000, Train Loss: 55.33683846213601, Valid Loss: 54.34748967488607\n",
      "Epoch: 170/5000, Train Loss: 55.155602888627485, Valid Loss: 54.37943649291992\n",
      "Epoch: 171/5000, Train Loss: 55.07655022361062, Valid Loss: 54.28754806518555\n",
      "Epoch: 172/5000, Train Loss: 55.08588790893555, Valid Loss: 54.27291742960612\n",
      "Epoch: 173/5000, Train Loss: 54.98844493519176, Valid Loss: 54.24812698364258\n",
      "Epoch: 174/5000, Train Loss: 54.91711356423118, Valid Loss: 54.177069346110024\n",
      "Epoch: 175/5000, Train Loss: 54.80735119906339, Valid Loss: 54.10840733846029\n",
      "Epoch: 176/5000, Train Loss: 54.830599004572086, Valid Loss: 54.03022384643555\n",
      "Epoch: 177/5000, Train Loss: 54.79733553799716, Valid Loss: 53.89322153727213\n",
      "Epoch: 178/5000, Train Loss: 54.6338285966353, Valid Loss: 53.8811149597168\n",
      "Epoch: 179/5000, Train Loss: 54.62321402809837, Valid Loss: 53.83796946207682\n",
      "Epoch: 180/5000, Train Loss: 54.51366008411754, Valid Loss: 53.803244272867836\n",
      "Epoch: 181/5000, Train Loss: 54.505065571178086, Valid Loss: 53.70379511515299\n",
      "Epoch: 182/5000, Train Loss: 54.34550649469549, Valid Loss: 53.73284912109375\n",
      "Epoch: 183/5000, Train Loss: 54.329524993896484, Valid Loss: 53.642171223958336\n",
      "Epoch: 184/5000, Train Loss: 54.28264652598988, Valid Loss: 53.50636418660482\n",
      "Epoch: 185/5000, Train Loss: 54.16958652843129, Valid Loss: 53.471649169921875\n",
      "Epoch: 186/5000, Train Loss: 54.10363145308061, Valid Loss: 53.378554026285805\n",
      "Epoch: 187/5000, Train Loss: 53.91735284978693, Valid Loss: 53.390543619791664\n",
      "Epoch: 188/5000, Train Loss: 53.993779615922406, Valid Loss: 53.26013946533203\n",
      "Epoch: 189/5000, Train Loss: 54.01664837923917, Valid Loss: 53.188883463541664\n",
      "Epoch: 190/5000, Train Loss: 53.806889967484906, Valid Loss: 53.1826057434082\n",
      "Epoch: 191/5000, Train Loss: 53.78247729214755, Valid Loss: 53.138206481933594\n",
      "Epoch: 192/5000, Train Loss: 53.79548228870738, Valid Loss: 52.99370320638021\n",
      "Epoch: 193/5000, Train Loss: 53.54777977683327, Valid Loss: 52.917588551839195\n",
      "Epoch: 194/5000, Train Loss: 53.553787578235976, Valid Loss: 52.86967468261719\n",
      "Epoch: 195/5000, Train Loss: 53.56326744773171, Valid Loss: 52.770240783691406\n",
      "Epoch: 196/5000, Train Loss: 53.464379050514914, Valid Loss: 52.750779469807945\n",
      "Epoch: 197/5000, Train Loss: 53.3701945218173, Valid Loss: 52.72057978312174\n",
      "Epoch: 198/5000, Train Loss: 53.35088764537465, Valid Loss: 52.73098627726237\n",
      "Epoch: 199/5000, Train Loss: 53.29159441861239, Valid Loss: 52.59870529174805\n",
      "Epoch: 200/5000, Train Loss: 53.10998257723722, Valid Loss: 52.49322382609049\n",
      "Epoch: 201/5000, Train Loss: 53.05311965942383, Valid Loss: 52.48136901855469\n",
      "Epoch: 202/5000, Train Loss: 52.955297990278765, Valid Loss: 52.40704600016276\n",
      "Epoch: 203/5000, Train Loss: 52.97560605135831, Valid Loss: 52.32925542195638\n",
      "Epoch: 204/5000, Train Loss: 52.88077718561346, Valid Loss: 52.25460306803385\n",
      "Epoch: 205/5000, Train Loss: 52.831232591108844, Valid Loss: 52.23401006062826\n",
      "Epoch: 206/5000, Train Loss: 52.64573218605735, Valid Loss: 52.14193852742513\n",
      "Epoch: 207/5000, Train Loss: 52.697449770840734, Valid Loss: 52.10164260864258\n",
      "Epoch: 208/5000, Train Loss: 52.5287919477983, Valid Loss: 51.94225947062174\n",
      "Epoch: 209/5000, Train Loss: 52.70446222478693, Valid Loss: 51.94537099202474\n",
      "Epoch: 210/5000, Train Loss: 52.50028194080699, Valid Loss: 51.86125564575195\n",
      "Epoch: 211/5000, Train Loss: 52.32884285666726, Valid Loss: 51.896661122639976\n",
      "Epoch: 212/5000, Train Loss: 52.223169500177555, Valid Loss: 51.69984690348307\n",
      "Epoch: 213/5000, Train Loss: 52.36178068681197, Valid Loss: 51.67353185017904\n",
      "Epoch: 214/5000, Train Loss: 52.284853501753375, Valid Loss: 51.72317377726237\n",
      "Epoch: 215/5000, Train Loss: 52.15252685546875, Valid Loss: 51.628255208333336\n",
      "Epoch: 216/5000, Train Loss: 52.09431388161399, Valid Loss: 51.52936681111654\n",
      "Epoch: 217/5000, Train Loss: 52.02794231068004, Valid Loss: 51.49492390950521\n",
      "Epoch: 218/5000, Train Loss: 51.887090162797406, Valid Loss: 51.31453069051107\n",
      "Epoch: 219/5000, Train Loss: 51.822947068647906, Valid Loss: 51.23576736450195\n",
      "Epoch: 220/5000, Train Loss: 51.79651017622514, Valid Loss: 51.19140879313151\n",
      "Epoch: 221/5000, Train Loss: 51.6460768959739, Valid Loss: 50.995505015055336\n",
      "Epoch: 222/5000, Train Loss: 51.600400404496625, Valid Loss: 51.0764414469401\n",
      "Epoch: 223/5000, Train Loss: 51.605314081365414, Valid Loss: 51.08789825439453\n",
      "Epoch: 224/5000, Train Loss: 51.603165019642226, Valid Loss: 51.016334533691406\n",
      "Epoch: 225/5000, Train Loss: 51.501369129527696, Valid Loss: 50.983909606933594\n",
      "Epoch: 226/5000, Train Loss: 51.33177254416726, Valid Loss: 50.80634307861328\n",
      "Epoch: 227/5000, Train Loss: 51.30504296042702, Valid Loss: 50.81930287679037\n",
      "Epoch: 228/5000, Train Loss: 51.29858883944425, Valid Loss: 50.766073862711586\n",
      "Epoch: 229/5000, Train Loss: 51.16547913984819, Valid Loss: 50.709704081217446\n",
      "Epoch: 230/5000, Train Loss: 51.131832122802734, Valid Loss: 50.673423767089844\n",
      "Epoch: 231/5000, Train Loss: 51.00815200805664, Valid Loss: 50.61390813191732\n",
      "Epoch: 232/5000, Train Loss: 51.02944148670543, Valid Loss: 50.48028691609701\n",
      "Epoch: 233/5000, Train Loss: 50.91628716208718, Valid Loss: 50.51702880859375\n",
      "Epoch: 234/5000, Train Loss: 50.84519958496094, Valid Loss: 50.37051645914713\n",
      "Epoch: 235/5000, Train Loss: 50.814089341597125, Valid Loss: 50.31287638346354\n",
      "Epoch: 236/5000, Train Loss: 50.75369852239435, Valid Loss: 50.265968322753906\n",
      "Epoch: 237/5000, Train Loss: 50.68611630526456, Valid Loss: 50.14636484781901\n",
      "Epoch: 238/5000, Train Loss: 50.589486208829015, Valid Loss: 50.041690826416016\n",
      "Epoch: 239/5000, Train Loss: 50.517053083939985, Valid Loss: 49.976061503092446\n",
      "Epoch: 240/5000, Train Loss: 50.55502978238192, Valid Loss: 49.967674255371094\n",
      "Epoch: 241/5000, Train Loss: 50.45795822143555, Valid Loss: 49.97507858276367\n",
      "Epoch: 242/5000, Train Loss: 50.24775453047319, Valid Loss: 49.91123708089193\n",
      "Epoch: 243/5000, Train Loss: 50.2896860296076, Valid Loss: 49.924704233805336\n",
      "Epoch: 244/5000, Train Loss: 50.10112068869851, Valid Loss: 49.89312235514323\n",
      "Epoch: 245/5000, Train Loss: 50.14980766989968, Valid Loss: 49.724379221598305\n",
      "Epoch: 246/5000, Train Loss: 50.10425775701349, Valid Loss: 49.724751790364586\n",
      "Epoch: 247/5000, Train Loss: 49.88858413696289, Valid Loss: 49.60320917765299\n",
      "Epoch: 248/5000, Train Loss: 49.91417243263938, Valid Loss: 49.473524729410805\n",
      "Epoch: 249/5000, Train Loss: 49.63450137051669, Valid Loss: 49.543968200683594\n",
      "Epoch: 250/5000, Train Loss: 49.65274290605025, Valid Loss: 49.33035786946615\n",
      "Epoch: 251/5000, Train Loss: 49.6986777565696, Valid Loss: 49.32326889038086\n",
      "Epoch: 252/5000, Train Loss: 49.63519287109375, Valid Loss: 49.33108393351237\n",
      "Epoch: 253/5000, Train Loss: 49.5283449346369, Valid Loss: 49.16833750406901\n",
      "Epoch: 254/5000, Train Loss: 49.4424601468173, Valid Loss: 49.073630015055336\n",
      "Epoch: 255/5000, Train Loss: 49.39281116832387, Valid Loss: 49.07796096801758\n",
      "Epoch: 256/5000, Train Loss: 49.33694735440341, Valid Loss: 49.04422505696615\n",
      "Epoch: 257/5000, Train Loss: 49.188956173983485, Valid Loss: 48.86237080891927\n",
      "Epoch: 258/5000, Train Loss: 49.23294379494407, Valid Loss: 48.8831418355306\n",
      "Epoch: 259/5000, Train Loss: 49.10045727816495, Valid Loss: 48.800008138020836\n",
      "Epoch: 260/5000, Train Loss: 48.96889253096147, Valid Loss: 48.76050694783529\n",
      "Epoch: 261/5000, Train Loss: 48.85105029019442, Valid Loss: 48.7453244527181\n",
      "Epoch: 262/5000, Train Loss: 49.024010398171164, Valid Loss: 48.6057014465332\n",
      "Epoch: 263/5000, Train Loss: 48.851249348033555, Valid Loss: 48.56116739908854\n",
      "Epoch: 264/5000, Train Loss: 48.67386211048473, Valid Loss: 48.53577168782552\n",
      "Epoch: 265/5000, Train Loss: 48.7529262195934, Valid Loss: 48.43136469523112\n",
      "Epoch: 266/5000, Train Loss: 48.59033341841264, Valid Loss: 48.36748377482096\n",
      "Epoch: 267/5000, Train Loss: 48.66192557594993, Valid Loss: 48.38383102416992\n",
      "Epoch: 268/5000, Train Loss: 48.53495129671964, Valid Loss: 48.27453104654948\n",
      "Epoch: 269/5000, Train Loss: 48.35588281804865, Valid Loss: 48.231727600097656\n",
      "Epoch: 270/5000, Train Loss: 48.39079388705167, Valid Loss: 48.26480611165365\n",
      "Epoch: 271/5000, Train Loss: 48.36030127785423, Valid Loss: 48.10459009806315\n",
      "Epoch: 272/5000, Train Loss: 48.29043925892223, Valid Loss: 48.033260345458984\n",
      "Epoch: 273/5000, Train Loss: 48.155973954634234, Valid Loss: 48.02929433186849\n",
      "Epoch: 274/5000, Train Loss: 48.24547923694957, Valid Loss: 47.91679000854492\n",
      "Epoch: 275/5000, Train Loss: 48.083145488392226, Valid Loss: 47.842247009277344\n",
      "Epoch: 276/5000, Train Loss: 47.943206093528055, Valid Loss: 47.76512781778971\n",
      "Epoch: 277/5000, Train Loss: 47.93902310458097, Valid Loss: 47.708291371663414\n",
      "Epoch: 278/5000, Train Loss: 47.8426364551891, Valid Loss: 47.684364318847656\n",
      "Epoch: 279/5000, Train Loss: 47.80678350275213, Valid Loss: 47.59882609049479\n",
      "Epoch: 280/5000, Train Loss: 47.69069914384322, Valid Loss: 47.525105794270836\n",
      "Epoch: 281/5000, Train Loss: 47.7906740361994, Valid Loss: 47.46768442789713\n",
      "Epoch: 282/5000, Train Loss: 47.56528056751598, Valid Loss: 47.46594492594401\n",
      "Epoch: 283/5000, Train Loss: 47.590660095214844, Valid Loss: 47.38677088419596\n",
      "Epoch: 284/5000, Train Loss: 47.32509231567383, Valid Loss: 47.41235605875651\n",
      "Epoch: 285/5000, Train Loss: 47.33546829223633, Valid Loss: 47.27798334757487\n",
      "Epoch: 286/5000, Train Loss: 47.35710213401101, Valid Loss: 47.229852040608726\n",
      "Epoch: 287/5000, Train Loss: 47.19977222789418, Valid Loss: 47.068703969319664\n",
      "Epoch: 288/5000, Train Loss: 46.989201979203656, Valid Loss: 47.0223274230957\n",
      "Epoch: 289/5000, Train Loss: 47.00459705699574, Valid Loss: 47.08735275268555\n",
      "Epoch: 290/5000, Train Loss: 47.01498725197532, Valid Loss: 47.022850036621094\n",
      "Epoch: 291/5000, Train Loss: 46.892057245427914, Valid Loss: 46.9685910542806\n",
      "Epoch: 292/5000, Train Loss: 46.77780463478782, Valid Loss: 46.88119633992513\n",
      "Epoch: 293/5000, Train Loss: 46.63523379239169, Valid Loss: 46.76992162068685\n",
      "Epoch: 294/5000, Train Loss: 46.65532684326172, Valid Loss: 46.738234202067055\n",
      "Epoch: 295/5000, Train Loss: 46.675937999378554, Valid Loss: 46.700923919677734\n",
      "Epoch: 296/5000, Train Loss: 46.66938018798828, Valid Loss: 46.57740275065104\n",
      "Epoch: 297/5000, Train Loss: 46.56512104381215, Valid Loss: 46.624226888020836\n",
      "Epoch: 298/5000, Train Loss: 46.41478035666726, Valid Loss: 46.575433095296226\n",
      "Epoch: 299/5000, Train Loss: 46.3645054210316, Valid Loss: 46.45724741617838\n",
      "Epoch: 300/5000, Train Loss: 46.24016016179865, Valid Loss: 46.35688145955404\n",
      "Epoch: 301/5000, Train Loss: 46.15564658425071, Valid Loss: 46.33298873901367\n",
      "Epoch: 302/5000, Train Loss: 46.145289334383875, Valid Loss: 46.2419802347819\n",
      "Epoch: 303/5000, Train Loss: 46.21525330977006, Valid Loss: 46.21485137939453\n",
      "Epoch: 304/5000, Train Loss: 46.034799402410336, Valid Loss: 46.08087158203125\n",
      "Epoch: 305/5000, Train Loss: 46.05623037164862, Valid Loss: 46.09257380167643\n",
      "Epoch: 306/5000, Train Loss: 46.03267218849876, Valid Loss: 45.95164362589518\n",
      "Epoch: 307/5000, Train Loss: 45.854000091552734, Valid Loss: 45.93537139892578\n",
      "Epoch: 308/5000, Train Loss: 45.83212557705966, Valid Loss: 45.97474161783854\n",
      "Epoch: 309/5000, Train Loss: 45.73455533114347, Valid Loss: 45.867838541666664\n",
      "Epoch: 310/5000, Train Loss: 45.70646702159535, Valid Loss: 45.739121754964195\n",
      "Epoch: 311/5000, Train Loss: 45.55574000965465, Valid Loss: 45.6060905456543\n",
      "Epoch: 312/5000, Train Loss: 45.42176922884855, Valid Loss: 45.70527776082357\n",
      "Epoch: 313/5000, Train Loss: 45.49128098921342, Valid Loss: 45.543890635172524\n",
      "Epoch: 314/5000, Train Loss: 45.49550663341176, Valid Loss: 45.47406514485677\n",
      "Epoch: 315/5000, Train Loss: 45.35087758844549, Valid Loss: 45.48872756958008\n",
      "Epoch: 316/5000, Train Loss: 45.5163608897816, Valid Loss: 45.44708251953125\n",
      "Epoch: 317/5000, Train Loss: 45.299743305553086, Valid Loss: 45.31971740722656\n",
      "Epoch: 318/5000, Train Loss: 45.1767130765048, Valid Loss: 45.32007853190104\n",
      "Epoch: 319/5000, Train Loss: 45.17183581265536, Valid Loss: 45.215990702311196\n",
      "Epoch: 320/5000, Train Loss: 44.99760367653587, Valid Loss: 45.109238942464195\n",
      "Epoch: 321/5000, Train Loss: 45.06820089166815, Valid Loss: 45.08953348795573\n",
      "Epoch: 322/5000, Train Loss: 44.98860411210494, Valid Loss: 45.031576792399086\n",
      "Epoch: 323/5000, Train Loss: 44.93891143798828, Valid Loss: 44.87562688191732\n",
      "Epoch: 324/5000, Train Loss: 44.61829341541637, Valid Loss: 44.87905756632487\n",
      "Epoch: 325/5000, Train Loss: 44.80468091097745, Valid Loss: 44.869056701660156\n",
      "Epoch: 326/5000, Train Loss: 44.55069143121893, Valid Loss: 44.83854548136393\n",
      "Epoch: 327/5000, Train Loss: 44.731811176646836, Valid Loss: 44.74571863810221\n",
      "Epoch: 328/5000, Train Loss: 44.55693955854936, Valid Loss: 44.65228525797526\n",
      "Epoch: 329/5000, Train Loss: 44.531498648903586, Valid Loss: 44.62440872192383\n",
      "Epoch: 330/5000, Train Loss: 44.35029498013583, Valid Loss: 44.63911565144857\n",
      "Epoch: 331/5000, Train Loss: 44.405209281227805, Valid Loss: 44.625256856282554\n",
      "Epoch: 332/5000, Train Loss: 44.28957852450284, Valid Loss: 44.436787923177086\n",
      "Epoch: 333/5000, Train Loss: 44.25648047707298, Valid Loss: 44.47809727986654\n",
      "Epoch: 334/5000, Train Loss: 44.230983387340196, Valid Loss: 44.38954289754232\n",
      "Epoch: 335/5000, Train Loss: 44.057588057084516, Valid Loss: 44.29194641113281\n",
      "Epoch: 336/5000, Train Loss: 43.954193115234375, Valid Loss: 44.152244567871094\n",
      "Epoch: 337/5000, Train Loss: 44.00979094071822, Valid Loss: 44.20538584391276\n",
      "Epoch: 338/5000, Train Loss: 44.09186068448153, Valid Loss: 44.12273279825846\n",
      "Epoch: 339/5000, Train Loss: 43.93541786887429, Valid Loss: 44.06929016113281\n",
      "Epoch: 340/5000, Train Loss: 44.015119379216976, Valid Loss: 44.1416130065918\n",
      "Epoch: 341/5000, Train Loss: 43.786705710671164, Valid Loss: 44.028212229410805\n",
      "Epoch: 342/5000, Train Loss: 43.95187031139027, Valid Loss: 43.798781077067055\n",
      "Epoch: 343/5000, Train Loss: 43.823878201571375, Valid Loss: 43.80329386393229\n",
      "Epoch: 344/5000, Train Loss: 43.68890242143111, Valid Loss: 43.773573557535805\n",
      "Epoch: 345/5000, Train Loss: 43.519681757146664, Valid Loss: 43.758490244547524\n",
      "Epoch: 346/5000, Train Loss: 43.47270306673917, Valid Loss: 43.75943501790365\n",
      "Epoch: 347/5000, Train Loss: 43.398197174072266, Valid Loss: 43.61281077067057\n",
      "Epoch: 348/5000, Train Loss: 43.58305809714577, Valid Loss: 43.55811564127604\n",
      "Epoch: 349/5000, Train Loss: 43.3954682783647, Valid Loss: 43.57190704345703\n",
      "Epoch: 350/5000, Train Loss: 43.14295127175071, Valid Loss: 43.567822774251304\n",
      "Epoch: 351/5000, Train Loss: 43.22551761973988, Valid Loss: 43.3107795715332\n",
      "Epoch: 352/5000, Train Loss: 43.18999689275568, Valid Loss: 43.358080546061196\n",
      "Epoch: 353/5000, Train Loss: 43.057980277321555, Valid Loss: 43.28326161702474\n",
      "Epoch: 354/5000, Train Loss: 42.93605561689897, Valid Loss: 43.28539911905924\n",
      "Epoch: 355/5000, Train Loss: 42.98592792857777, Valid Loss: 43.188299814860024\n",
      "Epoch: 356/5000, Train Loss: 42.788930372758344, Valid Loss: 43.10341008504232\n",
      "Epoch: 357/5000, Train Loss: 42.81361077048562, Valid Loss: 43.02473576863607\n",
      "Epoch: 358/5000, Train Loss: 42.606877066872336, Valid Loss: 42.98975499471029\n",
      "Epoch: 359/5000, Train Loss: 42.890365253795274, Valid Loss: 42.87794876098633\n",
      "Epoch: 360/5000, Train Loss: 42.55948985706676, Valid Loss: 42.946231842041016\n",
      "Epoch: 361/5000, Train Loss: 42.648002277721055, Valid Loss: 42.929430643717446\n",
      "Epoch: 362/5000, Train Loss: 42.78274362737482, Valid Loss: 42.724650065104164\n",
      "Epoch: 363/5000, Train Loss: 42.50407791137695, Valid Loss: 42.68001047770182\n",
      "Epoch: 364/5000, Train Loss: 42.2837607643821, Valid Loss: 42.680739084879555\n",
      "Epoch: 365/5000, Train Loss: 42.249323411421344, Valid Loss: 42.64961496988932\n",
      "Epoch: 366/5000, Train Loss: 42.28795797174627, Valid Loss: 42.61449178059896\n",
      "Epoch: 367/5000, Train Loss: 42.31431648947976, Valid Loss: 42.47279485066732\n",
      "Epoch: 368/5000, Train Loss: 42.09872783314098, Valid Loss: 42.531359354654946\n",
      "Epoch: 369/5000, Train Loss: 42.025574770840734, Valid Loss: 42.38958740234375\n",
      "Epoch: 370/5000, Train Loss: 41.91052974354137, Valid Loss: 42.40222930908203\n",
      "Epoch: 371/5000, Train Loss: 41.96794059059837, Valid Loss: 42.340216318766274\n",
      "Epoch: 372/5000, Train Loss: 42.13179883089933, Valid Loss: 42.37895075480143\n",
      "Epoch: 373/5000, Train Loss: 41.9455531727184, Valid Loss: 42.28021748860677\n",
      "Epoch: 374/5000, Train Loss: 42.02196433327415, Valid Loss: 42.27640024820963\n",
      "Epoch: 375/5000, Train Loss: 41.77874027598988, Valid Loss: 42.167012532552086\n",
      "Epoch: 376/5000, Train Loss: 41.43232623013583, Valid Loss: 42.050706227620445\n",
      "Epoch: 377/5000, Train Loss: 41.669015017422765, Valid Loss: 42.06285730997721\n",
      "Epoch: 378/5000, Train Loss: 41.6107590415261, Valid Loss: 41.96389897664388\n",
      "Epoch: 379/5000, Train Loss: 41.64404574307528, Valid Loss: 41.88370259602865\n",
      "Epoch: 380/5000, Train Loss: 41.51243417913263, Valid Loss: 41.92859649658203\n",
      "Epoch: 381/5000, Train Loss: 41.381515156139024, Valid Loss: 41.79163106282552\n",
      "Epoch: 382/5000, Train Loss: 41.04537617076527, Valid Loss: 41.77586237589518\n",
      "Epoch: 383/5000, Train Loss: 41.29290563409979, Valid Loss: 41.851741790771484\n",
      "Epoch: 384/5000, Train Loss: 41.25155327536843, Valid Loss: 41.82367451985677\n",
      "Epoch: 385/5000, Train Loss: 41.083251606334336, Valid Loss: 41.735277811686196\n",
      "Epoch: 386/5000, Train Loss: 41.19011376120827, Valid Loss: 41.526116689046226\n",
      "Epoch: 387/5000, Train Loss: 41.1748723116788, Valid Loss: 41.582427978515625\n",
      "Epoch: 388/5000, Train Loss: 41.04175706343217, Valid Loss: 41.55149459838867\n",
      "Epoch: 389/5000, Train Loss: 40.98259180242365, Valid Loss: 41.456494649251304\n",
      "Epoch: 390/5000, Train Loss: 41.01425483010032, Valid Loss: 41.35091908772787\n",
      "Epoch: 391/5000, Train Loss: 41.15196644176137, Valid Loss: 41.28904596964518\n",
      "Epoch: 392/5000, Train Loss: 41.03119035200639, Valid Loss: 41.265323638916016\n",
      "Epoch: 393/5000, Train Loss: 40.84701260653409, Valid Loss: 41.33748881022135\n",
      "Epoch: 394/5000, Train Loss: 40.681089574640446, Valid Loss: 41.21403121948242\n",
      "Epoch: 395/5000, Train Loss: 40.540193037553266, Valid Loss: 41.18036015828451\n",
      "Epoch: 396/5000, Train Loss: 40.626952084628016, Valid Loss: 41.09805043538412\n",
      "Epoch: 397/5000, Train Loss: 40.62431820956144, Valid Loss: 41.16938781738281\n",
      "Epoch: 398/5000, Train Loss: 40.44994701038707, Valid Loss: 41.035691579182945\n",
      "Epoch: 399/5000, Train Loss: 40.1999907060103, Valid Loss: 40.95131810506185\n",
      "Epoch: 400/5000, Train Loss: 40.40363935990767, Valid Loss: 40.920782725016274\n",
      "Epoch: 401/5000, Train Loss: 40.307390386408024, Valid Loss: 40.858351389567055\n",
      "Epoch: 402/5000, Train Loss: 40.32865593650124, Valid Loss: 40.89086786905924\n",
      "Epoch: 403/5000, Train Loss: 40.23375875299627, Valid Loss: 40.76670583089193\n",
      "Epoch: 404/5000, Train Loss: 40.403411865234375, Valid Loss: 40.798108418782554\n",
      "Epoch: 405/5000, Train Loss: 40.22730775312944, Valid Loss: 40.6047477722168\n",
      "Epoch: 406/5000, Train Loss: 40.00580042058771, Valid Loss: 40.57235972086588\n",
      "Epoch: 407/5000, Train Loss: 39.954607183283024, Valid Loss: 40.59789021809896\n",
      "Epoch: 408/5000, Train Loss: 39.860814874822445, Valid Loss: 40.53010686238607\n",
      "Epoch: 409/5000, Train Loss: 39.96770026467063, Valid Loss: 40.436211903889976\n",
      "Epoch: 410/5000, Train Loss: 40.12270806052468, Valid Loss: 40.410048166910805\n",
      "Epoch: 411/5000, Train Loss: 39.91473319313743, Valid Loss: 40.36764272054037\n",
      "Epoch: 412/5000, Train Loss: 39.66561785611239, Valid Loss: 40.2386105855306\n",
      "Epoch: 413/5000, Train Loss: 39.71543468128551, Valid Loss: 40.32999165852865\n",
      "Epoch: 414/5000, Train Loss: 39.67809018221769, Valid Loss: 40.19625600179037\n",
      "Epoch: 415/5000, Train Loss: 39.87484637173739, Valid Loss: 40.09247589111328\n",
      "Epoch: 416/5000, Train Loss: 39.56770116632635, Valid Loss: 40.13547388712565\n",
      "Epoch: 417/5000, Train Loss: 39.48765147816051, Valid Loss: 40.21549860636393\n",
      "Epoch: 418/5000, Train Loss: 39.47208300503817, Valid Loss: 40.15278625488281\n",
      "Epoch: 419/5000, Train Loss: 39.49830280650746, Valid Loss: 40.01940409342448\n",
      "Epoch: 420/5000, Train Loss: 39.496694044633344, Valid Loss: 40.0287717183431\n",
      "Epoch: 421/5000, Train Loss: 39.23063798384233, Valid Loss: 40.011817932128906\n",
      "Epoch: 422/5000, Train Loss: 39.331355355002664, Valid Loss: 39.920702616373696\n",
      "Epoch: 423/5000, Train Loss: 39.170856129039414, Valid Loss: 39.78768412272135\n",
      "Epoch: 424/5000, Train Loss: 39.333216580477625, Valid Loss: 39.744635264078774\n",
      "Epoch: 425/5000, Train Loss: 39.15306230024858, Valid Loss: 39.758453369140625\n",
      "Epoch: 426/5000, Train Loss: 39.1507436578924, Valid Loss: 39.578739166259766\n",
      "Epoch: 427/5000, Train Loss: 39.020464116876774, Valid Loss: 39.634745279947914\n",
      "Epoch: 428/5000, Train Loss: 39.221310355446555, Valid Loss: 39.61914825439453\n",
      "Epoch: 429/5000, Train Loss: 38.989859494295985, Valid Loss: 39.502480824788414\n",
      "Epoch: 430/5000, Train Loss: 39.04101978648793, Valid Loss: 39.55997848510742\n",
      "Epoch: 431/5000, Train Loss: 38.947926954789594, Valid Loss: 39.532021840413414\n",
      "Epoch: 432/5000, Train Loss: 39.093487826260656, Valid Loss: 39.47329076131185\n",
      "Epoch: 433/5000, Train Loss: 38.75763841108842, Valid Loss: 39.44609069824219\n",
      "Epoch: 434/5000, Train Loss: 38.86450888893821, Valid Loss: 39.329627990722656\n",
      "Epoch: 435/5000, Train Loss: 38.832767486572266, Valid Loss: 39.284863789876304\n",
      "Epoch: 436/5000, Train Loss: 38.693265741521664, Valid Loss: 39.28002039591471\n",
      "Epoch: 437/5000, Train Loss: 38.39306085759943, Valid Loss: 39.21842575073242\n",
      "Epoch: 438/5000, Train Loss: 38.67872134121981, Valid Loss: 39.1021359761556\n",
      "Epoch: 439/5000, Train Loss: 38.62820087779652, Valid Loss: 39.1395632425944\n",
      "Epoch: 440/5000, Train Loss: 38.556027152321555, Valid Loss: 39.108709971110024\n",
      "Epoch: 441/5000, Train Loss: 38.39885399558327, Valid Loss: 39.07284418741862\n",
      "Epoch: 442/5000, Train Loss: 38.46275572343306, Valid Loss: 39.01369857788086\n",
      "Epoch: 443/5000, Train Loss: 38.638295607133344, Valid Loss: 39.00913874308268\n",
      "Epoch: 444/5000, Train Loss: 38.40589904785156, Valid Loss: 38.95605977376302\n",
      "Epoch: 445/5000, Train Loss: 38.372502413663, Valid Loss: 38.93073399861654\n",
      "Epoch: 446/5000, Train Loss: 38.31531177867543, Valid Loss: 38.83557383219401\n",
      "Epoch: 447/5000, Train Loss: 38.24942744861949, Valid Loss: 38.78563563028971\n",
      "Epoch: 448/5000, Train Loss: 38.23902615633878, Valid Loss: 38.7900644938151\n",
      "Epoch: 449/5000, Train Loss: 38.05485777421431, Valid Loss: 38.756326039632164\n",
      "Epoch: 450/5000, Train Loss: 37.96676670421254, Valid Loss: 38.69023513793945\n",
      "Epoch: 451/5000, Train Loss: 37.90589349920099, Valid Loss: 38.62170155843099\n",
      "Epoch: 452/5000, Train Loss: 37.934648687189274, Valid Loss: 38.4921875\n",
      "Epoch: 453/5000, Train Loss: 37.9500319740989, Valid Loss: 38.57984924316406\n",
      "Epoch: 454/5000, Train Loss: 37.92508593472567, Valid Loss: 38.52541987101237\n",
      "Epoch: 455/5000, Train Loss: 37.99479501897638, Valid Loss: 38.47328567504883\n",
      "Epoch: 456/5000, Train Loss: 37.78622956709428, Valid Loss: 38.46442540486654\n",
      "Epoch: 457/5000, Train Loss: 37.62190107865767, Valid Loss: 38.399855295817055\n",
      "Epoch: 458/5000, Train Loss: 37.63434947620738, Valid Loss: 38.400952657063804\n",
      "Epoch: 459/5000, Train Loss: 37.58612407337535, Valid Loss: 38.289740244547524\n",
      "Epoch: 460/5000, Train Loss: 37.63436057350852, Valid Loss: 38.299948374430336\n",
      "Epoch: 461/5000, Train Loss: 37.573868144642226, Valid Loss: 38.263641357421875\n",
      "Epoch: 462/5000, Train Loss: 37.57001321965998, Valid Loss: 38.21472040812174\n",
      "Epoch: 463/5000, Train Loss: 37.58390010486949, Valid Loss: 38.2491200764974\n",
      "Epoch: 464/5000, Train Loss: 37.25437927246094, Valid Loss: 38.186431884765625\n",
      "Epoch: 465/5000, Train Loss: 37.32872702858665, Valid Loss: 38.0209592183431\n",
      "Epoch: 466/5000, Train Loss: 37.34938500144265, Valid Loss: 38.03120422363281\n",
      "Epoch: 467/5000, Train Loss: 37.455472772771664, Valid Loss: 38.046766916910805\n",
      "Epoch: 468/5000, Train Loss: 37.46194111217152, Valid Loss: 37.98436482747396\n",
      "Epoch: 469/5000, Train Loss: 37.06692470203746, Valid Loss: 37.983116149902344\n",
      "Epoch: 470/5000, Train Loss: 37.12675302678888, Valid Loss: 37.879014333089195\n",
      "Epoch: 471/5000, Train Loss: 37.076065410267226, Valid Loss: 37.857855478922524\n",
      "Epoch: 472/5000, Train Loss: 37.4490966796875, Valid Loss: 37.82711664835612\n",
      "Epoch: 473/5000, Train Loss: 37.21538578380238, Valid Loss: 37.783042907714844\n",
      "Epoch: 474/5000, Train Loss: 36.99233974109996, Valid Loss: 37.72550837198893\n",
      "Epoch: 475/5000, Train Loss: 37.19965154474432, Valid Loss: 37.64451599121094\n",
      "Epoch: 476/5000, Train Loss: 36.8258021961559, Valid Loss: 37.67788314819336\n",
      "Epoch: 477/5000, Train Loss: 37.24563772028143, Valid Loss: 37.6453603108724\n",
      "Epoch: 478/5000, Train Loss: 36.8032375682484, Valid Loss: 37.60946909586588\n",
      "Epoch: 479/5000, Train Loss: 36.745669625022195, Valid Loss: 37.579236348470054\n",
      "Epoch: 480/5000, Train Loss: 36.62419475208629, Valid Loss: 37.54250717163086\n",
      "Epoch: 481/5000, Train Loss: 36.670222542502664, Valid Loss: 37.47981389363607\n",
      "Epoch: 482/5000, Train Loss: 36.74272225119851, Valid Loss: 37.38719177246094\n",
      "Epoch: 483/5000, Train Loss: 36.65017422762784, Valid Loss: 37.36018753051758\n",
      "Epoch: 484/5000, Train Loss: 36.57586323131215, Valid Loss: 37.277914683024086\n",
      "Epoch: 485/5000, Train Loss: 36.61624943126332, Valid Loss: 37.219557444254555\n",
      "Epoch: 486/5000, Train Loss: 36.83580988103693, Valid Loss: 37.2779909769694\n",
      "Epoch: 487/5000, Train Loss: 36.55107498168945, Valid Loss: 37.20526123046875\n",
      "Epoch: 488/5000, Train Loss: 36.25564228404652, Valid Loss: 37.032474517822266\n",
      "Epoch: 489/5000, Train Loss: 36.60445993596857, Valid Loss: 36.98515319824219\n",
      "Epoch: 490/5000, Train Loss: 36.316682988947086, Valid Loss: 36.943904876708984\n",
      "Epoch: 491/5000, Train Loss: 36.35066292502663, Valid Loss: 36.9659309387207\n",
      "Epoch: 492/5000, Train Loss: 36.30262478915128, Valid Loss: 36.940077463785805\n",
      "Epoch: 493/5000, Train Loss: 36.052978862415664, Valid Loss: 36.918111165364586\n",
      "Epoch: 494/5000, Train Loss: 36.403806859796696, Valid Loss: 36.930739084879555\n",
      "Epoch: 495/5000, Train Loss: 36.26842845569957, Valid Loss: 36.874585469563804\n",
      "Epoch: 496/5000, Train Loss: 36.35878684303977, Valid Loss: 36.82190068562826\n",
      "Epoch: 497/5000, Train Loss: 36.21125966852362, Valid Loss: 36.826829274495445\n",
      "Epoch: 498/5000, Train Loss: 36.31120473688299, Valid Loss: 36.74621073404948\n",
      "Epoch: 499/5000, Train Loss: 36.265765797008164, Valid Loss: 36.64510854085287\n",
      "Epoch: 500/5000, Train Loss: 35.846880826083094, Valid Loss: 36.723532358805336\n",
      "Epoch: 501/5000, Train Loss: 35.861825076016515, Valid Loss: 36.70471700032552\n",
      "Epoch: 502/5000, Train Loss: 35.98649978637695, Valid Loss: 36.55792999267578\n",
      "Epoch: 503/5000, Train Loss: 35.92920407381925, Valid Loss: 36.47448857625326\n",
      "Epoch: 504/5000, Train Loss: 36.004940032958984, Valid Loss: 36.41338094075521\n",
      "Epoch: 505/5000, Train Loss: 35.92746422507546, Valid Loss: 36.428568522135414\n",
      "Epoch: 506/5000, Train Loss: 36.07710006020286, Valid Loss: 36.33047358194987\n",
      "Epoch: 507/5000, Train Loss: 35.93188753995028, Valid Loss: 36.36541493733724\n",
      "Epoch: 508/5000, Train Loss: 35.87618706443093, Valid Loss: 36.29230880737305\n",
      "Epoch: 509/5000, Train Loss: 35.85882845791903, Valid Loss: 36.3364372253418\n",
      "Epoch: 510/5000, Train Loss: 35.78654792092063, Valid Loss: 36.31128819783529\n",
      "Epoch: 511/5000, Train Loss: 35.65529251098633, Valid Loss: 36.11399714152018\n",
      "Epoch: 512/5000, Train Loss: 35.63996089588512, Valid Loss: 36.148590087890625\n",
      "Epoch: 513/5000, Train Loss: 35.57277540727095, Valid Loss: 36.10031255086263\n",
      "Epoch: 514/5000, Train Loss: 35.580963134765625, Valid Loss: 36.07415644327799\n",
      "Epoch: 515/5000, Train Loss: 35.48424079201438, Valid Loss: 36.06162643432617\n",
      "Epoch: 516/5000, Train Loss: 35.70120759443803, Valid Loss: 36.00137074788412\n",
      "Epoch: 517/5000, Train Loss: 35.62296121770685, Valid Loss: 35.947374979654946\n",
      "Epoch: 518/5000, Train Loss: 35.42235842618075, Valid Loss: 35.922072092692055\n",
      "Epoch: 519/5000, Train Loss: 35.695756218650125, Valid Loss: 35.90419387817383\n",
      "Epoch: 520/5000, Train Loss: 35.74244551225142, Valid Loss: 35.8657595316569\n",
      "Epoch: 521/5000, Train Loss: 35.51704163984819, Valid Loss: 35.86592102050781\n",
      "Epoch: 522/5000, Train Loss: 35.27979417280717, Valid Loss: 35.79452896118164\n",
      "Epoch: 523/5000, Train Loss: 35.33851346102628, Valid Loss: 35.78557586669922\n",
      "Epoch: 524/5000, Train Loss: 35.38745325261896, Valid Loss: 35.77741368611654\n",
      "Epoch: 525/5000, Train Loss: 35.31315196644176, Valid Loss: 35.797430674235024\n",
      "Epoch: 526/5000, Train Loss: 35.15270857377486, Valid Loss: 35.734920501708984\n",
      "Epoch: 527/5000, Train Loss: 35.26983850652521, Valid Loss: 35.68350346883138\n",
      "Epoch: 528/5000, Train Loss: 35.11072366887873, Valid Loss: 35.62864685058594\n",
      "Epoch: 529/5000, Train Loss: 35.36762445623224, Valid Loss: 35.60585276285807\n",
      "Epoch: 530/5000, Train Loss: 34.80158164284446, Valid Loss: 35.551649729410805\n",
      "Epoch: 531/5000, Train Loss: 35.128658294677734, Valid Loss: 35.49634552001953\n",
      "Epoch: 532/5000, Train Loss: 34.873018438165836, Valid Loss: 35.469373067220054\n",
      "Epoch: 533/5000, Train Loss: 35.2205455086448, Valid Loss: 35.470724741617836\n",
      "Epoch: 534/5000, Train Loss: 35.12697289206765, Valid Loss: 35.449761708577476\n",
      "Epoch: 535/5000, Train Loss: 34.9262299971147, Valid Loss: 35.40808359781901\n",
      "Epoch: 536/5000, Train Loss: 34.91221029108221, Valid Loss: 35.38869603474935\n",
      "Epoch: 537/5000, Train Loss: 34.58754487471147, Valid Loss: 35.32073211669922\n",
      "Epoch: 538/5000, Train Loss: 34.91059944846413, Valid Loss: 35.277880350748696\n",
      "Epoch: 539/5000, Train Loss: 34.64762167497115, Valid Loss: 35.3526980082194\n",
      "Epoch: 540/5000, Train Loss: 35.086546117609196, Valid Loss: 35.37258529663086\n",
      "Epoch: 541/5000, Train Loss: 34.81420412930575, Valid Loss: 35.31561787923177\n",
      "Epoch: 542/5000, Train Loss: 34.84952753240412, Valid Loss: 35.261905670166016\n",
      "Epoch: 543/5000, Train Loss: 34.65394765680487, Valid Loss: 35.24651336669922\n",
      "Epoch: 544/5000, Train Loss: 34.6809116710316, Valid Loss: 35.18957392374674\n",
      "Epoch: 545/5000, Train Loss: 34.65800580111417, Valid Loss: 35.064561208089195\n",
      "Epoch: 546/5000, Train Loss: 34.39947284351695, Valid Loss: 35.091548919677734\n",
      "Epoch: 547/5000, Train Loss: 34.536991812966086, Valid Loss: 35.03100840250651\n",
      "Epoch: 548/5000, Train Loss: 34.72258030284535, Valid Loss: 35.02105712890625\n",
      "Epoch: 549/5000, Train Loss: 34.67381182583895, Valid Loss: 35.08402633666992\n",
      "Epoch: 550/5000, Train Loss: 34.5485031821511, Valid Loss: 35.08641815185547\n",
      "Epoch: 551/5000, Train Loss: 34.732806119051844, Valid Loss: 35.014200846354164\n",
      "Epoch: 552/5000, Train Loss: 34.44835246693004, Valid Loss: 34.933573404947914\n",
      "Epoch: 553/5000, Train Loss: 34.304116855968125, Valid Loss: 34.85865275065104\n",
      "Epoch: 554/5000, Train Loss: 34.26014015891335, Valid Loss: 34.85082244873047\n",
      "Epoch: 555/5000, Train Loss: 34.50633066350763, Valid Loss: 34.86408233642578\n",
      "Epoch: 556/5000, Train Loss: 34.86799378828569, Valid Loss: 34.87628682454427\n",
      "Epoch: 557/5000, Train Loss: 34.48867138949308, Valid Loss: 34.85549672444662\n",
      "Epoch: 558/5000, Train Loss: 34.54368730024858, Valid Loss: 34.753058115641274\n",
      "Epoch: 559/5000, Train Loss: 34.447691830721766, Valid Loss: 34.81975173950195\n",
      "Epoch: 560/5000, Train Loss: 34.526973030783914, Valid Loss: 34.69456481933594\n",
      "Epoch: 561/5000, Train Loss: 34.46003237637606, Valid Loss: 34.712197621663414\n",
      "Epoch: 562/5000, Train Loss: 34.31438237970526, Valid Loss: 34.685743967692055\n",
      "Epoch: 563/5000, Train Loss: 34.308073564009234, Valid Loss: 34.707401275634766\n",
      "Epoch: 564/5000, Train Loss: 34.21692449396307, Valid Loss: 34.720288594563804\n",
      "Epoch: 565/5000, Train Loss: 34.20376049388539, Valid Loss: 34.61417897542318\n",
      "Epoch: 566/5000, Train Loss: 33.94905818592418, Valid Loss: 34.540994008382164\n",
      "Epoch: 567/5000, Train Loss: 34.30880962718617, Valid Loss: 34.56770324707031\n",
      "Epoch: 568/5000, Train Loss: 34.46792290427468, Valid Loss: 34.564449310302734\n",
      "Epoch: 569/5000, Train Loss: 33.78918873180043, Valid Loss: 34.52852121988932\n",
      "Epoch: 570/5000, Train Loss: 34.11870904402299, Valid Loss: 34.58398310343424\n",
      "Epoch: 571/5000, Train Loss: 34.02678090875799, Valid Loss: 34.53979746500651\n",
      "Epoch: 572/5000, Train Loss: 33.65745319019664, Valid Loss: 34.52070617675781\n",
      "Epoch: 573/5000, Train Loss: 34.13083197853782, Valid Loss: 34.56245676676432\n",
      "Epoch: 574/5000, Train Loss: 34.04397409612482, Valid Loss: 34.52295049031576\n",
      "Epoch: 575/5000, Train Loss: 34.14872880415483, Valid Loss: 34.505236307779946\n",
      "Epoch: 576/5000, Train Loss: 33.63419099287553, Valid Loss: 34.439690907796226\n",
      "Epoch: 577/5000, Train Loss: 33.958641745827414, Valid Loss: 34.395704905192055\n",
      "Epoch: 578/5000, Train Loss: 33.892637426202946, Valid Loss: 34.41984430948893\n",
      "Epoch: 579/5000, Train Loss: 33.997645984996446, Valid Loss: 34.40487798055013\n",
      "Epoch: 580/5000, Train Loss: 34.2055795842951, Valid Loss: 34.3995107014974\n",
      "Epoch: 581/5000, Train Loss: 34.04131369157271, Valid Loss: 34.31727981567383\n",
      "Epoch: 582/5000, Train Loss: 33.89512963728471, Valid Loss: 34.275726318359375\n",
      "Epoch: 583/5000, Train Loss: 34.198909065940164, Valid Loss: 34.33488337198893\n",
      "Epoch: 584/5000, Train Loss: 33.49870456348766, Valid Loss: 34.3429921468099\n",
      "Epoch: 585/5000, Train Loss: 33.75670762495561, Valid Loss: 34.34843317667643\n",
      "Epoch: 586/5000, Train Loss: 33.72450637817383, Valid Loss: 34.25460433959961\n",
      "Epoch: 587/5000, Train Loss: 34.0261511369185, Valid Loss: 34.24708684285482\n",
      "Epoch: 588/5000, Train Loss: 34.091649142178625, Valid Loss: 34.16832733154297\n",
      "Epoch: 589/5000, Train Loss: 33.86666072498668, Valid Loss: 34.12331517537435\n",
      "Epoch: 590/5000, Train Loss: 33.81876130537553, Valid Loss: 34.088540395100914\n",
      "Epoch: 591/5000, Train Loss: 33.764956907792524, Valid Loss: 34.10541661580404\n",
      "Epoch: 592/5000, Train Loss: 33.65450807051225, Valid Loss: 34.11384963989258\n",
      "Epoch: 593/5000, Train Loss: 33.76416501131925, Valid Loss: 34.08470916748047\n",
      "Epoch: 594/5000, Train Loss: 33.75106083263051, Valid Loss: 34.0940310160319\n",
      "Epoch: 595/5000, Train Loss: 33.64101791381836, Valid Loss: 33.98517354329427\n",
      "Epoch: 596/5000, Train Loss: 33.950091968883164, Valid Loss: 33.988059997558594\n",
      "Epoch: 597/5000, Train Loss: 33.766375975175336, Valid Loss: 33.999769846598305\n",
      "Epoch: 598/5000, Train Loss: 33.68904044411399, Valid Loss: 33.96595255533854\n",
      "Epoch: 599/5000, Train Loss: 33.55985294688832, Valid Loss: 33.92608960469564\n",
      "Epoch: 600/5000, Train Loss: 33.6453084078702, Valid Loss: 33.883537928263344\n",
      "Epoch: 601/5000, Train Loss: 33.345416502519086, Valid Loss: 33.91219584147135\n",
      "Epoch: 602/5000, Train Loss: 33.87991280989213, Valid Loss: 33.90895144144694\n",
      "Epoch: 603/5000, Train Loss: 33.52256549488414, Valid Loss: 33.81550534566244\n",
      "Epoch: 604/5000, Train Loss: 33.63705964521928, Valid Loss: 33.83294169108073\n",
      "Epoch: 605/5000, Train Loss: 33.64462037519975, Valid Loss: 33.8421433766683\n",
      "Epoch: 606/5000, Train Loss: 33.512859864668414, Valid Loss: 33.84946250915527\n",
      "Epoch: 607/5000, Train Loss: 33.51161766052246, Valid Loss: 33.8541628519694\n",
      "Epoch: 608/5000, Train Loss: 33.41916847229004, Valid Loss: 33.836469650268555\n",
      "Epoch: 609/5000, Train Loss: 33.19751826199618, Valid Loss: 33.828216552734375\n",
      "Epoch: 610/5000, Train Loss: 33.21750051325018, Valid Loss: 33.822802225748696\n",
      "Epoch: 611/5000, Train Loss: 33.46239107305353, Valid Loss: 33.7550843556722\n",
      "Epoch: 612/5000, Train Loss: 33.687015533447266, Valid Loss: 33.72124163309733\n",
      "Epoch: 613/5000, Train Loss: 33.464340383356266, Valid Loss: 33.67439524332682\n",
      "Epoch: 614/5000, Train Loss: 33.418805902654476, Valid Loss: 33.71961275736491\n",
      "Epoch: 615/5000, Train Loss: 33.21180239590731, Valid Loss: 33.73821767171224\n",
      "Epoch: 616/5000, Train Loss: 33.637142701582476, Valid Loss: 33.72554715474447\n",
      "Epoch: 617/5000, Train Loss: 33.193045529452235, Valid Loss: 33.656899770100914\n",
      "Epoch: 618/5000, Train Loss: 33.37039652737704, Valid Loss: 33.673675537109375\n",
      "Epoch: 619/5000, Train Loss: 33.691139221191406, Valid Loss: 33.67263221740723\n",
      "Epoch: 620/5000, Train Loss: 33.48482669483531, Valid Loss: 33.63238271077474\n",
      "Epoch: 621/5000, Train Loss: 32.93823224847967, Valid Loss: 33.60882568359375\n",
      "Epoch: 622/5000, Train Loss: 33.40112217989835, Valid Loss: 33.60483169555664\n",
      "Epoch: 623/5000, Train Loss: 33.42852228338068, Valid Loss: 33.56898307800293\n",
      "Epoch: 624/5000, Train Loss: 33.165245056152344, Valid Loss: 33.57908948262533\n",
      "Epoch: 625/5000, Train Loss: 33.707929437810726, Valid Loss: 33.547990798950195\n",
      "Epoch: 626/5000, Train Loss: 33.23897344415838, Valid Loss: 33.530338287353516\n",
      "Epoch: 627/5000, Train Loss: 33.30245746265758, Valid Loss: 33.571621576944985\n",
      "Epoch: 628/5000, Train Loss: 33.245995088057086, Valid Loss: 33.52281061808268\n",
      "Epoch: 629/5000, Train Loss: 33.187444340098985, Valid Loss: 33.517808278401695\n",
      "Epoch: 630/5000, Train Loss: 32.949701655994765, Valid Loss: 33.51589266459147\n",
      "Epoch: 631/5000, Train Loss: 33.22569760409269, Valid Loss: 33.46463203430176\n",
      "Epoch: 632/5000, Train Loss: 33.53429499539462, Valid Loss: 33.41567738850912\n",
      "Epoch: 633/5000, Train Loss: 33.03115047108043, Valid Loss: 33.348493576049805\n",
      "Epoch: 634/5000, Train Loss: 33.477033441716976, Valid Loss: 33.36755816141764\n",
      "Epoch: 635/5000, Train Loss: 33.5727143721147, Valid Loss: 33.327650705973305\n",
      "Epoch: 636/5000, Train Loss: 33.12479383295233, Valid Loss: 33.37474822998047\n",
      "Epoch: 637/5000, Train Loss: 33.11865841258656, Valid Loss: 33.40603446960449\n",
      "Epoch: 638/5000, Train Loss: 33.124846545132726, Valid Loss: 33.42050743103027\n",
      "Epoch: 639/5000, Train Loss: 33.05265652049672, Valid Loss: 33.41414833068848\n",
      "Epoch: 640/5000, Train Loss: 33.35382652282715, Valid Loss: 33.390350341796875\n",
      "Epoch: 641/5000, Train Loss: 33.37483215332031, Valid Loss: 33.332484563191734\n",
      "Epoch: 642/5000, Train Loss: 33.10525911504572, Valid Loss: 33.37549908955892\n",
      "Epoch: 643/5000, Train Loss: 33.11168150468306, Valid Loss: 33.36558278401693\n",
      "Epoch: 644/5000, Train Loss: 33.31737154180353, Valid Loss: 33.34649085998535\n",
      "Epoch: 645/5000, Train Loss: 33.30198183926669, Valid Loss: 33.322291692097984\n",
      "Epoch: 646/5000, Train Loss: 32.81152257052335, Valid Loss: 33.31137275695801\n",
      "Epoch: 647/5000, Train Loss: 33.39788766340776, Valid Loss: 33.316993713378906\n",
      "Epoch: 648/5000, Train Loss: 33.43786673112349, Valid Loss: 33.30909729003906\n",
      "Epoch: 649/5000, Train Loss: 33.302772521972656, Valid Loss: 33.31273523966471\n",
      "Epoch: 650/5000, Train Loss: 33.17130574313077, Valid Loss: 33.309455235799156\n",
      "Epoch: 651/5000, Train Loss: 33.07333842190829, Valid Loss: 33.3064079284668\n",
      "Epoch: 652/5000, Train Loss: 33.26086165688255, Valid Loss: 33.30063819885254\n",
      "Epoch: 653/5000, Train Loss: 33.30702157454057, Valid Loss: 33.30433972676595\n",
      "Epoch: 654/5000, Train Loss: 33.29431221701882, Valid Loss: 33.28239186604818\n",
      "Epoch: 655/5000, Train Loss: 33.010046698830344, Valid Loss: 33.2463747660319\n",
      "Epoch: 656/5000, Train Loss: 32.8957273309881, Valid Loss: 33.286081314086914\n",
      "Epoch: 657/5000, Train Loss: 32.83244375749068, Valid Loss: 33.284559885660805\n",
      "Epoch: 658/5000, Train Loss: 32.7907863963734, Valid Loss: 33.23250516255697\n",
      "Epoch: 659/5000, Train Loss: 33.20044673572887, Valid Loss: 33.24655787150065\n",
      "Epoch: 660/5000, Train Loss: 32.841999227350406, Valid Loss: 33.20143699645996\n",
      "Epoch: 661/5000, Train Loss: 33.125877900557086, Valid Loss: 33.209004720052086\n",
      "Epoch: 662/5000, Train Loss: 33.06804379549894, Valid Loss: 33.157837549845375\n",
      "Epoch: 663/5000, Train Loss: 33.07140003551137, Valid Loss: 33.1618766784668\n",
      "Epoch: 664/5000, Train Loss: 32.93237738175826, Valid Loss: 33.19134712219238\n",
      "Epoch: 665/5000, Train Loss: 32.88307970220392, Valid Loss: 33.21478144327799\n",
      "Epoch: 666/5000, Train Loss: 32.94182604009455, Valid Loss: 33.1639092763265\n",
      "Epoch: 667/5000, Train Loss: 32.95208861611106, Valid Loss: 33.17022705078125\n",
      "Epoch: 668/5000, Train Loss: 32.95692253112793, Valid Loss: 33.1295223236084\n",
      "Epoch: 669/5000, Train Loss: 33.01922746138139, Valid Loss: 33.13713264465332\n",
      "Epoch: 670/5000, Train Loss: 33.12092451615767, Valid Loss: 33.10352579752604\n",
      "Epoch: 671/5000, Train Loss: 32.676770817149766, Valid Loss: 33.16530799865723\n",
      "Epoch: 672/5000, Train Loss: 33.113997199318625, Valid Loss: 33.12065633138021\n",
      "Epoch: 673/5000, Train Loss: 32.986774097789414, Valid Loss: 33.12239519755045\n",
      "Epoch: 674/5000, Train Loss: 33.160606210882015, Valid Loss: 33.11849276224772\n",
      "Epoch: 675/5000, Train Loss: 32.506848942149766, Valid Loss: 33.105325063069664\n",
      "Epoch: 676/5000, Train Loss: 32.587636254050516, Valid Loss: 33.103089014689125\n",
      "Epoch: 677/5000, Train Loss: 32.91511119495738, Valid Loss: 33.03373336791992\n",
      "Epoch: 678/5000, Train Loss: 32.9338172565807, Valid Loss: 33.062198638916016\n",
      "Epoch: 679/5000, Train Loss: 33.07739171114835, Valid Loss: 33.07248497009277\n",
      "Epoch: 680/5000, Train Loss: 32.836950822310015, Valid Loss: 33.07960446675619\n",
      "Epoch: 681/5000, Train Loss: 32.90047160061923, Valid Loss: 33.10526911417643\n",
      "Epoch: 682/5000, Train Loss: 32.77575943686745, Valid Loss: 33.09355799357096\n",
      "Epoch: 683/5000, Train Loss: 33.03239284862172, Valid Loss: 33.09299405415853\n",
      "Epoch: 684/5000, Train Loss: 33.04290147261186, Valid Loss: 33.070400873819985\n",
      "Epoch: 685/5000, Train Loss: 32.97436315363104, Valid Loss: 33.02307955423991\n",
      "Epoch: 686/5000, Train Loss: 32.8895017450506, Valid Loss: 33.002374013264976\n",
      "Epoch: 687/5000, Train Loss: 32.67223618247292, Valid Loss: 33.03083292643229\n",
      "Epoch: 688/5000, Train Loss: 32.65122482993386, Valid Loss: 32.996316274007164\n",
      "Epoch: 689/5000, Train Loss: 33.02390636097301, Valid Loss: 32.97723070780436\n",
      "Epoch: 690/5000, Train Loss: 32.952362927523524, Valid Loss: 32.963195164998375\n",
      "Epoch: 691/5000, Train Loss: 32.804316780783914, Valid Loss: 32.963064193725586\n",
      "Epoch: 692/5000, Train Loss: 33.13044114546342, Valid Loss: 32.945935567220054\n",
      "Epoch: 693/5000, Train Loss: 33.20063434947621, Valid Loss: 32.957920710245766\n",
      "Epoch: 694/5000, Train Loss: 32.54774145646529, Valid Loss: 32.94963391621908\n",
      "Epoch: 695/5000, Train Loss: 32.665552312677555, Valid Loss: 32.933828353881836\n",
      "Epoch: 696/5000, Train Loss: 32.92514298178933, Valid Loss: 32.910834630330406\n",
      "Epoch: 697/5000, Train Loss: 32.57430666143244, Valid Loss: 32.920838038126625\n",
      "Epoch: 698/5000, Train Loss: 32.662172837690875, Valid Loss: 32.938032150268555\n",
      "Epoch: 699/5000, Train Loss: 32.59371722828258, Valid Loss: 32.95515251159668\n",
      "Epoch: 700/5000, Train Loss: 32.70112124356356, Valid Loss: 32.91135470072428\n",
      "Epoch: 701/5000, Train Loss: 32.988779588179156, Valid Loss: 32.905012130737305\n",
      "Epoch: 702/5000, Train Loss: 32.44201035933061, Valid Loss: 32.88056500752767\n",
      "Epoch: 703/5000, Train Loss: 32.29722092368386, Valid Loss: 32.86955134073893\n",
      "Epoch: 704/5000, Train Loss: 32.61461968855424, Valid Loss: 32.902743657430015\n",
      "Epoch: 705/5000, Train Loss: 32.87698763067072, Valid Loss: 32.90625890096029\n",
      "Epoch: 706/5000, Train Loss: 32.85249970176003, Valid Loss: 32.92240905761719\n",
      "Epoch: 707/5000, Train Loss: 32.60514189980247, Valid Loss: 32.9213752746582\n",
      "Epoch: 708/5000, Train Loss: 32.629399039528586, Valid Loss: 32.90120951334635\n",
      "Epoch: 709/5000, Train Loss: 32.88226509094238, Valid Loss: 32.902374903361\n",
      "Epoch: 710/5000, Train Loss: 32.71462891318581, Valid Loss: 32.86602656046549\n",
      "Epoch: 711/5000, Train Loss: 32.61427133733576, Valid Loss: 32.869912465413414\n",
      "Epoch: 712/5000, Train Loss: 32.64079700816762, Valid Loss: 32.864240646362305\n",
      "Epoch: 713/5000, Train Loss: 32.807937448674984, Valid Loss: 32.86575508117676\n",
      "Epoch: 714/5000, Train Loss: 33.15217104825106, Valid Loss: 32.85318120320638\n",
      "Epoch: 715/5000, Train Loss: 32.6036526073109, Valid Loss: 32.79280217488607\n",
      "Epoch: 716/5000, Train Loss: 32.6672059839422, Valid Loss: 32.81701405843099\n",
      "Epoch: 717/5000, Train Loss: 32.58080846613104, Valid Loss: 32.81733322143555\n",
      "Epoch: 718/5000, Train Loss: 32.875934774225406, Valid Loss: 32.83606465657552\n",
      "Epoch: 719/5000, Train Loss: 32.56291060014205, Valid Loss: 32.832536697387695\n",
      "Epoch: 720/5000, Train Loss: 32.55303348194469, Valid Loss: 32.8265495300293\n",
      "Epoch: 721/5000, Train Loss: 32.834023388949305, Valid Loss: 32.817535400390625\n",
      "Epoch: 722/5000, Train Loss: 32.53668490323153, Valid Loss: 32.84795888264974\n",
      "Epoch: 723/5000, Train Loss: 32.50660584189675, Valid Loss: 32.82319005330404\n",
      "Epoch: 724/5000, Train Loss: 32.42909691550515, Valid Loss: 32.81721941630045\n",
      "Epoch: 725/5000, Train Loss: 32.56745459816673, Valid Loss: 32.80258750915527\n",
      "Epoch: 726/5000, Train Loss: 32.50583076477051, Valid Loss: 32.8234806060791\n",
      "Epoch: 727/5000, Train Loss: 32.41511050137606, Valid Loss: 32.7892214457194\n",
      "Epoch: 728/5000, Train Loss: 32.57231192155318, Valid Loss: 32.80721346537272\n",
      "Epoch: 729/5000, Train Loss: 32.44719765403054, Valid Loss: 32.78124237060547\n",
      "Epoch: 730/5000, Train Loss: 32.49505632573908, Valid Loss: 32.82634417215983\n",
      "Epoch: 731/5000, Train Loss: 32.6201194416393, Valid Loss: 32.82144292195638\n",
      "Epoch: 732/5000, Train Loss: 32.63913969560103, Valid Loss: 32.81013043721517\n",
      "Epoch: 733/5000, Train Loss: 32.817184621637516, Valid Loss: 32.78321520487467\n",
      "Epoch: 734/5000, Train Loss: 32.58185421336781, Valid Loss: 32.78551038106283\n",
      "Epoch: 735/5000, Train Loss: 32.58446624062278, Valid Loss: 32.758619944254555\n",
      "Epoch: 736/5000, Train Loss: 32.645939566872336, Valid Loss: 32.791531244913735\n",
      "Epoch: 737/5000, Train Loss: 32.44698108326305, Valid Loss: 32.80586814880371\n",
      "Epoch: 738/5000, Train Loss: 32.39066141301935, Valid Loss: 32.771759033203125\n",
      "Epoch: 739/5000, Train Loss: 32.2814774946733, Valid Loss: 32.760294596354164\n",
      "Epoch: 740/5000, Train Loss: 32.3453816500577, Valid Loss: 32.7815310160319\n",
      "Epoch: 741/5000, Train Loss: 32.603031158447266, Valid Loss: 32.76249313354492\n",
      "Epoch: 742/5000, Train Loss: 32.3181077783758, Valid Loss: 32.70777384440104\n",
      "Epoch: 743/5000, Train Loss: 32.67335111444647, Valid Loss: 32.73860232035319\n",
      "Epoch: 744/5000, Train Loss: 32.34042791886763, Valid Loss: 32.76599884033203\n",
      "Epoch: 745/5000, Train Loss: 32.9172059839422, Valid Loss: 32.7644100189209\n",
      "Epoch: 746/5000, Train Loss: 32.48035032098944, Valid Loss: 32.7903798421224\n",
      "Epoch: 747/5000, Train Loss: 32.86779091574929, Valid Loss: 32.71589724222819\n",
      "Epoch: 748/5000, Train Loss: 32.46489923650568, Valid Loss: 32.71992746988932\n",
      "Epoch: 749/5000, Train Loss: 32.715554150668055, Valid Loss: 32.735500971476235\n",
      "Epoch: 750/5000, Train Loss: 32.51323717290705, Valid Loss: 32.75291124979655\n",
      "Epoch: 751/5000, Train Loss: 32.695974349975586, Valid Loss: 32.735769271850586\n",
      "Epoch: 752/5000, Train Loss: 32.594051881269976, Valid Loss: 32.74809964497884\n",
      "Epoch: 753/5000, Train Loss: 32.58680274269798, Valid Loss: 32.76547050476074\n",
      "Epoch: 754/5000, Train Loss: 32.49280340021307, Valid Loss: 32.761304219563804\n",
      "Epoch: 755/5000, Train Loss: 32.358247236772016, Valid Loss: 32.774007161458336\n",
      "Epoch: 756/5000, Train Loss: 32.84355874495073, Valid Loss: 32.726706186930336\n",
      "Epoch: 757/5000, Train Loss: 32.527393861250445, Valid Loss: 32.713221867879234\n",
      "Epoch: 758/5000, Train Loss: 32.51784844831987, Valid Loss: 32.66641426086426\n",
      "Epoch: 759/5000, Train Loss: 32.47971638766202, Valid Loss: 32.65705490112305\n",
      "Epoch: 760/5000, Train Loss: 32.349083640358664, Valid Loss: 32.68944549560547\n",
      "Epoch: 761/5000, Train Loss: 32.429805582219906, Valid Loss: 32.713404973347984\n",
      "Epoch: 762/5000, Train Loss: 32.52623454007235, Valid Loss: 32.75863774617513\n",
      "Epoch: 763/5000, Train Loss: 32.485603679310195, Valid Loss: 32.733338038126625\n",
      "Epoch: 764/5000, Train Loss: 32.5962985645641, Valid Loss: 32.74197514851888\n",
      "Epoch: 765/5000, Train Loss: 32.53151789578524, Valid Loss: 32.69361368815104\n",
      "Epoch: 766/5000, Train Loss: 32.371220501986414, Valid Loss: 32.745749155680336\n",
      "Epoch: 767/5000, Train Loss: 32.45487681302157, Valid Loss: 32.70625368754069\n",
      "Epoch: 768/5000, Train Loss: 32.84747747941451, Valid Loss: 32.74994468688965\n",
      "Epoch: 769/5000, Train Loss: 32.28473403237083, Valid Loss: 32.73634147644043\n",
      "Epoch: 770/5000, Train Loss: 32.279869079589844, Valid Loss: 32.751410802205406\n",
      "Epoch: 771/5000, Train Loss: 32.64076735756614, Valid Loss: 32.746359507242836\n",
      "Epoch: 772/5000, Train Loss: 32.66219745982777, Valid Loss: 32.73519388834635\n",
      "Epoch: 773/5000, Train Loss: 32.41200100291859, Valid Loss: 32.72954877217611\n",
      "Epoch: 774/5000, Train Loss: 32.68673393943093, Valid Loss: 32.79629707336426\n",
      "Epoch: 775/5000, Train Loss: 32.83952504938299, Valid Loss: 32.78096389770508\n",
      "Epoch: 776/5000, Train Loss: 32.255229776555844, Valid Loss: 32.70336469014486\n",
      "Epoch: 777/5000, Train Loss: 32.562564156272195, Valid Loss: 32.687683741251625\n",
      "Epoch: 778/5000, Train Loss: 32.61482880332253, Valid Loss: 32.69034194946289\n",
      "Epoch: 779/5000, Train Loss: 32.576726393266156, Valid Loss: 32.68957837422689\n",
      "Epoch: 780/5000, Train Loss: 32.33608627319336, Valid Loss: 32.70370229085287\n",
      "Epoch: 781/5000, Train Loss: 32.313579906116836, Valid Loss: 32.71479606628418\n",
      "Epoch: 782/5000, Train Loss: 32.48428379405629, Valid Loss: 32.69111315409342\n",
      "Epoch: 783/5000, Train Loss: 32.567063591697, Valid Loss: 32.71883455912272\n",
      "Epoch: 784/5000, Train Loss: 32.48694818670099, Valid Loss: 32.73010698954264\n",
      "Epoch: 785/5000, Train Loss: 32.34380409934304, Valid Loss: 32.70026969909668\n",
      "Epoch: 786/5000, Train Loss: 32.256857958706945, Valid Loss: 32.685010274251304\n",
      "Epoch: 787/5000, Train Loss: 32.130297053943984, Valid Loss: 32.7196839650472\n",
      "Epoch: 788/5000, Train Loss: 32.30570741133256, Valid Loss: 32.69995435078939\n",
      "Epoch: 789/5000, Train Loss: 32.40460968017578, Valid Loss: 32.67960866292318\n",
      "Epoch: 790/5000, Train Loss: 32.091896924105555, Valid Loss: 32.656471252441406\n",
      "Epoch: 791/5000, Train Loss: 32.28009796142578, Valid Loss: 32.678314208984375\n",
      "Epoch: 792/5000, Train Loss: 32.73709297180176, Valid Loss: 32.67604446411133\n",
      "Epoch: 793/5000, Train Loss: 32.48532780733976, Valid Loss: 32.67362531026205\n",
      "Epoch: 794/5000, Train Loss: 32.415984240445226, Valid Loss: 32.6783135732015\n",
      "Epoch: 795/5000, Train Loss: 32.31387901306152, Valid Loss: 32.663798014322914\n",
      "Epoch: 796/5000, Train Loss: 32.531877691095524, Valid Loss: 32.68501663208008\n",
      "Epoch: 797/5000, Train Loss: 32.266460765491836, Valid Loss: 32.61356099446615\n",
      "Epoch: 798/5000, Train Loss: 32.324572649869054, Valid Loss: 32.66347630818685\n",
      "Epoch: 799/5000, Train Loss: 32.281494487415664, Valid Loss: 32.61949221293131\n",
      "Epoch: 800/5000, Train Loss: 32.221541838212445, Valid Loss: 32.66120592753092\n",
      "Epoch: 801/5000, Train Loss: 32.28773099725897, Valid Loss: 32.66618982950846\n",
      "Epoch: 802/5000, Train Loss: 32.36333049427379, Valid Loss: 32.67336463928223\n",
      "Epoch: 803/5000, Train Loss: 32.436796708540484, Valid Loss: 32.66697565714518\n",
      "Epoch: 804/5000, Train Loss: 32.50717215104537, Valid Loss: 32.66101964314779\n",
      "Epoch: 805/5000, Train Loss: 32.469756213101476, Valid Loss: 32.652269999186196\n",
      "Epoch: 806/5000, Train Loss: 32.31157250837846, Valid Loss: 32.603004455566406\n",
      "Epoch: 807/5000, Train Loss: 32.39913264187899, Valid Loss: 32.60018793741862\n",
      "Epoch: 808/5000, Train Loss: 32.540841189297765, Valid Loss: 32.63524182637533\n",
      "Epoch: 809/5000, Train Loss: 32.11280649358576, Valid Loss: 32.65160115559896\n",
      "Epoch: 810/5000, Train Loss: 32.36567705327814, Valid Loss: 32.654873530069985\n",
      "Epoch: 811/5000, Train Loss: 32.48377279801802, Valid Loss: 32.60042190551758\n",
      "Epoch: 812/5000, Train Loss: 32.40018983320756, Valid Loss: 32.63991610209147\n",
      "Epoch: 813/5000, Train Loss: 32.01510516079989, Valid Loss: 32.62799644470215\n",
      "Epoch: 814/5000, Train Loss: 32.14017937400124, Valid Loss: 32.59791374206543\n",
      "Epoch: 815/5000, Train Loss: 32.381744384765625, Valid Loss: 32.600714365641274\n",
      "Epoch: 816/5000, Train Loss: 32.5235481262207, Valid Loss: 32.631808598836265\n",
      "Epoch: 817/5000, Train Loss: 32.46948658336293, Valid Loss: 32.64144961039225\n",
      "Epoch: 818/5000, Train Loss: 32.32274835759943, Valid Loss: 32.61861356099447\n",
      "Epoch: 819/5000, Train Loss: 32.43486283042214, Valid Loss: 32.595227559407554\n",
      "Epoch: 820/5000, Train Loss: 32.311750238591976, Valid Loss: 32.614784240722656\n",
      "Epoch: 821/5000, Train Loss: 32.48487437855113, Valid Loss: 32.610922495524086\n",
      "Epoch: 822/5000, Train Loss: 32.403477755459875, Valid Loss: 32.61007817586263\n",
      "Epoch: 823/5000, Train Loss: 32.348242499611594, Valid Loss: 32.64370028177897\n",
      "Epoch: 824/5000, Train Loss: 32.131920381025836, Valid Loss: 32.6036631266276\n",
      "Epoch: 825/5000, Train Loss: 32.34166838906028, Valid Loss: 32.56581687927246\n",
      "Epoch: 826/5000, Train Loss: 32.51875860040838, Valid Loss: 32.6225840250651\n",
      "Epoch: 827/5000, Train Loss: 32.62789379466664, Valid Loss: 32.65073839823405\n",
      "Epoch: 828/5000, Train Loss: 32.329113179987125, Valid Loss: 32.61286290486654\n",
      "Epoch: 829/5000, Train Loss: 32.291408192027696, Valid Loss: 32.587636947631836\n",
      "Epoch: 830/5000, Train Loss: 32.127985520796344, Valid Loss: 32.60974566141764\n",
      "Epoch: 831/5000, Train Loss: 32.52012946388938, Valid Loss: 32.601243336995445\n",
      "Epoch: 832/5000, Train Loss: 32.25494176691229, Valid Loss: 32.58570226033529\n",
      "Epoch: 833/5000, Train Loss: 32.20874803716486, Valid Loss: 32.61160087585449\n",
      "Epoch: 834/5000, Train Loss: 32.41791690479625, Valid Loss: 32.63294474283854\n",
      "Epoch: 835/5000, Train Loss: 32.1028301932595, Valid Loss: 32.60472615559896\n",
      "Epoch: 836/5000, Train Loss: 32.4928861097856, Valid Loss: 32.61052958170573\n",
      "Epoch: 837/5000, Train Loss: 32.114647605202414, Valid Loss: 32.609994888305664\n",
      "Epoch: 838/5000, Train Loss: 32.06531489979137, Valid Loss: 32.581869761149086\n",
      "Epoch: 839/5000, Train Loss: 32.50035910172896, Valid Loss: 32.60285758972168\n",
      "Epoch: 840/5000, Train Loss: 32.80180636319247, Valid Loss: 32.55819892883301\n",
      "Epoch: 841/5000, Train Loss: 32.0876487385143, Valid Loss: 32.55102348327637\n",
      "Epoch: 842/5000, Train Loss: 32.17415341463956, Valid Loss: 32.57392946879069\n",
      "Epoch: 843/5000, Train Loss: 32.22305488586426, Valid Loss: 32.606377919514976\n",
      "Epoch: 844/5000, Train Loss: 32.231262553821914, Valid Loss: 32.58987045288086\n",
      "Epoch: 845/5000, Train Loss: 32.47387539256703, Valid Loss: 32.60982322692871\n",
      "Epoch: 846/5000, Train Loss: 32.52124665000222, Valid Loss: 32.56983184814453\n",
      "Epoch: 847/5000, Train Loss: 32.30880061062899, Valid Loss: 32.56718508402506\n",
      "Epoch: 848/5000, Train Loss: 32.12425509366122, Valid Loss: 32.558572133382164\n",
      "Epoch: 849/5000, Train Loss: 32.26159251819957, Valid Loss: 32.552207946777344\n",
      "Epoch: 850/5000, Train Loss: 32.16080873662775, Valid Loss: 32.52715237935384\n",
      "Epoch: 851/5000, Train Loss: 32.07893094149503, Valid Loss: 32.5316359202067\n",
      "Epoch: 852/5000, Train Loss: 32.20140075683594, Valid Loss: 32.53779983520508\n",
      "Epoch: 853/5000, Train Loss: 32.53173151883212, Valid Loss: 32.54881731669108\n",
      "Epoch: 854/5000, Train Loss: 32.222198486328125, Valid Loss: 32.541144053141274\n",
      "Epoch: 855/5000, Train Loss: 32.446351138028234, Valid Loss: 32.549412409464516\n",
      "Epoch: 856/5000, Train Loss: 32.291721690784804, Valid Loss: 32.56078783671061\n",
      "Epoch: 857/5000, Train Loss: 32.31992860273881, Valid Loss: 32.51604080200195\n",
      "Epoch: 858/5000, Train Loss: 32.4161302393133, Valid Loss: 32.510449727376304\n",
      "Epoch: 859/5000, Train Loss: 32.22272075306285, Valid Loss: 32.536065419514976\n",
      "Epoch: 860/5000, Train Loss: 32.31680245832963, Valid Loss: 32.513855616251625\n",
      "Epoch: 861/5000, Train Loss: 31.895118193192914, Valid Loss: 32.50190734863281\n",
      "Epoch: 862/5000, Train Loss: 32.27596577731046, Valid Loss: 32.51036707560221\n",
      "Epoch: 863/5000, Train Loss: 32.46805693886497, Valid Loss: 32.53853988647461\n",
      "Epoch: 864/5000, Train Loss: 32.443829623135656, Valid Loss: 32.51498667399088\n",
      "Epoch: 865/5000, Train Loss: 32.18595366044478, Valid Loss: 32.52160898844401\n",
      "Epoch: 866/5000, Train Loss: 32.23995728926225, Valid Loss: 32.532952626546226\n",
      "Epoch: 867/5000, Train Loss: 32.45752594687722, Valid Loss: 32.48101488749186\n",
      "Epoch: 868/5000, Train Loss: 32.29991756786, Valid Loss: 32.51433817545573\n",
      "Epoch: 869/5000, Train Loss: 32.392309709028765, Valid Loss: 32.58017094930013\n",
      "Epoch: 870/5000, Train Loss: 31.822357524525035, Valid Loss: 32.559347788492836\n",
      "Epoch: 871/5000, Train Loss: 32.21045754172585, Valid Loss: 32.49149258931478\n",
      "Epoch: 872/5000, Train Loss: 32.491435831243344, Valid Loss: 32.47461064656576\n",
      "Epoch: 873/5000, Train Loss: 32.08525796370073, Valid Loss: 32.48906707763672\n",
      "Epoch: 874/5000, Train Loss: 32.33812349492853, Valid Loss: 32.49795405069987\n",
      "Epoch: 875/5000, Train Loss: 32.62875487587669, Valid Loss: 32.50464630126953\n",
      "Epoch: 876/5000, Train Loss: 32.10174768621271, Valid Loss: 32.52507209777832\n",
      "Epoch: 877/5000, Train Loss: 32.65806822343306, Valid Loss: 32.50782712300619\n",
      "Epoch: 878/5000, Train Loss: 32.25301551818848, Valid Loss: 32.5401922861735\n",
      "Epoch: 879/5000, Train Loss: 32.1781137639826, Valid Loss: 32.49789492289225\n",
      "Epoch: 880/5000, Train Loss: 32.36274857954545, Valid Loss: 32.4986572265625\n",
      "Epoch: 881/5000, Train Loss: 32.09917501969771, Valid Loss: 32.49518839518229\n",
      "Epoch: 882/5000, Train Loss: 32.122417970137164, Valid Loss: 32.547247568766274\n",
      "Epoch: 883/5000, Train Loss: 31.953763961791992, Valid Loss: 32.552788416544594\n",
      "Epoch: 884/5000, Train Loss: 32.29836498607289, Valid Loss: 32.52472496032715\n",
      "Epoch: 885/5000, Train Loss: 31.998298818414863, Valid Loss: 32.55283800760905\n",
      "Epoch: 886/5000, Train Loss: 32.33225094188344, Valid Loss: 32.49872652689616\n",
      "Epoch: 887/5000, Train Loss: 32.41793424432928, Valid Loss: 32.52234013875326\n",
      "Epoch: 888/5000, Train Loss: 31.96125065196644, Valid Loss: 32.503735860188804\n",
      "Epoch: 889/5000, Train Loss: 31.958640358664773, Valid Loss: 32.51714960734049\n",
      "Epoch: 890/5000, Train Loss: 32.1950718272816, Valid Loss: 32.52390098571777\n",
      "Epoch: 891/5000, Train Loss: 32.11326668479226, Valid Loss: 32.54609298706055\n",
      "Epoch: 892/5000, Train Loss: 32.193159103393555, Valid Loss: 32.519212086995445\n",
      "Epoch: 893/5000, Train Loss: 31.887572028420188, Valid Loss: 32.5179500579834\n",
      "Epoch: 894/5000, Train Loss: 32.329495863481, Valid Loss: 32.53057289123535\n",
      "Epoch: 895/5000, Train Loss: 32.1787183934992, Valid Loss: 32.55827967325846\n",
      "Epoch: 896/5000, Train Loss: 32.229197068647906, Valid Loss: 32.57816251118978\n",
      "Epoch: 897/5000, Train Loss: 31.87908259305087, Valid Loss: 32.579095204671226\n",
      "Epoch: 898/5000, Train Loss: 32.536373138427734, Valid Loss: 32.53681500752767\n",
      "Epoch: 899/5000, Train Loss: 32.0060960596258, Valid Loss: 32.56336975097656\n",
      "Epoch: 900/5000, Train Loss: 32.476606369018555, Valid Loss: 32.53700955708822\n",
      "Epoch: 901/5000, Train Loss: 32.25416807694869, Valid Loss: 32.56049410502116\n",
      "Epoch: 902/5000, Train Loss: 31.98977297002619, Valid Loss: 32.543097178141274\n",
      "Epoch: 903/5000, Train Loss: 32.12359497763894, Valid Loss: 32.54386075337728\n",
      "Epoch: 904/5000, Train Loss: 32.17160346291282, Valid Loss: 32.55295944213867\n",
      "Epoch: 905/5000, Train Loss: 32.21554322676225, Valid Loss: 32.58841196695963\n",
      "Epoch: 906/5000, Train Loss: 32.232085488059305, Valid Loss: 32.585156758626304\n",
      "Epoch: 907/5000, Train Loss: 32.03735334222967, Valid Loss: 32.55794715881348\n",
      "Epoch: 908/5000, Train Loss: 32.40541076660156, Valid Loss: 32.57823244730631\n",
      "Epoch: 909/5000, Train Loss: 32.085556203668766, Valid Loss: 32.57196044921875\n",
      "Epoch: 910/5000, Train Loss: 32.32764712246981, Valid Loss: 32.54180590311686\n",
      "Epoch: 911/5000, Train Loss: 32.205746390602805, Valid Loss: 32.543511072794594\n",
      "Epoch: 912/5000, Train Loss: 32.10390836542303, Valid Loss: 32.59021759033203\n",
      "Epoch: 913/5000, Train Loss: 32.27880096435547, Valid Loss: 32.59127934773763\n",
      "Epoch: 914/5000, Train Loss: 31.92791002446955, Valid Loss: 32.557699839274086\n",
      "Epoch: 915/5000, Train Loss: 31.918929360129617, Valid Loss: 32.54229863484701\n",
      "Epoch: 916/5000, Train Loss: 32.29420211098411, Valid Loss: 32.534477869669594\n",
      "Epoch: 917/5000, Train Loss: 32.22203046625311, Valid Loss: 32.55673789978027\n",
      "Epoch: 918/5000, Train Loss: 32.25420240922408, Valid Loss: 32.57373237609863\n",
      "Epoch: 919/5000, Train Loss: 32.570612994107336, Valid Loss: 32.52023124694824\n",
      "Epoch: 920/5000, Train Loss: 31.94953016801314, Valid Loss: 32.502166748046875\n",
      "Epoch: 921/5000, Train Loss: 32.387502843683414, Valid Loss: 32.54965591430664\n",
      "Epoch: 922/5000, Train Loss: 32.1272310777144, Valid Loss: 32.57462819417318\n",
      "Epoch: 923/5000, Train Loss: 32.05626054243608, Valid Loss: 32.5423641204834\n",
      "Epoch: 924/5000, Train Loss: 32.630746668035336, Valid Loss: 32.52715810139974\n",
      "Epoch: 925/5000, Train Loss: 31.820633281360973, Valid Loss: 32.54459571838379\n",
      "Epoch: 926/5000, Train Loss: 32.042339671741836, Valid Loss: 32.54789670308431\n",
      "Epoch: 927/5000, Train Loss: 31.930687297474254, Valid Loss: 32.51316515604655\n",
      "Epoch: 928/5000, Train Loss: 32.31987849148837, Valid Loss: 32.53525988260905\n",
      "Epoch: 929/5000, Train Loss: 32.23345357721502, Valid Loss: 32.50909423828125\n",
      "Epoch: 930/5000, Train Loss: 32.131682482632726, Valid Loss: 32.53558858235677\n",
      "Epoch: 931/5000, Train Loss: 32.60030885176225, Valid Loss: 32.54094950358073\n",
      "Epoch: 932/5000, Train Loss: 32.25077854503285, Valid Loss: 32.55139605204264\n",
      "Epoch: 933/5000, Train Loss: 32.60564769398082, Valid Loss: 32.582632064819336\n",
      "Epoch: 934/5000, Train Loss: 32.09484152360396, Valid Loss: 32.55726941426595\n",
      "Epoch: 935/5000, Train Loss: 32.208433324640446, Valid Loss: 32.51829401652018\n",
      "Epoch: 936/5000, Train Loss: 32.16296438737349, Valid Loss: 32.52303250630697\n",
      "Epoch: 937/5000, Train Loss: 32.27861265702681, Valid Loss: 32.4983164469401\n",
      "Epoch: 938/5000, Train Loss: 32.08802171186967, Valid Loss: 32.485154469807945\n",
      "Epoch: 939/5000, Train Loss: 32.334978970614344, Valid Loss: 32.50086466471354\n",
      "Epoch: 940/5000, Train Loss: 32.536596991799094, Valid Loss: 32.51223691304525\n",
      "Epoch: 941/5000, Train Loss: 32.003667484630235, Valid Loss: 32.49160194396973\n",
      "Epoch: 942/5000, Train Loss: 32.60555492747914, Valid Loss: 32.53341547648112\n",
      "Epoch: 943/5000, Train Loss: 31.803560777144, Valid Loss: 32.52336057027181\n",
      "Epoch: 944/5000, Train Loss: 32.1411659934304, Valid Loss: 32.505348205566406\n",
      "Epoch: 945/5000, Train Loss: 31.850108059969816, Valid Loss: 32.5065123240153\n",
      "Epoch: 946/5000, Train Loss: 32.175836909901015, Valid Loss: 32.502041498819985\n",
      "Epoch: 947/5000, Train Loss: 31.846342260187324, Valid Loss: 32.47863006591797\n",
      "Epoch: 948/5000, Train Loss: 31.936145955866035, Valid Loss: 32.49584706624349\n",
      "Epoch: 949/5000, Train Loss: 32.09933575716886, Valid Loss: 32.5006415049235\n",
      "Epoch: 950/5000, Train Loss: 32.150195208462804, Valid Loss: 32.48779296875\n",
      "Epoch: 951/5000, Train Loss: 32.089675729924984, Valid Loss: 32.48401705423991\n",
      "Epoch: 952/5000, Train Loss: 32.542950196699664, Valid Loss: 32.54022089640299\n",
      "Epoch: 953/5000, Train Loss: 32.29778931357644, Valid Loss: 32.56531588236491\n",
      "Epoch: 954/5000, Train Loss: 32.676429921930485, Valid Loss: 32.50618044535319\n",
      "Epoch: 955/5000, Train Loss: 32.11757590553977, Valid Loss: 32.529322942097984\n",
      "Epoch: 956/5000, Train Loss: 32.02751506458629, Valid Loss: 32.502509435017906\n",
      "Epoch: 957/5000, Train Loss: 32.2014194835316, Valid Loss: 32.48956171671549\n",
      "Epoch: 958/5000, Train Loss: 31.89462488347834, Valid Loss: 32.48520151774088\n",
      "Epoch: 959/5000, Train Loss: 31.962488868019797, Valid Loss: 32.500597635904946\n",
      "Epoch: 960/5000, Train Loss: 32.37794668024237, Valid Loss: 32.53059196472168\n",
      "Epoch: 961/5000, Train Loss: 32.18075734918768, Valid Loss: 32.515551249186196\n",
      "Epoch: 962/5000, Train Loss: 32.265601591630414, Valid Loss: 32.51337560017904\n",
      "Epoch: 963/5000, Train Loss: 31.85904312133789, Valid Loss: 32.52462387084961\n",
      "Epoch: 964/5000, Train Loss: 32.016374761408024, Valid Loss: 32.51497586568197\n",
      "Epoch: 965/5000, Train Loss: 32.0695405439897, Valid Loss: 32.51745414733887\n",
      "Epoch: 966/5000, Train Loss: 32.06805836070668, Valid Loss: 32.51612154642741\n",
      "Epoch: 967/5000, Train Loss: 32.121319857510656, Valid Loss: 32.549844106038414\n",
      "Epoch: 968/5000, Train Loss: 32.396608872847125, Valid Loss: 32.51249885559082\n",
      "Epoch: 969/5000, Train Loss: 32.15363745255904, Valid Loss: 32.51400947570801\n",
      "Epoch: 970/5000, Train Loss: 32.161523992365055, Valid Loss: 32.514506022135414\n",
      "Epoch: 971/5000, Train Loss: 32.10218811035156, Valid Loss: 32.506476720174156\n",
      "Epoch: 972/5000, Train Loss: 32.20157224481756, Valid Loss: 32.477349599202476\n",
      "얼리 스토핑: 100 에포크 동안 검증 손실이 향상되지 않음. 에포크 972에서 훈련 중단.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Start: MLM\")\n",
    "model_MLM = train(train_MLM_loader, valid_MLM_loader, model_MLM, criterion, optimizer_MLM, epochs=CFG['EPOCHS'])\n",
    "\n",
    "print(\"Training Start: HLM\")\n",
    "model_HLM = train(train_HLM_loader, valid_HLM_loader, model_HLM, criterion, optimizer_HLM, epochs=CFG['EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0fa8976d",
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1692630229503,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "0fa8976d"
   },
   "outputs": [],
   "source": [
    "torch.save(model_MLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_MLM_1.pth')  # 모델 객체의 state_dict 저장\n",
    "torch.save(model_HLM.state_dict(), '/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_HLM_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e894642",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 470,
     "status": "ok",
     "timestamp": 1692258355712,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "1e894642",
    "outputId": "68a5a747-a149-40a9-8670-87934c7fb0bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_MLM_1.pth'))\n",
    "model_HLM.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/models/OnlyNewFeature_DNN_Model_HLM_1.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "AiGfdAGsemIp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1692630235503,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "AiGfdAGsemIp",
    "outputId": "41ea7fe2-2c33-4860-aac1-5477d87c5631"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c02f8cdc-5d85-4a15-9aad-e3d95603b7d0\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>Molecular_Weight</th>\n",
       "      <th>Num_H_Acceptors</th>\n",
       "      <th>Num_H_Donors</th>\n",
       "      <th>Num_RotatableBonds</th>\n",
       "      <th>LogD</th>\n",
       "      <th>Molecular_PolarSurfaceArea</th>\n",
       "      <th>logP</th>\n",
       "      <th>apka</th>\n",
       "      <th>num_rotatable_bonds</th>\n",
       "      <th>num_heteroatoms</th>\n",
       "      <th>num_hydrogen_acceptors</th>\n",
       "      <th>num_hydrogen_donors</th>\n",
       "      <th>morgan_fingerprint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>CC(C)Nc1ccnc(N2CCN(Cc3cccs3)C(CCO)C2)n1</td>\n",
       "      <td>2.641</td>\n",
       "      <td>361.505</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2.635</td>\n",
       "      <td>92.76</td>\n",
       "      <td>2.43160</td>\n",
       "      <td>361.515</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>COc1cc(=O)n(-c2ccccc2)cc1C(=O)N1CCC2(CC1)OCCO2</td>\n",
       "      <td>0.585</td>\n",
       "      <td>370.399</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.585</td>\n",
       "      <td>68.31</td>\n",
       "      <td>1.82520</td>\n",
       "      <td>370.405</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>Cc1cccc(NC(=N)/N=c2\\nc(O)c(Cc3ccccc3)c(C)[nH]2)c1</td>\n",
       "      <td>4.276</td>\n",
       "      <td>347.414</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.290</td>\n",
       "      <td>92.86</td>\n",
       "      <td>3.27051</td>\n",
       "      <td>347.422</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>O=C(c1nc2ncccn2n1)N1CCCn2cc(-c3ccccc3)nc21</td>\n",
       "      <td>1.795</td>\n",
       "      <td>345.358</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.795</td>\n",
       "      <td>81.21</td>\n",
       "      <td>2.03830</td>\n",
       "      <td>345.366</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>CCN1CCN(C(=O)c2cc3c(=O)n4cc(C)ccc4nc3n2C)CC1</td>\n",
       "      <td>1.219</td>\n",
       "      <td>353.418</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.169</td>\n",
       "      <td>61.15</td>\n",
       "      <td>1.27232</td>\n",
       "      <td>353.426</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>TEST_478</td>\n",
       "      <td>CCc1noc(CC)c1CC(=O)NCC1(CC)CCCCC1</td>\n",
       "      <td>4.207</td>\n",
       "      <td>306.443</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4.207</td>\n",
       "      <td>55.13</td>\n",
       "      <td>3.81860</td>\n",
       "      <td>306.450</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>TEST_479</td>\n",
       "      <td>CC(=O)N1CCC2(CC1)OC(=O)C(C)=C2C(=O)N1CCN(C)CC1</td>\n",
       "      <td>-0.608</td>\n",
       "      <td>335.398</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.736</td>\n",
       "      <td>70.16</td>\n",
       "      <td>0.01480</td>\n",
       "      <td>335.404</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>TEST_480</td>\n",
       "      <td>CC(C)NC(=O)CN1C(=O)c2ccccc2N2C(=O)c3ccccc3C12</td>\n",
       "      <td>1.792</td>\n",
       "      <td>349.383</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.792</td>\n",
       "      <td>69.72</td>\n",
       "      <td>2.32600</td>\n",
       "      <td>349.390</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEST_481</td>\n",
       "      <td>Cn1cc(Br)c(=O)c(NC(=O)c2ccc(O)cc2F)c1</td>\n",
       "      <td>0.790</td>\n",
       "      <td>341.132</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.423</td>\n",
       "      <td>69.64</td>\n",
       "      <td>2.24480</td>\n",
       "      <td>341.136</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>TEST_482</td>\n",
       "      <td>CC(C)C(CCN1CCN(C)CC1)c1ccco1</td>\n",
       "      <td>2.782</td>\n",
       "      <td>250.380</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.606</td>\n",
       "      <td>19.62</td>\n",
       "      <td>2.65670</td>\n",
       "      <td>250.386</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows × 16 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c02f8cdc-5d85-4a15-9aad-e3d95603b7d0')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c02f8cdc-5d85-4a15-9aad-e3d95603b7d0 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c02f8cdc-5d85-4a15-9aad-e3d95603b7d0');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-2c89deca-e8a6-4410-a308-770e7f2a1e52\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c89deca-e8a6-4410-a308-770e7f2a1e52')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const charts = await google.colab.kernel.invokeFunction(\n",
       "          'suggestCharts', [key], {});\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-2c89deca-e8a6-4410-a308-770e7f2a1e52 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           id                                             SMILES  AlogP  \\\n",
       "0    TEST_000            CC(C)Nc1ccnc(N2CCN(Cc3cccs3)C(CCO)C2)n1  2.641   \n",
       "1    TEST_001     COc1cc(=O)n(-c2ccccc2)cc1C(=O)N1CCC2(CC1)OCCO2  0.585   \n",
       "2    TEST_002  Cc1cccc(NC(=N)/N=c2\\nc(O)c(Cc3ccccc3)c(C)[nH]2)c1  4.276   \n",
       "3    TEST_003         O=C(c1nc2ncccn2n1)N1CCCn2cc(-c3ccccc3)nc21  1.795   \n",
       "4    TEST_004       CCN1CCN(C(=O)c2cc3c(=O)n4cc(C)ccc4nc3n2C)CC1  1.219   \n",
       "..        ...                                                ...    ...   \n",
       "478  TEST_478                  CCc1noc(CC)c1CC(=O)NCC1(CC)CCCCC1  4.207   \n",
       "479  TEST_479     CC(=O)N1CCC2(CC1)OC(=O)C(C)=C2C(=O)N1CCN(C)CC1 -0.608   \n",
       "480  TEST_480      CC(C)NC(=O)CN1C(=O)c2ccccc2N2C(=O)c3ccccc3C12  1.792   \n",
       "481  TEST_481              Cn1cc(Br)c(=O)c(NC(=O)c2ccc(O)cc2F)c1  0.790   \n",
       "482  TEST_482                       CC(C)C(CCN1CCN(C)CC1)c1ccco1  2.782   \n",
       "\n",
       "     Molecular_Weight  Num_H_Acceptors  Num_H_Donors  Num_RotatableBonds  \\\n",
       "0             361.505                4             2                   7   \n",
       "1             370.399                5             0                   3   \n",
       "2             347.414                4             4                   5   \n",
       "3             345.358                5             0                   2   \n",
       "4             353.418                4             0                   2   \n",
       "..                ...              ...           ...                 ...   \n",
       "478           306.443                2             1                   7   \n",
       "479           335.398                5             0                   1   \n",
       "480           349.383                3             1                   3   \n",
       "481           341.132                3             2                   2   \n",
       "482           250.380                2             0                   5   \n",
       "\n",
       "      LogD  Molecular_PolarSurfaceArea     logP     apka  num_rotatable_bonds  \\\n",
       "0    2.635                       92.76  2.43160  361.515                    7   \n",
       "1    0.585                       68.31  1.82520  370.405                    3   \n",
       "2    4.290                       92.86  3.27051  347.422                    3   \n",
       "3    1.795                       81.21  2.03830  345.366                    2   \n",
       "4    0.169                       61.15  1.27232  353.426                    2   \n",
       "..     ...                         ...      ...      ...                  ...   \n",
       "478  4.207                       55.13  3.81860  306.450                    7   \n",
       "479 -1.736                       70.16  0.01480  335.404                    1   \n",
       "480  1.792                       69.72  2.32600  349.390                    3   \n",
       "481  0.423                       69.64  2.24480  341.136                    2   \n",
       "482  0.606                       19.62  2.65670  250.386                    5   \n",
       "\n",
       "     num_heteroatoms  num_hydrogen_acceptors  num_hydrogen_donors  \\\n",
       "0                  7                       7                    2   \n",
       "1                  7                       6                    0   \n",
       "2                  6                       3                    4   \n",
       "3                  8                       7                    0   \n",
       "4                  7                       6                    0   \n",
       "..               ...                     ...                  ...   \n",
       "478                4                       3                    1   \n",
       "479                7                       5                    0   \n",
       "480                6                       3                    1   \n",
       "481                7                       4                    2   \n",
       "482                3                       3                    0   \n",
       "\n",
       "                                    morgan_fingerprint  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "478  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "479  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
       "480  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "481  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "482  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, ...  \n",
       "\n",
       "[483 rows x 16 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f8054263",
   "metadata": {
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1692630246921,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "f8054263"
   },
   "outputs": [],
   "source": [
    "test_MLM = CustomDataset(test, target_col=None, transform=transform, is_test=True)\n",
    "test_HLM = CustomDataset(test, target_col=None, transform=transform, is_test=True)\n",
    "\n",
    "test_MLM_loader = DataLoader(dataset=test_MLM,\n",
    "                             batch_size=CFG['BATCH_SIZE'],\n",
    "                             shuffle=False)\n",
    "\n",
    "test_HLM_loader = DataLoader(dataset=test_HLM,\n",
    "                             batch_size=CFG['BATCH_SIZE'],\n",
    "                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5c7701c7",
   "metadata": {
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1692630250074,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "5c7701c7"
   },
   "outputs": [],
   "source": [
    "def inference(test_loader, model):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            output = model(inputs)\n",
    "            preds.extend(output.cpu().numpy().flatten().tolist())\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5c713f79",
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1692630251543,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "5c713f79"
   },
   "outputs": [],
   "source": [
    "predictions_MLM = inference(test_MLM_loader, model_MLM)\n",
    "predictions_HLM = inference(test_HLM_loader, model_HLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "30663a84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1692630253905,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "30663a84",
    "outputId": "8817908d-9b1c-4d10-d978-b51d869b2331"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-216b6de2-6917-4799-ac7d-b8524da46337\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>MLM</th>\n",
       "      <th>HLM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_000</td>\n",
       "      <td>10.312803</td>\n",
       "      <td>39.138935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_001</td>\n",
       "      <td>72.564072</td>\n",
       "      <td>81.700409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_002</td>\n",
       "      <td>25.773985</td>\n",
       "      <td>63.171459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_003</td>\n",
       "      <td>53.536915</td>\n",
       "      <td>72.244041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_004</td>\n",
       "      <td>55.700375</td>\n",
       "      <td>81.811401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>TEST_478</td>\n",
       "      <td>5.170557</td>\n",
       "      <td>18.745346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>TEST_479</td>\n",
       "      <td>74.937355</td>\n",
       "      <td>82.382797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>TEST_480</td>\n",
       "      <td>63.339695</td>\n",
       "      <td>74.157692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>TEST_481</td>\n",
       "      <td>50.817631</td>\n",
       "      <td>70.127655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>TEST_482</td>\n",
       "      <td>33.421608</td>\n",
       "      <td>42.076237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>483 rows × 3 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-216b6de2-6917-4799-ac7d-b8524da46337')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-216b6de2-6917-4799-ac7d-b8524da46337 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-216b6de2-6917-4799-ac7d-b8524da46337');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-42ae9bc5-1ce8-4e5f-813b-d6a9a005b791\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-42ae9bc5-1ce8-4e5f-813b-d6a9a005b791')\"\n",
       "            title=\"Suggest charts.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "    background-color: #E8F0FE;\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: #1967D2;\n",
       "    height: 32px;\n",
       "    padding: 0 0 0 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: #E2EBFA;\n",
       "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: #174EA6;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "    background-color: #3B4455;\n",
       "    fill: #D2E3FC;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart:hover {\n",
       "    background-color: #434B5C;\n",
       "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "    fill: #FFFFFF;\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const charts = await google.colab.kernel.invokeFunction(\n",
       "          'suggestCharts', [key], {});\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-42ae9bc5-1ce8-4e5f-813b-d6a9a005b791 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           id        MLM        HLM\n",
       "0    TEST_000  10.312803  39.138935\n",
       "1    TEST_001  72.564072  81.700409\n",
       "2    TEST_002  25.773985  63.171459\n",
       "3    TEST_003  53.536915  72.244041\n",
       "4    TEST_004  55.700375  81.811401\n",
       "..        ...        ...        ...\n",
       "478  TEST_478   5.170557  18.745346\n",
       "479  TEST_479  74.937355  82.382797\n",
       "480  TEST_480  63.339695  74.157692\n",
       "481  TEST_481  50.817631  70.127655\n",
       "482  TEST_482  33.421608  42.076237\n",
       "\n",
       "[483 rows x 3 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/data/sample_submission.csv')\n",
    "\n",
    "submission['MLM'] = predictions_MLM\n",
    "submission['HLM'] = predictions_HLM\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "2415c221",
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1692630255916,
     "user": {
      "displayName": "임송재",
      "userId": "10220915962739075092"
     },
     "user_tz": -540
    },
    "id": "2415c221"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('/content/drive/MyDrive/Colab Notebooks/AIDrug_Competition/submissions/OnlyNewFeature_DNN_Model_1_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aJpMgK_OdxXR",
   "metadata": {
    "id": "aJpMgK_OdxXR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
